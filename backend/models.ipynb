{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zxjyFQjJdTse",
        "tQSV5XHRcGHr",
        "Lf8KFDDWdR7K",
        "-0yZ3l27sApe",
        "ExWhzXqrs3yR"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unl4ip04MKF4"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVeCp6LGug3F"
      },
      "source": [
        "import pandas as pd\n",
        "from math import pi\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Getting rid of the stochasticy from our models by fixing the random number generator\n",
        "from numpy.random import seed\n",
        "seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9Hf0_XeuE8D",
        "outputId": "1ef5e49b-ede5-4762-a87e-d190fddf64c2"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX7F4F4qMVa2"
      },
      "source": [
        "df=pd.read_csv(\"../datasets/clean-playlists-one-artist.csv\")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTqQsEuT0V5z"
      },
      "source": [
        "# Models with only Audio Features.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq0uXnIW6L2w"
      },
      "source": [
        "# First we will import the necessary libraries for the model Creation\r\n",
        "from sklearn.feature_selection import RFECV\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeeRfz3I6htD"
      },
      "source": [
        "In this section two different studies will be selected in order to classify different Songs into Playlist. \r\n",
        "\r\n",
        "\r\n",
        "The first one will avoid and forget our analytical analysis from the Data Handling note book and will take in cosideration a Recursive Feature Elimination used to select the most important features in our set.\r\n",
        "\r\n",
        "\r\n",
        "Afterwards, A model with only the selected features from our previous study will be construted.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D4CIiNZ0oWB"
      },
      "source": [
        "Following a **Classification** model will be implemented using three different Classifying techniques:\r\n",
        "\r\n",
        "*   **Random Forest Classifier**\r\n",
        "*   **Naive Bayes**\r\n",
        "*   **Deep Neural Network**\r\n",
        "\r\n",
        "\r\n",
        "In order to do so, \"**Playlist**\" will be selected as the Target Label. From the previous study in the Data-Handling notebook, a list of the 10 most different Playlists is obtained. \r\n",
        "\r\n",
        "Each song in the Test set will be classified into one of the different Playlists by taking in consideration  whole set of **Audio Features** from that song, since it would give the best accuracy as RFE described. As previously explained, Audio Features are the Characteristics from a song, which are retrieved from Spotify´s API.\r\n",
        "\r\n",
        "The list of Playlists is:\r\n",
        "\r\n",
        "*   Tuff\r\n",
        "*   BlueBallads\r\n",
        "*   Punk Español\r\n",
        "*   Rap Español(TLob)\r\n",
        "*   Metal\r\n",
        "*   Romanticism\r\n",
        "*   PowerHour\r\n",
        "*   GoldSchool\r\n",
        "*   Chill\r\n",
        "*   CountryNights\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baLSHxbW0d5y"
      },
      "source": [
        "## Features selected by RFE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkSTbKQf-G7l"
      },
      "source": [
        " **Recursive Feature Elimination** (RFE) with random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaVD5cNs-E0Z"
      },
      "source": [
        "Recursive feature elimination (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. \r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "for i in range(1,len(features)):\r\n",
        "  rf_classifier = RandomForestClassifier() \r\n",
        "  rfecv = RFECV(estimator=rf_classifier , step=1, cv=i,scoring='accuracy') \r\n",
        "  rfecv = rfecv.fit(x_train, y_train)\r\n",
        "\r\n",
        "  print('Optimal number of features :', rfecv.n_features_)\r\n",
        "  print('Best features :', x_train.columns[rfecv.support_])\r\n",
        "\r\n",
        "\r\n",
        "  plt.figure()\r\n",
        "  plt.xlabel(\"Number of features selected\")\r\n",
        "  plt.ylabel(\"Cross validation score of number of selected features\")\r\n",
        "  plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\r\n",
        "  plt.show()\r\n",
        "```\r\n",
        "\r\n",
        "Finally, we find that **ALL** Audio Features will be needed for best classification. Now the model will be constructed\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4ulfBd_72r1"
      },
      "source": [
        "The Following cell will include:\r\n",
        "\r\n",
        "*   **The Sampling Process**: by selecting only the Chosen Playlists\r\n",
        "*   **Definition** of **X** set for the Audio Features\r\n",
        "*   **Standarization** of **X** \r\n",
        "*   **Definition** of Playlist as our **Y** value or our target value\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbiSUxMS3btz"
      },
      "source": [
        "#First we Sort our DataFrame for the selected Playlists in the list\r\n",
        "chosen_pls = ['Tuff', 'BlueBallads', 'PunkEspanol', 'RapEspanol(TLob)', 'Metal', 'Romanticism', 'PowerHour', 'GoldSchool', 'Chill', 'CountryNights']\r\n",
        "Cut_df = df.loc[df['Playlist'].isin(chosen_pls)]\r\n",
        "\r\n",
        "#Creating our Features\r\n",
        "X=Cut_df.iloc[:,3:-1]\r\n",
        "features_names=X.columns\r\n",
        "indexes=X.index\r\n",
        "\r\n",
        "#Creating our Label \r\n",
        "y=Cut_df.loc[:,[\"Playlist\"]].reset_index()\r\n",
        "y.columns=[\"Song Index\", \"Playlist\"]\r\n",
        "\r\n",
        "#Standarize the features\r\n",
        "X_Standarized=StandardScaler().fit_transform(X)\r\n",
        "X_Standarized=pd.DataFrame(X_Standarized)\r\n",
        "X_Standarized.columns=features_names\r\n",
        "X_Standarized[\"Song Index\"]=indexes\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te8dAAW8-MtA"
      },
      "source": [
        "### ***Random Forest Classification.*** \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO_MfQaq-XrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a138aa-c9da-456d-d259-88c4ed0a1a83"
      },
      "source": [
        "#Random Forest Classifier\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "RFClassifier= RandomForestClassifier(random_state=2)\r\n",
        "RFAccuracy=[]\r\n",
        "for i in range(0,10):\r\n",
        "\r\n",
        "  # Generate a print\r\n",
        "  print('------------------------------------------------------------------------')\r\n",
        "  print(f'Training for fold {i+1} ...')\r\n",
        "\r\n",
        "  #split test set\r\n",
        "  x_test=X_Standarized.sample(n=85, random_state=i)\r\n",
        "  indexes=x_test.index\r\n",
        "  y_test=y.iloc[indexes,:]\r\n",
        "  \r\n",
        "  #split train set\r\n",
        "  x_train=X_Standarized.drop(indexes)\r\n",
        "  y_train=y.drop(indexes)\r\n",
        "\r\n",
        "  #fiting for the train set and predicting with the test set\r\n",
        "  RFClassifier= RFClassifier.fit(x_train.iloc[:,:-1],y_train.iloc[:,1])\r\n",
        "  prediction=RFClassifier.predict(x_test.iloc[:,:-1])\r\n",
        "\r\n",
        "  #Random Forest Classifier Accuracy \r\n",
        "  RFAccuracy.append(accuracy_score(y_test.iloc[:,1],prediction))\r\n",
        "\r\n",
        "  #printing results\r\n",
        "  print(f'Score for fold {i*1}  {RFAccuracy[i]*100}%')\r\n",
        "\r\n",
        "\r\n",
        "print('------------------------------------------------------------------------')\r\n",
        "print('Average scores for all folds:')\r\n",
        "print(f'> Accuracy: {np.mean(RFAccuracy)} (+- {np.std(RFAccuracy)})')\r\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 0  74.11764705882354%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Score for fold 1  72.94117647058823%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Score for fold 2  82.35294117647058%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Score for fold 3  74.11764705882354%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Score for fold 4  83.52941176470588%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Score for fold 5  76.47058823529412%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Score for fold 6  78.82352941176471%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Score for fold 7  72.94117647058823%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Score for fold 8  83.52941176470588%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Score for fold 9  80.0%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 0.7788235294117646 (+- 0.041024931233324)\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9yvf1CS9s35"
      },
      "source": [
        "### ***Naive Bayes Classification.*** \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2hbcvbR9q5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922761e5-4395-47c9-d31c-0d22e5228497"
      },
      "source": [
        "#Naive Bayes Classifier \r\n",
        "NBClassifier = GaussianNB()\r\n",
        "NBAccuracy=[]\r\n",
        "\r\n",
        "#CrossValidation\r\n",
        "for i in range(0,10):\r\n",
        "  \r\n",
        "  # Generate a print\r\n",
        "  print('------------------------------------------------------------------------')\r\n",
        "  print(f'Training for fold {i+1} ...')\r\n",
        "\r\n",
        "  #split test set\r\n",
        "  x_test=X_Standarized.sample(n=85, random_state=i)\r\n",
        "  indexes=x_test.index\r\n",
        "  y_test=y.iloc[indexes,:]\r\n",
        "  \r\n",
        "  #split train set\r\n",
        "  x_train=X_Standarized.drop(indexes)\r\n",
        "  y_train=y.drop(indexes)\r\n",
        "\r\n",
        "  #fiting for the train set and predicting with the test set\r\n",
        "  NBClassifier= NBClassifier.fit(x_train.iloc[:,:-1],y_train.iloc[:,1])\r\n",
        "  prediction=NBClassifier.predict(x_test.iloc[:,:-1])\r\n",
        "\r\n",
        "  #Naive Bayes Classifier Accuracy \r\n",
        "  NBAccuracy.append(accuracy_score(y_test.iloc[:,1], prediction))\r\n",
        "  \r\n",
        "  #printing fold results\r\n",
        "  print(f'Score for fold {i*1}  {NBAccuracy[i]*100}%')\r\n",
        "\r\n",
        "#printing  overall results\r\n",
        "print('------------------------------------------------------------------------')\r\n",
        "print('Average scores for all folds:')\r\n",
        "print(f'> Accuracy: {np.mean(NBAccuracy)} (+- {np.std(NBAccuracy)})')\r\n",
        "print('------------------------------------------------------------------------')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 0  36.470588235294116%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Score for fold 1  35.294117647058826%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Score for fold 2  47.05882352941176%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Score for fold 3  41.17647058823529%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Score for fold 4  45.88235294117647%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Score for fold 5  31.76470588235294%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Score for fold 6  35.294117647058826%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Score for fold 7  29.411764705882355%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Score for fold 8  41.17647058823529%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Score for fold 9  42.35294117647059%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 0.38588235294117645 (+- 0.05563101372958536)\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhM8hvzE5sl5"
      },
      "source": [
        "### **Deep Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "AzRMQYQdgPO3",
        "outputId": "8abd1322-f617-4215-9dba-a991a36b75b2"
      },
      "source": [
        "# Scale continuous data\n",
        "non_wanted_features = ['Name', 'Artist', 'Playlist', 'Album'] # we want now all continuous features \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_continuous=Cut_df.drop(columns=non_wanted_features)\n",
        "features=X_continuous.columns\n",
        "\n",
        "X_continuous = scaler.fit_transform(X_continuous)\n",
        "X_continuous = pd.DataFrame(X_continuous)\n",
        "X_continuous.columns=features\n",
        "\n",
        "X_continuous"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Duration</th>\n",
              "      <th>Explicit</th>\n",
              "      <th>Popularity</th>\n",
              "      <th>Key</th>\n",
              "      <th>Mode</th>\n",
              "      <th>Time Signature</th>\n",
              "      <th>Acousticness</th>\n",
              "      <th>Danceability</th>\n",
              "      <th>Energy</th>\n",
              "      <th>Instrumentalness</th>\n",
              "      <th>Liveness</th>\n",
              "      <th>Loudness</th>\n",
              "      <th>Speechiness</th>\n",
              "      <th>Valence</th>\n",
              "      <th>Tempo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.067015</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.022727</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.052408</td>\n",
              "      <td>0.609729</td>\n",
              "      <td>0.601225</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.466076</td>\n",
              "      <td>0.891234</td>\n",
              "      <td>0.183161</td>\n",
              "      <td>0.455461</td>\n",
              "      <td>0.410860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.075587</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.038954</td>\n",
              "      <td>0.961538</td>\n",
              "      <td>0.546984</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078189</td>\n",
              "      <td>0.837884</td>\n",
              "      <td>0.456425</td>\n",
              "      <td>0.710580</td>\n",
              "      <td>0.270645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.151691</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.886364</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.016163</td>\n",
              "      <td>0.520362</td>\n",
              "      <td>0.915625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.765749</td>\n",
              "      <td>0.912589</td>\n",
              "      <td>0.131462</td>\n",
              "      <td>0.583021</td>\n",
              "      <td>0.651194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.085218</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.784091</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.014958</td>\n",
              "      <td>0.949095</td>\n",
              "      <td>0.466626</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.333122</td>\n",
              "      <td>0.821763</td>\n",
              "      <td>0.140325</td>\n",
              "      <td>0.273234</td>\n",
              "      <td>0.535097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.106419</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.534091</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.019577</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.913616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.225493</td>\n",
              "      <td>0.920134</td>\n",
              "      <td>0.048006</td>\n",
              "      <td>0.957123</td>\n",
              "      <td>0.483551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>0.375817</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.990950</td>\n",
              "      <td>0.561047</td>\n",
              "      <td>0.626927</td>\n",
              "      <td>0.064577</td>\n",
              "      <td>0.836041</td>\n",
              "      <td>0.638109</td>\n",
              "      <td>0.235717</td>\n",
              "      <td>0.483474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>891</th>\n",
              "      <td>0.068400</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>0.897059</td>\n",
              "      <td>0.930692</td>\n",
              "      <td>0.004275</td>\n",
              "      <td>0.080089</td>\n",
              "      <td>0.965154</td>\n",
              "      <td>0.242245</td>\n",
              "      <td>0.511202</td>\n",
              "      <td>0.470708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>0.108448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.420455</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.006534</td>\n",
              "      <td>0.679864</td>\n",
              "      <td>0.783035</td>\n",
              "      <td>0.018397</td>\n",
              "      <td>0.063100</td>\n",
              "      <td>0.954095</td>\n",
              "      <td>0.023634</td>\n",
              "      <td>0.184264</td>\n",
              "      <td>0.509424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>0.101885</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.563348</td>\n",
              "      <td>0.637386</td>\n",
              "      <td>0.808839</td>\n",
              "      <td>0.062573</td>\n",
              "      <td>0.866857</td>\n",
              "      <td>0.024815</td>\n",
              "      <td>0.049416</td>\n",
              "      <td>0.483293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>894</th>\n",
              "      <td>0.087236</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.001795</td>\n",
              "      <td>0.684389</td>\n",
              "      <td>0.914620</td>\n",
              "      <td>0.763618</td>\n",
              "      <td>0.219162</td>\n",
              "      <td>0.864055</td>\n",
              "      <td>0.049778</td>\n",
              "      <td>0.700933</td>\n",
              "      <td>0.470618</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>895 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Duration  Explicit  Popularity  ...  Speechiness   Valence     Tempo\n",
              "0    0.067015       1.0    0.022727  ...     0.183161  0.455461  0.410860\n",
              "1    0.075587       1.0    0.818182  ...     0.456425  0.710580  0.270645\n",
              "2    0.151691       1.0    0.886364  ...     0.131462  0.583021  0.651194\n",
              "3    0.085218       1.0    0.784091  ...     0.140325  0.273234  0.535097\n",
              "4    0.106419       0.0    0.534091  ...     0.048006  0.957123  0.483551\n",
              "..        ...       ...         ...  ...          ...       ...       ...\n",
              "890  0.375817       0.0    0.409091  ...     0.638109  0.235717  0.483474\n",
              "891  0.068400       0.0    0.590909  ...     0.242245  0.511202  0.470708\n",
              "892  0.108448       0.0    0.420455  ...     0.023634  0.184264  0.509424\n",
              "893  0.101885       0.0    0.500000  ...     0.024815  0.049416  0.483293\n",
              "894  0.087236       0.0    0.443182  ...     0.049778  0.700933  0.470618\n",
              "\n",
              "[895 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWF5-2ZC6vNk"
      },
      "source": [
        "#### One Hot Encode Target Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKITEmjN6vNz"
      },
      "source": [
        "### --- One Hot Encode Classes ---\n",
        "# Convert target playlist to one hot encoded playlist for Neural Network\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# First encode target values as integers from string\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Cut_df['Playlist'])\n",
        "hot_Y = encoder.transform(Cut_df['Playlist'])\n",
        "# Then perform one hot encoding\n",
        "hot_Y = np_utils.to_categorical(hot_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwkxN_c36vN0"
      },
      "source": [
        "### --- Split data into test and train data ---\n",
        "x_train, x_test, hot_y_train, hot_y_test = train_test_split(X_continuous, hot_Y, test_size=0.3)\n",
        "\n",
        "# Convert data to friendly arrays of TensorFlow\n",
        "x_train, x_test = x_train.values, x_test.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tJkFD6e6xOA"
      },
      "source": [
        "#### Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5_klxtK5rKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6235b1ec-fed9-444e-d8ff-e6c090a7b134"
      },
      "source": [
        "from keras.layers import Dense, Input \n",
        "from keras import Model\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "continuous_inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((hot_y_train,hot_y_test), axis=0)\n",
        "print(continuous_inputs.shape)\n",
        "## --- Model configuration ---\n",
        "batch_size = 16\n",
        "epochs = 60\n",
        "\n",
        "## --- Cross Validation ---\n",
        "# Define the K-fold Cross Validator\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(continuous_inputs, targets):\n",
        "  ## --- Model Architecture ---\n",
        "  n_numerical_feats = 15\n",
        "  n_classes = 10 # number of classes/playlists\n",
        "  \n",
        "  # numerical features input\n",
        "  size_input = n_numerical_feats\n",
        "  numerical_input = Input(shape=(size_input), name='numeric_input')\n",
        "\n",
        "  # hidden layers\n",
        "  # we want to make the network abstract the input information by reducing the dimensions\n",
        "  size_hidden1 = int(size_input*32)\n",
        "  size_hidden2 = int(size_input*32) \n",
        "\n",
        "  hidden1 = Dense(size_hidden1, activation='relu')(numerical_input)\n",
        "  hidden2 = Dense(size_hidden2, activation='relu')(hidden1)\n",
        "\n",
        "  # output layers\n",
        "  output = Dense(n_classes, activation='softmax')(hidden2)\n",
        "\n",
        "  # define the model\n",
        "  model = Model(numerical_input, output)\n",
        "  # compile the model\n",
        "  model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'], experimental_run_tf_function=False)\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  print(targets[train].shape)\n",
        "  history = model.fit(continuous_inputs[train] , targets[train],batch_size=batch_size,epochs=epochs,verbose=2)\n",
        "  \n",
        "  # Plot the training vs validation curves to see if we are overfitting the model\n",
        "  #plot_training(history, fold_no)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(continuous_inputs[test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# Provide average scores\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(895, 15)\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "(805, 10)\n",
            "Train on 805 samples\n",
            "Epoch 1/60\n",
            "805/805 - 0s - loss: 1.6523 - accuracy: 0.4609\n",
            "Epoch 2/60\n",
            "805/805 - 0s - loss: 1.0701 - accuracy: 0.6273\n",
            "Epoch 3/60\n",
            "805/805 - 0s - loss: 0.8956 - accuracy: 0.6807\n",
            "Epoch 4/60\n",
            "805/805 - 0s - loss: 0.8046 - accuracy: 0.7404\n",
            "Epoch 5/60\n",
            "805/805 - 0s - loss: 0.7588 - accuracy: 0.7478\n",
            "Epoch 6/60\n",
            "805/805 - 0s - loss: 0.7876 - accuracy: 0.7242\n",
            "Epoch 7/60\n",
            "805/805 - 0s - loss: 0.7224 - accuracy: 0.7553\n",
            "Epoch 8/60\n",
            "805/805 - 0s - loss: 0.6843 - accuracy: 0.7677\n",
            "Epoch 9/60\n",
            "805/805 - 0s - loss: 0.6770 - accuracy: 0.7739\n",
            "Epoch 10/60\n",
            "805/805 - 0s - loss: 0.6502 - accuracy: 0.7789\n",
            "Epoch 11/60\n",
            "805/805 - 0s - loss: 0.6505 - accuracy: 0.7590\n",
            "Epoch 12/60\n",
            "805/805 - 0s - loss: 0.6203 - accuracy: 0.7876\n",
            "Epoch 13/60\n",
            "805/805 - 0s - loss: 0.5999 - accuracy: 0.7839\n",
            "Epoch 14/60\n",
            "805/805 - 0s - loss: 0.5746 - accuracy: 0.8000\n",
            "Epoch 15/60\n",
            "805/805 - 0s - loss: 0.5674 - accuracy: 0.8186\n",
            "Epoch 16/60\n",
            "805/805 - 0s - loss: 0.5301 - accuracy: 0.8224\n",
            "Epoch 17/60\n",
            "805/805 - 0s - loss: 0.5145 - accuracy: 0.8248\n",
            "Epoch 18/60\n",
            "805/805 - 0s - loss: 0.5224 - accuracy: 0.8236\n",
            "Epoch 19/60\n",
            "805/805 - 0s - loss: 0.4873 - accuracy: 0.8298\n",
            "Epoch 20/60\n",
            "805/805 - 0s - loss: 0.4868 - accuracy: 0.8236\n",
            "Epoch 21/60\n",
            "805/805 - 0s - loss: 0.4796 - accuracy: 0.8311\n",
            "Epoch 22/60\n",
            "805/805 - 0s - loss: 0.4622 - accuracy: 0.8286\n",
            "Epoch 23/60\n",
            "805/805 - 0s - loss: 0.4413 - accuracy: 0.8410\n",
            "Epoch 24/60\n",
            "805/805 - 0s - loss: 0.4256 - accuracy: 0.8609\n",
            "Epoch 25/60\n",
            "805/805 - 0s - loss: 0.4505 - accuracy: 0.8385\n",
            "Epoch 26/60\n",
            "805/805 - 0s - loss: 0.4148 - accuracy: 0.8547\n",
            "Epoch 27/60\n",
            "805/805 - 0s - loss: 0.4137 - accuracy: 0.8584\n",
            "Epoch 28/60\n",
            "805/805 - 0s - loss: 0.3847 - accuracy: 0.8708\n",
            "Epoch 29/60\n",
            "805/805 - 0s - loss: 0.4030 - accuracy: 0.8646\n",
            "Epoch 30/60\n",
            "805/805 - 0s - loss: 0.4025 - accuracy: 0.8584\n",
            "Epoch 31/60\n",
            "805/805 - 0s - loss: 0.3731 - accuracy: 0.8609\n",
            "Epoch 32/60\n",
            "805/805 - 0s - loss: 0.3533 - accuracy: 0.8807\n",
            "Epoch 33/60\n",
            "805/805 - 0s - loss: 0.3528 - accuracy: 0.8733\n",
            "Epoch 34/60\n",
            "805/805 - 0s - loss: 0.3465 - accuracy: 0.8832\n",
            "Epoch 35/60\n",
            "805/805 - 0s - loss: 0.3390 - accuracy: 0.8745\n",
            "Epoch 36/60\n",
            "805/805 - 0s - loss: 0.3174 - accuracy: 0.8870\n",
            "Epoch 37/60\n",
            "805/805 - 0s - loss: 0.3238 - accuracy: 0.8857\n",
            "Epoch 38/60\n",
            "805/805 - 0s - loss: 0.3161 - accuracy: 0.8770\n",
            "Epoch 39/60\n",
            "805/805 - 0s - loss: 0.3333 - accuracy: 0.8646\n",
            "Epoch 40/60\n",
            "805/805 - 0s - loss: 0.3173 - accuracy: 0.8894\n",
            "Epoch 41/60\n",
            "805/805 - 0s - loss: 0.3054 - accuracy: 0.8845\n",
            "Epoch 42/60\n",
            "805/805 - 0s - loss: 0.2782 - accuracy: 0.8994\n",
            "Epoch 43/60\n",
            "805/805 - 0s - loss: 0.2765 - accuracy: 0.8969\n",
            "Epoch 44/60\n",
            "805/805 - 0s - loss: 0.2699 - accuracy: 0.9043\n",
            "Epoch 45/60\n",
            "805/805 - 0s - loss: 0.2502 - accuracy: 0.9093\n",
            "Epoch 46/60\n",
            "805/805 - 0s - loss: 0.2414 - accuracy: 0.9267\n",
            "Epoch 47/60\n",
            "805/805 - 0s - loss: 0.2460 - accuracy: 0.9217\n",
            "Epoch 48/60\n",
            "805/805 - 0s - loss: 0.2557 - accuracy: 0.9006\n",
            "Epoch 49/60\n",
            "805/805 - 0s - loss: 0.2585 - accuracy: 0.9118\n",
            "Epoch 50/60\n",
            "805/805 - 0s - loss: 0.2261 - accuracy: 0.9205\n",
            "Epoch 51/60\n",
            "805/805 - 0s - loss: 0.2216 - accuracy: 0.9329\n",
            "Epoch 52/60\n",
            "805/805 - 0s - loss: 0.2326 - accuracy: 0.9217\n",
            "Epoch 53/60\n",
            "805/805 - 0s - loss: 0.2113 - accuracy: 0.9242\n",
            "Epoch 54/60\n",
            "805/805 - 0s - loss: 0.2113 - accuracy: 0.9267\n",
            "Epoch 55/60\n",
            "805/805 - 0s - loss: 0.1969 - accuracy: 0.9366\n",
            "Epoch 56/60\n",
            "805/805 - 0s - loss: 0.1893 - accuracy: 0.9317\n",
            "Epoch 57/60\n",
            "805/805 - 0s - loss: 0.1830 - accuracy: 0.9379\n",
            "Epoch 58/60\n",
            "805/805 - 0s - loss: 0.1629 - accuracy: 0.9441\n",
            "Epoch 59/60\n",
            "805/805 - 0s - loss: 0.1804 - accuracy: 0.9317\n",
            "Epoch 60/60\n",
            "805/805 - 0s - loss: 0.1770 - accuracy: 0.9528\n",
            "Score for fold 1: loss of 1.444472368558248; accuracy of 67.77777671813965%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "(805, 10)\n",
            "Train on 805 samples\n",
            "Epoch 1/60\n",
            "805/805 - 0s - loss: 1.7163 - accuracy: 0.4286\n",
            "Epoch 2/60\n",
            "805/805 - 0s - loss: 1.0876 - accuracy: 0.6236\n",
            "Epoch 3/60\n",
            "805/805 - 0s - loss: 0.9103 - accuracy: 0.6882\n",
            "Epoch 4/60\n",
            "805/805 - 0s - loss: 0.8307 - accuracy: 0.7168\n",
            "Epoch 5/60\n",
            "805/805 - 0s - loss: 0.8338 - accuracy: 0.7093\n",
            "Epoch 6/60\n",
            "805/805 - 0s - loss: 0.7843 - accuracy: 0.7329\n",
            "Epoch 7/60\n",
            "805/805 - 0s - loss: 0.7358 - accuracy: 0.7429\n",
            "Epoch 8/60\n",
            "805/805 - 0s - loss: 0.6971 - accuracy: 0.7739\n",
            "Epoch 9/60\n",
            "805/805 - 0s - loss: 0.6707 - accuracy: 0.7702\n",
            "Epoch 10/60\n",
            "805/805 - 0s - loss: 0.6521 - accuracy: 0.7739\n",
            "Epoch 11/60\n",
            "805/805 - 0s - loss: 0.6222 - accuracy: 0.7863\n",
            "Epoch 12/60\n",
            "805/805 - 0s - loss: 0.6175 - accuracy: 0.8037\n",
            "Epoch 13/60\n",
            "805/805 - 0s - loss: 0.5926 - accuracy: 0.7888\n",
            "Epoch 14/60\n",
            "805/805 - 0s - loss: 0.5804 - accuracy: 0.7963\n",
            "Epoch 15/60\n",
            "805/805 - 0s - loss: 0.5530 - accuracy: 0.8211\n",
            "Epoch 16/60\n",
            "805/805 - 0s - loss: 0.5627 - accuracy: 0.8000\n",
            "Epoch 17/60\n",
            "805/805 - 0s - loss: 0.5282 - accuracy: 0.8236\n",
            "Epoch 18/60\n",
            "805/805 - 0s - loss: 0.5296 - accuracy: 0.8161\n",
            "Epoch 19/60\n",
            "805/805 - 0s - loss: 0.5111 - accuracy: 0.8174\n",
            "Epoch 20/60\n",
            "805/805 - 0s - loss: 0.4897 - accuracy: 0.8224\n",
            "Epoch 21/60\n",
            "805/805 - 0s - loss: 0.4796 - accuracy: 0.8348\n",
            "Epoch 22/60\n",
            "805/805 - 0s - loss: 0.4785 - accuracy: 0.8373\n",
            "Epoch 23/60\n",
            "805/805 - 0s - loss: 0.4649 - accuracy: 0.8385\n",
            "Epoch 24/60\n",
            "805/805 - 0s - loss: 0.4615 - accuracy: 0.8410\n",
            "Epoch 25/60\n",
            "805/805 - 0s - loss: 0.4198 - accuracy: 0.8534\n",
            "Epoch 26/60\n",
            "805/805 - 0s - loss: 0.4172 - accuracy: 0.8522\n",
            "Epoch 27/60\n",
            "805/805 - 0s - loss: 0.4139 - accuracy: 0.8497\n",
            "Epoch 28/60\n",
            "805/805 - 0s - loss: 0.3969 - accuracy: 0.8609\n",
            "Epoch 29/60\n",
            "805/805 - 0s - loss: 0.3835 - accuracy: 0.8571\n",
            "Epoch 30/60\n",
            "805/805 - 0s - loss: 0.3796 - accuracy: 0.8696\n",
            "Epoch 31/60\n",
            "805/805 - 0s - loss: 0.3721 - accuracy: 0.8758\n",
            "Epoch 32/60\n",
            "805/805 - 0s - loss: 0.3633 - accuracy: 0.8733\n",
            "Epoch 33/60\n",
            "805/805 - 0s - loss: 0.3706 - accuracy: 0.8621\n",
            "Epoch 34/60\n",
            "805/805 - 0s - loss: 0.3695 - accuracy: 0.8696\n",
            "Epoch 35/60\n",
            "805/805 - 0s - loss: 0.3512 - accuracy: 0.8720\n",
            "Epoch 36/60\n",
            "805/805 - 0s - loss: 0.3153 - accuracy: 0.8845\n",
            "Epoch 37/60\n",
            "805/805 - 0s - loss: 0.3173 - accuracy: 0.8758\n",
            "Epoch 38/60\n",
            "805/805 - 0s - loss: 0.2982 - accuracy: 0.8981\n",
            "Epoch 39/60\n",
            "805/805 - 0s - loss: 0.2954 - accuracy: 0.8957\n",
            "Epoch 40/60\n",
            "805/805 - 0s - loss: 0.2992 - accuracy: 0.8981\n",
            "Epoch 41/60\n",
            "805/805 - 0s - loss: 0.2840 - accuracy: 0.8969\n",
            "Epoch 42/60\n",
            "805/805 - 0s - loss: 0.2990 - accuracy: 0.8932\n",
            "Epoch 43/60\n",
            "805/805 - 0s - loss: 0.2793 - accuracy: 0.8994\n",
            "Epoch 44/60\n",
            "805/805 - 0s - loss: 0.2684 - accuracy: 0.9043\n",
            "Epoch 45/60\n",
            "805/805 - 0s - loss: 0.2603 - accuracy: 0.9081\n",
            "Epoch 46/60\n",
            "805/805 - 0s - loss: 0.2674 - accuracy: 0.8944\n",
            "Epoch 47/60\n",
            "805/805 - 0s - loss: 0.2574 - accuracy: 0.9043\n",
            "Epoch 48/60\n",
            "805/805 - 0s - loss: 0.2485 - accuracy: 0.9155\n",
            "Epoch 49/60\n",
            "805/805 - 0s - loss: 0.2345 - accuracy: 0.9217\n",
            "Epoch 50/60\n",
            "805/805 - 0s - loss: 0.1995 - accuracy: 0.9292\n",
            "Epoch 51/60\n",
            "805/805 - 0s - loss: 0.2086 - accuracy: 0.9242\n",
            "Epoch 52/60\n",
            "805/805 - 0s - loss: 0.1949 - accuracy: 0.9292\n",
            "Epoch 53/60\n",
            "805/805 - 0s - loss: 0.1955 - accuracy: 0.9342\n",
            "Epoch 54/60\n",
            "805/805 - 0s - loss: 0.1892 - accuracy: 0.9329\n",
            "Epoch 55/60\n",
            "805/805 - 0s - loss: 0.1767 - accuracy: 0.9453\n",
            "Epoch 56/60\n",
            "805/805 - 0s - loss: 0.1747 - accuracy: 0.9453\n",
            "Epoch 57/60\n",
            "805/805 - 0s - loss: 0.1774 - accuracy: 0.9404\n",
            "Epoch 58/60\n",
            "805/805 - 0s - loss: 0.1666 - accuracy: 0.9503\n",
            "Epoch 59/60\n",
            "805/805 - 0s - loss: 0.1770 - accuracy: 0.9404\n",
            "Epoch 60/60\n",
            "805/805 - 0s - loss: 0.1654 - accuracy: 0.9404\n",
            "Score for fold 2: loss of 1.0279531485504574; accuracy of 73.33333492279053%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "(805, 10)\n",
            "Train on 805 samples\n",
            "Epoch 1/60\n",
            "805/805 - 0s - loss: 1.6942 - accuracy: 0.4497\n",
            "Epoch 2/60\n",
            "805/805 - 0s - loss: 1.1191 - accuracy: 0.6062\n",
            "Epoch 3/60\n",
            "805/805 - 0s - loss: 0.9692 - accuracy: 0.6422\n",
            "Epoch 4/60\n",
            "805/805 - 0s - loss: 0.8535 - accuracy: 0.6919\n",
            "Epoch 5/60\n",
            "805/805 - 0s - loss: 0.8291 - accuracy: 0.7068\n",
            "Epoch 6/60\n",
            "805/805 - 0s - loss: 0.7631 - accuracy: 0.7304\n",
            "Epoch 7/60\n",
            "805/805 - 0s - loss: 0.7343 - accuracy: 0.7416\n",
            "Epoch 8/60\n",
            "805/805 - 0s - loss: 0.6769 - accuracy: 0.7702\n",
            "Epoch 9/60\n",
            "805/805 - 0s - loss: 0.6982 - accuracy: 0.7640\n",
            "Epoch 10/60\n",
            "805/805 - 0s - loss: 0.6595 - accuracy: 0.7863\n",
            "Epoch 11/60\n",
            "805/805 - 0s - loss: 0.6353 - accuracy: 0.7839\n",
            "Epoch 12/60\n",
            "805/805 - 0s - loss: 0.6385 - accuracy: 0.7888\n",
            "Epoch 13/60\n",
            "805/805 - 0s - loss: 0.6236 - accuracy: 0.7789\n",
            "Epoch 14/60\n",
            "805/805 - 0s - loss: 0.5697 - accuracy: 0.7963\n",
            "Epoch 15/60\n",
            "805/805 - 0s - loss: 0.5741 - accuracy: 0.8112\n",
            "Epoch 16/60\n",
            "805/805 - 0s - loss: 0.5530 - accuracy: 0.8050\n",
            "Epoch 17/60\n",
            "805/805 - 0s - loss: 0.5324 - accuracy: 0.8248\n",
            "Epoch 18/60\n",
            "805/805 - 0s - loss: 0.5418 - accuracy: 0.8037\n",
            "Epoch 19/60\n",
            "805/805 - 0s - loss: 0.5127 - accuracy: 0.8348\n",
            "Epoch 20/60\n",
            "805/805 - 0s - loss: 0.5131 - accuracy: 0.8273\n",
            "Epoch 21/60\n",
            "805/805 - 0s - loss: 0.5197 - accuracy: 0.8211\n",
            "Epoch 22/60\n",
            "805/805 - 0s - loss: 0.4855 - accuracy: 0.8323\n",
            "Epoch 23/60\n",
            "805/805 - 0s - loss: 0.4653 - accuracy: 0.8398\n",
            "Epoch 24/60\n",
            "805/805 - 0s - loss: 0.4613 - accuracy: 0.8298\n",
            "Epoch 25/60\n",
            "805/805 - 0s - loss: 0.4385 - accuracy: 0.8596\n",
            "Epoch 26/60\n",
            "805/805 - 0s - loss: 0.4478 - accuracy: 0.8484\n",
            "Epoch 27/60\n",
            "805/805 - 0s - loss: 0.4158 - accuracy: 0.8720\n",
            "Epoch 28/60\n",
            "805/805 - 0s - loss: 0.4228 - accuracy: 0.8422\n",
            "Epoch 29/60\n",
            "805/805 - 0s - loss: 0.4131 - accuracy: 0.8509\n",
            "Epoch 30/60\n",
            "805/805 - 0s - loss: 0.4139 - accuracy: 0.8559\n",
            "Epoch 31/60\n",
            "805/805 - 0s - loss: 0.3952 - accuracy: 0.8671\n",
            "Epoch 32/60\n",
            "805/805 - 0s - loss: 0.3729 - accuracy: 0.8745\n",
            "Epoch 33/60\n",
            "805/805 - 0s - loss: 0.3556 - accuracy: 0.8820\n",
            "Epoch 34/60\n",
            "805/805 - 0s - loss: 0.3507 - accuracy: 0.8845\n",
            "Epoch 35/60\n",
            "805/805 - 0s - loss: 0.3436 - accuracy: 0.8770\n",
            "Epoch 36/60\n",
            "805/805 - 0s - loss: 0.3406 - accuracy: 0.8882\n",
            "Epoch 37/60\n",
            "805/805 - 0s - loss: 0.3256 - accuracy: 0.8907\n",
            "Epoch 38/60\n",
            "805/805 - 0s - loss: 0.3276 - accuracy: 0.8957\n",
            "Epoch 39/60\n",
            "805/805 - 0s - loss: 0.3176 - accuracy: 0.8845\n",
            "Epoch 40/60\n",
            "805/805 - 0s - loss: 0.2999 - accuracy: 0.8957\n",
            "Epoch 41/60\n",
            "805/805 - 0s - loss: 0.3072 - accuracy: 0.8919\n",
            "Epoch 42/60\n",
            "805/805 - 0s - loss: 0.3003 - accuracy: 0.8907\n",
            "Epoch 43/60\n",
            "805/805 - 0s - loss: 0.2901 - accuracy: 0.8944\n",
            "Epoch 44/60\n",
            "805/805 - 0s - loss: 0.2635 - accuracy: 0.9093\n",
            "Epoch 45/60\n",
            "805/805 - 0s - loss: 0.2806 - accuracy: 0.9068\n",
            "Epoch 46/60\n",
            "805/805 - 0s - loss: 0.2719 - accuracy: 0.9056\n",
            "Epoch 47/60\n",
            "805/805 - 0s - loss: 0.2465 - accuracy: 0.9130\n",
            "Epoch 48/60\n",
            "805/805 - 0s - loss: 0.2594 - accuracy: 0.9056\n",
            "Epoch 49/60\n",
            "805/805 - 0s - loss: 0.2293 - accuracy: 0.9093\n",
            "Epoch 50/60\n",
            "805/805 - 0s - loss: 0.2267 - accuracy: 0.9267\n",
            "Epoch 51/60\n",
            "805/805 - 0s - loss: 0.2371 - accuracy: 0.9205\n",
            "Epoch 52/60\n",
            "805/805 - 0s - loss: 0.2281 - accuracy: 0.9217\n",
            "Epoch 53/60\n",
            "805/805 - 0s - loss: 0.2429 - accuracy: 0.9019\n",
            "Epoch 54/60\n",
            "805/805 - 0s - loss: 0.2114 - accuracy: 0.9304\n",
            "Epoch 55/60\n",
            "805/805 - 0s - loss: 0.2001 - accuracy: 0.9304\n",
            "Epoch 56/60\n",
            "805/805 - 0s - loss: 0.2193 - accuracy: 0.9193\n",
            "Epoch 57/60\n",
            "805/805 - 0s - loss: 0.2208 - accuracy: 0.9193\n",
            "Epoch 58/60\n",
            "805/805 - 0s - loss: 0.1802 - accuracy: 0.9342\n",
            "Epoch 59/60\n",
            "805/805 - 0s - loss: 0.1822 - accuracy: 0.9292\n",
            "Epoch 60/60\n",
            "805/805 - 0s - loss: 0.1705 - accuracy: 0.9404\n",
            "Score for fold 3: loss of 0.6889876113997565; accuracy of 81.11110925674438%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "(805, 10)\n",
            "Train on 805 samples\n",
            "Epoch 1/60\n",
            "805/805 - 0s - loss: 1.6824 - accuracy: 0.4472\n",
            "Epoch 2/60\n",
            "805/805 - 0s - loss: 1.0641 - accuracy: 0.6360\n",
            "Epoch 3/60\n",
            "805/805 - 0s - loss: 0.9367 - accuracy: 0.6534\n",
            "Epoch 4/60\n",
            "805/805 - 0s - loss: 0.8297 - accuracy: 0.7019\n",
            "Epoch 5/60\n",
            "805/805 - 0s - loss: 0.8076 - accuracy: 0.7118\n",
            "Epoch 6/60\n",
            "805/805 - 0s - loss: 0.7622 - accuracy: 0.7466\n",
            "Epoch 7/60\n",
            "805/805 - 0s - loss: 0.7238 - accuracy: 0.7528\n",
            "Epoch 8/60\n",
            "805/805 - 0s - loss: 0.7099 - accuracy: 0.7416\n",
            "Epoch 9/60\n",
            "805/805 - 0s - loss: 0.6968 - accuracy: 0.7602\n",
            "Epoch 10/60\n",
            "805/805 - 0s - loss: 0.6745 - accuracy: 0.7702\n",
            "Epoch 11/60\n",
            "805/805 - 0s - loss: 0.6419 - accuracy: 0.7814\n",
            "Epoch 12/60\n",
            "805/805 - 0s - loss: 0.6345 - accuracy: 0.7752\n",
            "Epoch 13/60\n",
            "805/805 - 0s - loss: 0.6133 - accuracy: 0.7888\n",
            "Epoch 14/60\n",
            "805/805 - 0s - loss: 0.5939 - accuracy: 0.7851\n",
            "Epoch 15/60\n",
            "805/805 - 0s - loss: 0.5909 - accuracy: 0.7963\n",
            "Epoch 16/60\n",
            "805/805 - 0s - loss: 0.5524 - accuracy: 0.8025\n",
            "Epoch 17/60\n",
            "805/805 - 0s - loss: 0.5626 - accuracy: 0.8149\n",
            "Epoch 18/60\n",
            "805/805 - 0s - loss: 0.5453 - accuracy: 0.8149\n",
            "Epoch 19/60\n",
            "805/805 - 0s - loss: 0.5402 - accuracy: 0.8000\n",
            "Epoch 20/60\n",
            "805/805 - 0s - loss: 0.5425 - accuracy: 0.8062\n",
            "Epoch 21/60\n",
            "805/805 - 0s - loss: 0.5094 - accuracy: 0.8236\n",
            "Epoch 22/60\n",
            "805/805 - 0s - loss: 0.4881 - accuracy: 0.8311\n",
            "Epoch 23/60\n",
            "805/805 - 0s - loss: 0.4621 - accuracy: 0.8311\n",
            "Epoch 24/60\n",
            "805/805 - 0s - loss: 0.4455 - accuracy: 0.8447\n",
            "Epoch 25/60\n",
            "805/805 - 0s - loss: 0.4691 - accuracy: 0.8236\n",
            "Epoch 26/60\n",
            "805/805 - 0s - loss: 0.4317 - accuracy: 0.8522\n",
            "Epoch 27/60\n",
            "805/805 - 0s - loss: 0.4265 - accuracy: 0.8447\n",
            "Epoch 28/60\n",
            "805/805 - 0s - loss: 0.4284 - accuracy: 0.8472\n",
            "Epoch 29/60\n",
            "805/805 - 0s - loss: 0.3977 - accuracy: 0.8596\n",
            "Epoch 30/60\n",
            "805/805 - 0s - loss: 0.4223 - accuracy: 0.8522\n",
            "Epoch 31/60\n",
            "805/805 - 0s - loss: 0.3865 - accuracy: 0.8596\n",
            "Epoch 32/60\n",
            "805/805 - 0s - loss: 0.3897 - accuracy: 0.8547\n",
            "Epoch 33/60\n",
            "805/805 - 0s - loss: 0.3929 - accuracy: 0.8646\n",
            "Epoch 34/60\n",
            "805/805 - 0s - loss: 0.3783 - accuracy: 0.8696\n",
            "Epoch 35/60\n",
            "805/805 - 0s - loss: 0.3477 - accuracy: 0.8758\n",
            "Epoch 36/60\n",
            "805/805 - 0s - loss: 0.3474 - accuracy: 0.8745\n",
            "Epoch 37/60\n",
            "805/805 - 0s - loss: 0.3228 - accuracy: 0.8894\n",
            "Epoch 38/60\n",
            "805/805 - 0s - loss: 0.3458 - accuracy: 0.8584\n",
            "Epoch 39/60\n",
            "805/805 - 0s - loss: 0.3094 - accuracy: 0.8944\n",
            "Epoch 40/60\n",
            "805/805 - 0s - loss: 0.2964 - accuracy: 0.8894\n",
            "Epoch 41/60\n",
            "805/805 - 0s - loss: 0.3005 - accuracy: 0.8944\n",
            "Epoch 42/60\n",
            "805/805 - 0s - loss: 0.3066 - accuracy: 0.8783\n",
            "Epoch 43/60\n",
            "805/805 - 0s - loss: 0.2747 - accuracy: 0.9081\n",
            "Epoch 44/60\n",
            "805/805 - 0s - loss: 0.2773 - accuracy: 0.9031\n",
            "Epoch 45/60\n",
            "805/805 - 0s - loss: 0.2807 - accuracy: 0.8932\n",
            "Epoch 46/60\n",
            "805/805 - 0s - loss: 0.2417 - accuracy: 0.9143\n",
            "Epoch 47/60\n",
            "805/805 - 0s - loss: 0.2307 - accuracy: 0.9255\n",
            "Epoch 48/60\n",
            "805/805 - 0s - loss: 0.2425 - accuracy: 0.9180\n",
            "Epoch 49/60\n",
            "805/805 - 0s - loss: 0.2413 - accuracy: 0.9106\n",
            "Epoch 50/60\n",
            "805/805 - 0s - loss: 0.2282 - accuracy: 0.9230\n",
            "Epoch 51/60\n",
            "805/805 - 0s - loss: 0.2466 - accuracy: 0.9130\n",
            "Epoch 52/60\n",
            "805/805 - 0s - loss: 0.2122 - accuracy: 0.9217\n",
            "Epoch 53/60\n",
            "805/805 - 0s - loss: 0.2122 - accuracy: 0.9267\n",
            "Epoch 54/60\n",
            "805/805 - 0s - loss: 0.2193 - accuracy: 0.9168\n",
            "Epoch 55/60\n",
            "805/805 - 0s - loss: 0.1976 - accuracy: 0.9366\n",
            "Epoch 56/60\n",
            "805/805 - 0s - loss: 0.1984 - accuracy: 0.9230\n",
            "Epoch 57/60\n",
            "805/805 - 0s - loss: 0.1881 - accuracy: 0.9366\n",
            "Epoch 58/60\n",
            "805/805 - 0s - loss: 0.1744 - accuracy: 0.9503\n",
            "Epoch 59/60\n",
            "805/805 - 0s - loss: 0.1698 - accuracy: 0.9478\n",
            "Epoch 60/60\n",
            "805/805 - 0s - loss: 0.1723 - accuracy: 0.9453\n",
            "Score for fold 4: loss of 0.8601152486271328; accuracy of 77.77777910232544%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "(805, 10)\n",
            "Train on 805 samples\n",
            "Epoch 1/60\n",
            "805/805 - 0s - loss: 1.7289 - accuracy: 0.4298\n",
            "Epoch 2/60\n",
            "805/805 - 0s - loss: 1.1307 - accuracy: 0.5988\n",
            "Epoch 3/60\n",
            "805/805 - 0s - loss: 0.9467 - accuracy: 0.6658\n",
            "Epoch 4/60\n",
            "805/805 - 0s - loss: 0.8594 - accuracy: 0.6969\n",
            "Epoch 5/60\n",
            "805/805 - 0s - loss: 0.8200 - accuracy: 0.7180\n",
            "Epoch 6/60\n",
            "805/805 - 0s - loss: 0.7695 - accuracy: 0.7280\n",
            "Epoch 7/60\n",
            "805/805 - 0s - loss: 0.7501 - accuracy: 0.7317\n",
            "Epoch 8/60\n",
            "805/805 - 0s - loss: 0.6986 - accuracy: 0.7516\n",
            "Epoch 9/60\n",
            "805/805 - 0s - loss: 0.6875 - accuracy: 0.7578\n",
            "Epoch 10/60\n",
            "805/805 - 0s - loss: 0.6728 - accuracy: 0.7615\n",
            "Epoch 11/60\n",
            "805/805 - 0s - loss: 0.6750 - accuracy: 0.7814\n",
            "Epoch 12/60\n",
            "805/805 - 0s - loss: 0.6327 - accuracy: 0.7677\n",
            "Epoch 13/60\n",
            "805/805 - 0s - loss: 0.6258 - accuracy: 0.7901\n",
            "Epoch 14/60\n",
            "805/805 - 0s - loss: 0.5937 - accuracy: 0.7925\n",
            "Epoch 15/60\n",
            "805/805 - 0s - loss: 0.5854 - accuracy: 0.8075\n",
            "Epoch 16/60\n",
            "805/805 - 0s - loss: 0.5768 - accuracy: 0.7975\n",
            "Epoch 17/60\n",
            "805/805 - 0s - loss: 0.5579 - accuracy: 0.7938\n",
            "Epoch 18/60\n",
            "805/805 - 0s - loss: 0.5575 - accuracy: 0.7950\n",
            "Epoch 19/60\n",
            "805/805 - 0s - loss: 0.5527 - accuracy: 0.7963\n",
            "Epoch 20/60\n",
            "805/805 - 0s - loss: 0.5103 - accuracy: 0.8298\n",
            "Epoch 21/60\n",
            "805/805 - 0s - loss: 0.5301 - accuracy: 0.8037\n",
            "Epoch 22/60\n",
            "805/805 - 0s - loss: 0.4978 - accuracy: 0.8211\n",
            "Epoch 23/60\n",
            "805/805 - 0s - loss: 0.4998 - accuracy: 0.8149\n",
            "Epoch 24/60\n",
            "805/805 - 0s - loss: 0.4707 - accuracy: 0.8360\n",
            "Epoch 25/60\n",
            "805/805 - 0s - loss: 0.4741 - accuracy: 0.8335\n",
            "Epoch 26/60\n",
            "805/805 - 0s - loss: 0.4514 - accuracy: 0.8398\n",
            "Epoch 27/60\n",
            "805/805 - 0s - loss: 0.4781 - accuracy: 0.8398\n",
            "Epoch 28/60\n",
            "805/805 - 0s - loss: 0.4203 - accuracy: 0.8447\n",
            "Epoch 29/60\n",
            "805/805 - 0s - loss: 0.4180 - accuracy: 0.8571\n",
            "Epoch 30/60\n",
            "805/805 - 0s - loss: 0.3974 - accuracy: 0.8609\n",
            "Epoch 31/60\n",
            "805/805 - 0s - loss: 0.3961 - accuracy: 0.8497\n",
            "Epoch 32/60\n",
            "805/805 - 0s - loss: 0.4193 - accuracy: 0.8435\n",
            "Epoch 33/60\n",
            "805/805 - 0s - loss: 0.3684 - accuracy: 0.8683\n",
            "Epoch 34/60\n",
            "805/805 - 0s - loss: 0.3910 - accuracy: 0.8547\n",
            "Epoch 35/60\n",
            "805/805 - 0s - loss: 0.3531 - accuracy: 0.8646\n",
            "Epoch 36/60\n",
            "805/805 - 0s - loss: 0.3708 - accuracy: 0.8646\n",
            "Epoch 37/60\n",
            "805/805 - 0s - loss: 0.3364 - accuracy: 0.8807\n",
            "Epoch 38/60\n",
            "805/805 - 0s - loss: 0.3345 - accuracy: 0.8720\n",
            "Epoch 39/60\n",
            "805/805 - 0s - loss: 0.3717 - accuracy: 0.8559\n",
            "Epoch 40/60\n",
            "805/805 - 0s - loss: 0.3174 - accuracy: 0.8820\n",
            "Epoch 41/60\n",
            "805/805 - 0s - loss: 0.3080 - accuracy: 0.8919\n",
            "Epoch 42/60\n",
            "805/805 - 0s - loss: 0.3106 - accuracy: 0.8745\n",
            "Epoch 43/60\n",
            "805/805 - 0s - loss: 0.2754 - accuracy: 0.8944\n",
            "Epoch 44/60\n",
            "805/805 - 0s - loss: 0.2905 - accuracy: 0.8907\n",
            "Epoch 45/60\n",
            "805/805 - 0s - loss: 0.2786 - accuracy: 0.8944\n",
            "Epoch 46/60\n",
            "805/805 - 0s - loss: 0.2721 - accuracy: 0.8957\n",
            "Epoch 47/60\n",
            "805/805 - 0s - loss: 0.2586 - accuracy: 0.8994\n",
            "Epoch 48/60\n",
            "805/805 - 0s - loss: 0.2712 - accuracy: 0.8932\n",
            "Epoch 49/60\n",
            "805/805 - 0s - loss: 0.2707 - accuracy: 0.9019\n",
            "Epoch 50/60\n",
            "805/805 - 0s - loss: 0.2560 - accuracy: 0.9019\n",
            "Epoch 51/60\n",
            "805/805 - 0s - loss: 0.2447 - accuracy: 0.9106\n",
            "Epoch 52/60\n",
            "805/805 - 0s - loss: 0.2562 - accuracy: 0.8994\n",
            "Epoch 53/60\n",
            "805/805 - 0s - loss: 0.2374 - accuracy: 0.9093\n",
            "Epoch 54/60\n",
            "805/805 - 0s - loss: 0.2342 - accuracy: 0.9168\n",
            "Epoch 55/60\n",
            "805/805 - 0s - loss: 0.2274 - accuracy: 0.9093\n",
            "Epoch 56/60\n",
            "805/805 - 0s - loss: 0.1956 - accuracy: 0.9280\n",
            "Epoch 57/60\n",
            "805/805 - 0s - loss: 0.1992 - accuracy: 0.9304\n",
            "Epoch 58/60\n",
            "805/805 - 0s - loss: 0.2068 - accuracy: 0.9230\n",
            "Epoch 59/60\n",
            "805/805 - 0s - loss: 0.1870 - accuracy: 0.9317\n",
            "Epoch 60/60\n",
            "805/805 - 0s - loss: 0.2003 - accuracy: 0.9317\n",
            "Score for fold 5: loss of 0.6397473586930169; accuracy of 80.0000011920929%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "(806, 10)\n",
            "Train on 806 samples\n",
            "Epoch 1/60\n",
            "806/806 - 0s - loss: 1.6690 - accuracy: 0.4640\n",
            "Epoch 2/60\n",
            "806/806 - 0s - loss: 1.0649 - accuracy: 0.6141\n",
            "Epoch 3/60\n",
            "806/806 - 0s - loss: 0.8933 - accuracy: 0.6873\n",
            "Epoch 4/60\n",
            "806/806 - 0s - loss: 0.8461 - accuracy: 0.7035\n",
            "Epoch 5/60\n",
            "806/806 - 0s - loss: 0.8097 - accuracy: 0.7171\n",
            "Epoch 6/60\n",
            "806/806 - 0s - loss: 0.7699 - accuracy: 0.7295\n",
            "Epoch 7/60\n",
            "806/806 - 0s - loss: 0.7295 - accuracy: 0.7419\n",
            "Epoch 8/60\n",
            "806/806 - 0s - loss: 0.6947 - accuracy: 0.7556\n",
            "Epoch 9/60\n",
            "806/806 - 0s - loss: 0.6678 - accuracy: 0.7692\n",
            "Epoch 10/60\n",
            "806/806 - 0s - loss: 0.6477 - accuracy: 0.7854\n",
            "Epoch 11/60\n",
            "806/806 - 0s - loss: 0.6452 - accuracy: 0.7730\n",
            "Epoch 12/60\n",
            "806/806 - 0s - loss: 0.6330 - accuracy: 0.7816\n",
            "Epoch 13/60\n",
            "806/806 - 0s - loss: 0.6192 - accuracy: 0.7779\n",
            "Epoch 14/60\n",
            "806/806 - 0s - loss: 0.6042 - accuracy: 0.8077\n",
            "Epoch 15/60\n",
            "806/806 - 0s - loss: 0.5796 - accuracy: 0.8065\n",
            "Epoch 16/60\n",
            "806/806 - 0s - loss: 0.5484 - accuracy: 0.8226\n",
            "Epoch 17/60\n",
            "806/806 - 0s - loss: 0.5237 - accuracy: 0.8151\n",
            "Epoch 18/60\n",
            "806/806 - 0s - loss: 0.5473 - accuracy: 0.8065\n",
            "Epoch 19/60\n",
            "806/806 - 0s - loss: 0.5156 - accuracy: 0.8313\n",
            "Epoch 20/60\n",
            "806/806 - 0s - loss: 0.4866 - accuracy: 0.8412\n",
            "Epoch 21/60\n",
            "806/806 - 0s - loss: 0.4836 - accuracy: 0.8424\n",
            "Epoch 22/60\n",
            "806/806 - 0s - loss: 0.4998 - accuracy: 0.8238\n",
            "Epoch 23/60\n",
            "806/806 - 0s - loss: 0.4608 - accuracy: 0.8387\n",
            "Epoch 24/60\n",
            "806/806 - 0s - loss: 0.4696 - accuracy: 0.8313\n",
            "Epoch 25/60\n",
            "806/806 - 0s - loss: 0.4372 - accuracy: 0.8474\n",
            "Epoch 26/60\n",
            "806/806 - 0s - loss: 0.4208 - accuracy: 0.8524\n",
            "Epoch 27/60\n",
            "806/806 - 0s - loss: 0.4246 - accuracy: 0.8586\n",
            "Epoch 28/60\n",
            "806/806 - 0s - loss: 0.4024 - accuracy: 0.8586\n",
            "Epoch 29/60\n",
            "806/806 - 0s - loss: 0.4374 - accuracy: 0.8400\n",
            "Epoch 30/60\n",
            "806/806 - 0s - loss: 0.3875 - accuracy: 0.8474\n",
            "Epoch 31/60\n",
            "806/806 - 0s - loss: 0.3916 - accuracy: 0.8635\n",
            "Epoch 32/60\n",
            "806/806 - 0s - loss: 0.3670 - accuracy: 0.8672\n",
            "Epoch 33/60\n",
            "806/806 - 0s - loss: 0.3807 - accuracy: 0.8648\n",
            "Epoch 34/60\n",
            "806/806 - 0s - loss: 0.3717 - accuracy: 0.8610\n",
            "Epoch 35/60\n",
            "806/806 - 0s - loss: 0.3434 - accuracy: 0.8747\n",
            "Epoch 36/60\n",
            "806/806 - 0s - loss: 0.3279 - accuracy: 0.8759\n",
            "Epoch 37/60\n",
            "806/806 - 0s - loss: 0.3171 - accuracy: 0.8896\n",
            "Epoch 38/60\n",
            "806/806 - 0s - loss: 0.3213 - accuracy: 0.8883\n",
            "Epoch 39/60\n",
            "806/806 - 0s - loss: 0.3256 - accuracy: 0.8797\n",
            "Epoch 40/60\n",
            "806/806 - 0s - loss: 0.3006 - accuracy: 0.9007\n",
            "Epoch 41/60\n",
            "806/806 - 0s - loss: 0.2933 - accuracy: 0.8995\n",
            "Epoch 42/60\n",
            "806/806 - 0s - loss: 0.3095 - accuracy: 0.8797\n",
            "Epoch 43/60\n",
            "806/806 - 0s - loss: 0.2782 - accuracy: 0.9007\n",
            "Epoch 44/60\n",
            "806/806 - 0s - loss: 0.2618 - accuracy: 0.9082\n",
            "Epoch 45/60\n",
            "806/806 - 0s - loss: 0.2671 - accuracy: 0.9007\n",
            "Epoch 46/60\n",
            "806/806 - 0s - loss: 0.2608 - accuracy: 0.9020\n",
            "Epoch 47/60\n",
            "806/806 - 0s - loss: 0.2466 - accuracy: 0.9194\n",
            "Epoch 48/60\n",
            "806/806 - 0s - loss: 0.2362 - accuracy: 0.9181\n",
            "Epoch 49/60\n",
            "806/806 - 0s - loss: 0.2218 - accuracy: 0.9280\n",
            "Epoch 50/60\n",
            "806/806 - 0s - loss: 0.2227 - accuracy: 0.9293\n",
            "Epoch 51/60\n",
            "806/806 - 0s - loss: 0.2369 - accuracy: 0.9181\n",
            "Epoch 52/60\n",
            "806/806 - 0s - loss: 0.2411 - accuracy: 0.9119\n",
            "Epoch 53/60\n",
            "806/806 - 0s - loss: 0.2070 - accuracy: 0.9256\n",
            "Epoch 54/60\n",
            "806/806 - 0s - loss: 0.1944 - accuracy: 0.9417\n",
            "Epoch 55/60\n",
            "806/806 - 0s - loss: 0.2447 - accuracy: 0.9107\n",
            "Epoch 56/60\n",
            "806/806 - 0s - loss: 0.1897 - accuracy: 0.9392\n",
            "Epoch 57/60\n",
            "806/806 - 0s - loss: 0.1896 - accuracy: 0.9256\n",
            "Epoch 58/60\n",
            "806/806 - 0s - loss: 0.1793 - accuracy: 0.9380\n",
            "Epoch 59/60\n",
            "806/806 - 0s - loss: 0.1816 - accuracy: 0.9380\n",
            "Epoch 60/60\n",
            "806/806 - 0s - loss: 0.2276 - accuracy: 0.9231\n",
            "Score for fold 6: loss of 0.9547809174891269; accuracy of 71.91011309623718%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "(806, 10)\n",
            "Train on 806 samples\n",
            "Epoch 1/60\n",
            "806/806 - 0s - loss: 1.6770 - accuracy: 0.4367\n",
            "Epoch 2/60\n",
            "806/806 - 0s - loss: 1.0611 - accuracy: 0.6402\n",
            "Epoch 3/60\n",
            "806/806 - 0s - loss: 0.9170 - accuracy: 0.6638\n",
            "Epoch 4/60\n",
            "806/806 - 0s - loss: 0.8461 - accuracy: 0.6911\n",
            "Epoch 5/60\n",
            "806/806 - 0s - loss: 0.7957 - accuracy: 0.7134\n",
            "Epoch 6/60\n",
            "806/806 - 0s - loss: 0.7425 - accuracy: 0.7494\n",
            "Epoch 7/60\n",
            "806/806 - 0s - loss: 0.7018 - accuracy: 0.7581\n",
            "Epoch 8/60\n",
            "806/806 - 0s - loss: 0.6971 - accuracy: 0.7556\n",
            "Epoch 9/60\n",
            "806/806 - 0s - loss: 0.6503 - accuracy: 0.7829\n",
            "Epoch 10/60\n",
            "806/806 - 0s - loss: 0.6498 - accuracy: 0.7655\n",
            "Epoch 11/60\n",
            "806/806 - 0s - loss: 0.6502 - accuracy: 0.7717\n",
            "Epoch 12/60\n",
            "806/806 - 0s - loss: 0.6191 - accuracy: 0.7804\n",
            "Epoch 13/60\n",
            "806/806 - 0s - loss: 0.6005 - accuracy: 0.7878\n",
            "Epoch 14/60\n",
            "806/806 - 0s - loss: 0.5807 - accuracy: 0.7866\n",
            "Epoch 15/60\n",
            "806/806 - 0s - loss: 0.5703 - accuracy: 0.8027\n",
            "Epoch 16/60\n",
            "806/806 - 0s - loss: 0.5438 - accuracy: 0.8089\n",
            "Epoch 17/60\n",
            "806/806 - 0s - loss: 0.5141 - accuracy: 0.8102\n",
            "Epoch 18/60\n",
            "806/806 - 0s - loss: 0.5095 - accuracy: 0.8226\n",
            "Epoch 19/60\n",
            "806/806 - 0s - loss: 0.5076 - accuracy: 0.8151\n",
            "Epoch 20/60\n",
            "806/806 - 0s - loss: 0.4996 - accuracy: 0.8201\n",
            "Epoch 21/60\n",
            "806/806 - 0s - loss: 0.4922 - accuracy: 0.8189\n",
            "Epoch 22/60\n",
            "806/806 - 0s - loss: 0.4701 - accuracy: 0.8313\n",
            "Epoch 23/60\n",
            "806/806 - 0s - loss: 0.4740 - accuracy: 0.8375\n",
            "Epoch 24/60\n",
            "806/806 - 0s - loss: 0.4483 - accuracy: 0.8400\n",
            "Epoch 25/60\n",
            "806/806 - 0s - loss: 0.4448 - accuracy: 0.8387\n",
            "Epoch 26/60\n",
            "806/806 - 0s - loss: 0.4379 - accuracy: 0.8337\n",
            "Epoch 27/60\n",
            "806/806 - 0s - loss: 0.4531 - accuracy: 0.8437\n",
            "Epoch 28/60\n",
            "806/806 - 0s - loss: 0.4200 - accuracy: 0.8598\n",
            "Epoch 29/60\n",
            "806/806 - 0s - loss: 0.3891 - accuracy: 0.8499\n",
            "Epoch 30/60\n",
            "806/806 - 0s - loss: 0.4079 - accuracy: 0.8350\n",
            "Epoch 31/60\n",
            "806/806 - 0s - loss: 0.3751 - accuracy: 0.8648\n",
            "Epoch 32/60\n",
            "806/806 - 0s - loss: 0.3776 - accuracy: 0.8672\n",
            "Epoch 33/60\n",
            "806/806 - 0s - loss: 0.3576 - accuracy: 0.8734\n",
            "Epoch 34/60\n",
            "806/806 - 0s - loss: 0.3482 - accuracy: 0.8958\n",
            "Epoch 35/60\n",
            "806/806 - 0s - loss: 0.3352 - accuracy: 0.8821\n",
            "Epoch 36/60\n",
            "806/806 - 0s - loss: 0.3364 - accuracy: 0.8759\n",
            "Epoch 37/60\n",
            "806/806 - 0s - loss: 0.3302 - accuracy: 0.8747\n",
            "Epoch 38/60\n",
            "806/806 - 0s - loss: 0.3200 - accuracy: 0.8945\n",
            "Epoch 39/60\n",
            "806/806 - 0s - loss: 0.3084 - accuracy: 0.8896\n",
            "Epoch 40/60\n",
            "806/806 - 0s - loss: 0.3002 - accuracy: 0.8995\n",
            "Epoch 41/60\n",
            "806/806 - 0s - loss: 0.3048 - accuracy: 0.8958\n",
            "Epoch 42/60\n",
            "806/806 - 0s - loss: 0.2874 - accuracy: 0.9020\n",
            "Epoch 43/60\n",
            "806/806 - 0s - loss: 0.2651 - accuracy: 0.9156\n",
            "Epoch 44/60\n",
            "806/806 - 0s - loss: 0.3024 - accuracy: 0.8933\n",
            "Epoch 45/60\n",
            "806/806 - 0s - loss: 0.2792 - accuracy: 0.8983\n",
            "Epoch 46/60\n",
            "806/806 - 0s - loss: 0.2519 - accuracy: 0.9218\n",
            "Epoch 47/60\n",
            "806/806 - 0s - loss: 0.2461 - accuracy: 0.9156\n",
            "Epoch 48/60\n",
            "806/806 - 0s - loss: 0.2417 - accuracy: 0.9293\n",
            "Epoch 49/60\n",
            "806/806 - 0s - loss: 0.2279 - accuracy: 0.9119\n",
            "Epoch 50/60\n",
            "806/806 - 0s - loss: 0.2300 - accuracy: 0.9268\n",
            "Epoch 51/60\n",
            "806/806 - 0s - loss: 0.2325 - accuracy: 0.9169\n",
            "Epoch 52/60\n",
            "806/806 - 0s - loss: 0.2170 - accuracy: 0.9231\n",
            "Epoch 53/60\n",
            "806/806 - 0s - loss: 0.2261 - accuracy: 0.9169\n",
            "Epoch 54/60\n",
            "806/806 - 0s - loss: 0.2137 - accuracy: 0.9318\n",
            "Epoch 55/60\n",
            "806/806 - 0s - loss: 0.2147 - accuracy: 0.9280\n",
            "Epoch 56/60\n",
            "806/806 - 0s - loss: 0.2058 - accuracy: 0.9268\n",
            "Epoch 57/60\n",
            "806/806 - 0s - loss: 0.1916 - accuracy: 0.9330\n",
            "Epoch 58/60\n",
            "806/806 - 0s - loss: 0.2047 - accuracy: 0.9280\n",
            "Epoch 59/60\n",
            "806/806 - 0s - loss: 0.1931 - accuracy: 0.9305\n",
            "Epoch 60/60\n",
            "806/806 - 0s - loss: 0.1674 - accuracy: 0.9454\n",
            "Score for fold 7: loss of 1.125199763292677; accuracy of 80.89887499809265%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "(806, 10)\n",
            "Train on 806 samples\n",
            "Epoch 1/60\n",
            "806/806 - 0s - loss: 1.6825 - accuracy: 0.4243\n",
            "Epoch 2/60\n",
            "806/806 - 0s - loss: 1.0874 - accuracy: 0.6266\n",
            "Epoch 3/60\n",
            "806/806 - 0s - loss: 0.9294 - accuracy: 0.6625\n",
            "Epoch 4/60\n",
            "806/806 - 0s - loss: 0.8358 - accuracy: 0.7134\n",
            "Epoch 5/60\n",
            "806/806 - 0s - loss: 0.7923 - accuracy: 0.7233\n",
            "Epoch 6/60\n",
            "806/806 - 0s - loss: 0.7700 - accuracy: 0.7395\n",
            "Epoch 7/60\n",
            "806/806 - 0s - loss: 0.7311 - accuracy: 0.7531\n",
            "Epoch 8/60\n",
            "806/806 - 0s - loss: 0.7154 - accuracy: 0.7444\n",
            "Epoch 9/60\n",
            "806/806 - 0s - loss: 0.6912 - accuracy: 0.7655\n",
            "Epoch 10/60\n",
            "806/806 - 0s - loss: 0.6503 - accuracy: 0.7829\n",
            "Epoch 11/60\n",
            "806/806 - 0s - loss: 0.6485 - accuracy: 0.7767\n",
            "Epoch 12/60\n",
            "806/806 - 0s - loss: 0.6479 - accuracy: 0.7940\n",
            "Epoch 13/60\n",
            "806/806 - 0s - loss: 0.6156 - accuracy: 0.7903\n",
            "Epoch 14/60\n",
            "806/806 - 0s - loss: 0.5822 - accuracy: 0.8065\n",
            "Epoch 15/60\n",
            "806/806 - 0s - loss: 0.5691 - accuracy: 0.8002\n",
            "Epoch 16/60\n",
            "806/806 - 0s - loss: 0.5335 - accuracy: 0.8089\n",
            "Epoch 17/60\n",
            "806/806 - 0s - loss: 0.5347 - accuracy: 0.8189\n",
            "Epoch 18/60\n",
            "806/806 - 0s - loss: 0.5214 - accuracy: 0.8201\n",
            "Epoch 19/60\n",
            "806/806 - 0s - loss: 0.5006 - accuracy: 0.8263\n",
            "Epoch 20/60\n",
            "806/806 - 0s - loss: 0.5334 - accuracy: 0.8089\n",
            "Epoch 21/60\n",
            "806/806 - 0s - loss: 0.4823 - accuracy: 0.8263\n",
            "Epoch 22/60\n",
            "806/806 - 0s - loss: 0.4647 - accuracy: 0.8288\n",
            "Epoch 23/60\n",
            "806/806 - 0s - loss: 0.4331 - accuracy: 0.8474\n",
            "Epoch 24/60\n",
            "806/806 - 0s - loss: 0.4393 - accuracy: 0.8387\n",
            "Epoch 25/60\n",
            "806/806 - 0s - loss: 0.4307 - accuracy: 0.8499\n",
            "Epoch 26/60\n",
            "806/806 - 0s - loss: 0.4010 - accuracy: 0.8536\n",
            "Epoch 27/60\n",
            "806/806 - 0s - loss: 0.4086 - accuracy: 0.8486\n",
            "Epoch 28/60\n",
            "806/806 - 0s - loss: 0.3899 - accuracy: 0.8635\n",
            "Epoch 29/60\n",
            "806/806 - 0s - loss: 0.4003 - accuracy: 0.8598\n",
            "Epoch 30/60\n",
            "806/806 - 0s - loss: 0.3733 - accuracy: 0.8685\n",
            "Epoch 31/60\n",
            "806/806 - 0s - loss: 0.3698 - accuracy: 0.8710\n",
            "Epoch 32/60\n",
            "806/806 - 0s - loss: 0.3477 - accuracy: 0.8821\n",
            "Epoch 33/60\n",
            "806/806 - 0s - loss: 0.3405 - accuracy: 0.8772\n",
            "Epoch 34/60\n",
            "806/806 - 0s - loss: 0.3400 - accuracy: 0.8747\n",
            "Epoch 35/60\n",
            "806/806 - 0s - loss: 0.3320 - accuracy: 0.8772\n",
            "Epoch 36/60\n",
            "806/806 - 0s - loss: 0.3128 - accuracy: 0.8821\n",
            "Epoch 37/60\n",
            "806/806 - 0s - loss: 0.3090 - accuracy: 0.8933\n",
            "Epoch 38/60\n",
            "806/806 - 0s - loss: 0.2998 - accuracy: 0.8921\n",
            "Epoch 39/60\n",
            "806/806 - 0s - loss: 0.2950 - accuracy: 0.8896\n",
            "Epoch 40/60\n",
            "806/806 - 0s - loss: 0.2677 - accuracy: 0.8970\n",
            "Epoch 41/60\n",
            "806/806 - 0s - loss: 0.2836 - accuracy: 0.8970\n",
            "Epoch 42/60\n",
            "806/806 - 0s - loss: 0.2655 - accuracy: 0.9007\n",
            "Epoch 43/60\n",
            "806/806 - 0s - loss: 0.2501 - accuracy: 0.9181\n",
            "Epoch 44/60\n",
            "806/806 - 0s - loss: 0.2382 - accuracy: 0.9132\n",
            "Epoch 45/60\n",
            "806/806 - 0s - loss: 0.2431 - accuracy: 0.9169\n",
            "Epoch 46/60\n",
            "806/806 - 0s - loss: 0.2573 - accuracy: 0.9132\n",
            "Epoch 47/60\n",
            "806/806 - 0s - loss: 0.2275 - accuracy: 0.9231\n",
            "Epoch 48/60\n",
            "806/806 - 0s - loss: 0.2154 - accuracy: 0.9243\n",
            "Epoch 49/60\n",
            "806/806 - 0s - loss: 0.2156 - accuracy: 0.9218\n",
            "Epoch 50/60\n",
            "806/806 - 0s - loss: 0.2114 - accuracy: 0.9206\n",
            "Epoch 51/60\n",
            "806/806 - 0s - loss: 0.1978 - accuracy: 0.9404\n",
            "Epoch 52/60\n",
            "806/806 - 0s - loss: 0.2149 - accuracy: 0.9156\n",
            "Epoch 53/60\n",
            "806/806 - 0s - loss: 0.1814 - accuracy: 0.9429\n",
            "Epoch 54/60\n",
            "806/806 - 0s - loss: 0.1663 - accuracy: 0.9529\n",
            "Epoch 55/60\n",
            "806/806 - 0s - loss: 0.1633 - accuracy: 0.9479\n",
            "Epoch 56/60\n",
            "806/806 - 0s - loss: 0.1759 - accuracy: 0.9380\n",
            "Epoch 57/60\n",
            "806/806 - 0s - loss: 0.1466 - accuracy: 0.9553\n",
            "Epoch 58/60\n",
            "806/806 - 0s - loss: 0.1479 - accuracy: 0.9454\n",
            "Epoch 59/60\n",
            "806/806 - 0s - loss: 0.1366 - accuracy: 0.9628\n",
            "Epoch 60/60\n",
            "806/806 - 0s - loss: 0.1610 - accuracy: 0.9479\n",
            "Score for fold 8: loss of 0.9078287897485026; accuracy of 75.28089880943298%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "(806, 10)\n",
            "Train on 806 samples\n",
            "Epoch 1/60\n",
            "806/806 - 0s - loss: 1.6909 - accuracy: 0.4355\n",
            "Epoch 2/60\n",
            "806/806 - 0s - loss: 1.0785 - accuracy: 0.6303\n",
            "Epoch 3/60\n",
            "806/806 - 0s - loss: 0.9075 - accuracy: 0.6886\n",
            "Epoch 4/60\n",
            "806/806 - 0s - loss: 0.8653 - accuracy: 0.6799\n",
            "Epoch 5/60\n",
            "806/806 - 0s - loss: 0.7974 - accuracy: 0.7208\n",
            "Epoch 6/60\n",
            "806/806 - 0s - loss: 0.7518 - accuracy: 0.7333\n",
            "Epoch 7/60\n",
            "806/806 - 0s - loss: 0.7258 - accuracy: 0.7444\n",
            "Epoch 8/60\n",
            "806/806 - 0s - loss: 0.7034 - accuracy: 0.7680\n",
            "Epoch 9/60\n",
            "806/806 - 0s - loss: 0.6616 - accuracy: 0.7705\n",
            "Epoch 10/60\n",
            "806/806 - 0s - loss: 0.6505 - accuracy: 0.7730\n",
            "Epoch 11/60\n",
            "806/806 - 0s - loss: 0.6467 - accuracy: 0.7742\n",
            "Epoch 12/60\n",
            "806/806 - 0s - loss: 0.6161 - accuracy: 0.7854\n",
            "Epoch 13/60\n",
            "806/806 - 0s - loss: 0.5946 - accuracy: 0.7916\n",
            "Epoch 14/60\n",
            "806/806 - 0s - loss: 0.5825 - accuracy: 0.8089\n",
            "Epoch 15/60\n",
            "806/806 - 0s - loss: 0.5662 - accuracy: 0.8027\n",
            "Epoch 16/60\n",
            "806/806 - 0s - loss: 0.5835 - accuracy: 0.7903\n",
            "Epoch 17/60\n",
            "806/806 - 0s - loss: 0.5290 - accuracy: 0.8201\n",
            "Epoch 18/60\n",
            "806/806 - 0s - loss: 0.5375 - accuracy: 0.8052\n",
            "Epoch 19/60\n",
            "806/806 - 0s - loss: 0.4970 - accuracy: 0.8238\n",
            "Epoch 20/60\n",
            "806/806 - 0s - loss: 0.5204 - accuracy: 0.8176\n",
            "Epoch 21/60\n",
            "806/806 - 0s - loss: 0.4763 - accuracy: 0.8337\n",
            "Epoch 22/60\n",
            "806/806 - 0s - loss: 0.4770 - accuracy: 0.8288\n",
            "Epoch 23/60\n",
            "806/806 - 0s - loss: 0.4649 - accuracy: 0.8375\n",
            "Epoch 24/60\n",
            "806/806 - 0s - loss: 0.4588 - accuracy: 0.8350\n",
            "Epoch 25/60\n",
            "806/806 - 0s - loss: 0.4482 - accuracy: 0.8350\n",
            "Epoch 26/60\n",
            "806/806 - 0s - loss: 0.4451 - accuracy: 0.8375\n",
            "Epoch 27/60\n",
            "806/806 - 0s - loss: 0.4198 - accuracy: 0.8474\n",
            "Epoch 28/60\n",
            "806/806 - 0s - loss: 0.4076 - accuracy: 0.8548\n",
            "Epoch 29/60\n",
            "806/806 - 0s - loss: 0.4008 - accuracy: 0.8610\n",
            "Epoch 30/60\n",
            "806/806 - 0s - loss: 0.4029 - accuracy: 0.8586\n",
            "Epoch 31/60\n",
            "806/806 - 0s - loss: 0.3829 - accuracy: 0.8548\n",
            "Epoch 32/60\n",
            "806/806 - 0s - loss: 0.3624 - accuracy: 0.8660\n",
            "Epoch 33/60\n",
            "806/806 - 0s - loss: 0.3785 - accuracy: 0.8598\n",
            "Epoch 34/60\n",
            "806/806 - 0s - loss: 0.3596 - accuracy: 0.8734\n",
            "Epoch 35/60\n",
            "806/806 - 0s - loss: 0.3682 - accuracy: 0.8710\n",
            "Epoch 36/60\n",
            "806/806 - 0s - loss: 0.3242 - accuracy: 0.8784\n",
            "Epoch 37/60\n",
            "806/806 - 0s - loss: 0.3284 - accuracy: 0.8859\n",
            "Epoch 38/60\n",
            "806/806 - 0s - loss: 0.3147 - accuracy: 0.8846\n",
            "Epoch 39/60\n",
            "806/806 - 0s - loss: 0.3252 - accuracy: 0.8772\n",
            "Epoch 40/60\n",
            "806/806 - 0s - loss: 0.3093 - accuracy: 0.8908\n",
            "Epoch 41/60\n",
            "806/806 - 0s - loss: 0.2942 - accuracy: 0.9020\n",
            "Epoch 42/60\n",
            "806/806 - 0s - loss: 0.2805 - accuracy: 0.9007\n",
            "Epoch 43/60\n",
            "806/806 - 0s - loss: 0.2794 - accuracy: 0.8958\n",
            "Epoch 44/60\n",
            "806/806 - 0s - loss: 0.2821 - accuracy: 0.8983\n",
            "Epoch 45/60\n",
            "806/806 - 0s - loss: 0.2583 - accuracy: 0.9107\n",
            "Epoch 46/60\n",
            "806/806 - 0s - loss: 0.2725 - accuracy: 0.9045\n",
            "Epoch 47/60\n",
            "806/806 - 0s - loss: 0.2537 - accuracy: 0.9082\n",
            "Epoch 48/60\n",
            "806/806 - 0s - loss: 0.2551 - accuracy: 0.9057\n",
            "Epoch 49/60\n",
            "806/806 - 0s - loss: 0.2507 - accuracy: 0.9181\n",
            "Epoch 50/60\n",
            "806/806 - 0s - loss: 0.2471 - accuracy: 0.9107\n",
            "Epoch 51/60\n",
            "806/806 - 0s - loss: 0.2397 - accuracy: 0.9032\n",
            "Epoch 52/60\n",
            "806/806 - 0s - loss: 0.2225 - accuracy: 0.9206\n",
            "Epoch 53/60\n",
            "806/806 - 0s - loss: 0.2162 - accuracy: 0.9305\n",
            "Epoch 54/60\n",
            "806/806 - 0s - loss: 0.2087 - accuracy: 0.9280\n",
            "Epoch 55/60\n",
            "806/806 - 0s - loss: 0.2077 - accuracy: 0.9169\n",
            "Epoch 56/60\n",
            "806/806 - 0s - loss: 0.2287 - accuracy: 0.9181\n",
            "Epoch 57/60\n",
            "806/806 - 0s - loss: 0.1908 - accuracy: 0.9243\n",
            "Epoch 58/60\n",
            "806/806 - 0s - loss: 0.2034 - accuracy: 0.9268\n",
            "Epoch 59/60\n",
            "806/806 - 0s - loss: 0.2115 - accuracy: 0.9280\n",
            "Epoch 60/60\n",
            "806/806 - 0s - loss: 0.2102 - accuracy: 0.9206\n",
            "Score for fold 9: loss of 0.9719217218709796; accuracy of 70.78651785850525%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "(806, 10)\n",
            "Train on 806 samples\n",
            "Epoch 1/60\n",
            "806/806 - 0s - loss: 1.6884 - accuracy: 0.4367\n",
            "Epoch 2/60\n",
            "806/806 - 0s - loss: 1.0922 - accuracy: 0.6179\n",
            "Epoch 3/60\n",
            "806/806 - 0s - loss: 0.9226 - accuracy: 0.6799\n",
            "Epoch 4/60\n",
            "806/806 - 0s - loss: 0.8348 - accuracy: 0.7097\n",
            "Epoch 5/60\n",
            "806/806 - 0s - loss: 0.8053 - accuracy: 0.7171\n",
            "Epoch 6/60\n",
            "806/806 - 0s - loss: 0.7438 - accuracy: 0.7407\n",
            "Epoch 7/60\n",
            "806/806 - 0s - loss: 0.7266 - accuracy: 0.7531\n",
            "Epoch 8/60\n",
            "806/806 - 0s - loss: 0.6877 - accuracy: 0.7605\n",
            "Epoch 9/60\n",
            "806/806 - 0s - loss: 0.6737 - accuracy: 0.7643\n",
            "Epoch 10/60\n",
            "806/806 - 0s - loss: 0.6488 - accuracy: 0.7754\n",
            "Epoch 11/60\n",
            "806/806 - 0s - loss: 0.6354 - accuracy: 0.7841\n",
            "Epoch 12/60\n",
            "806/806 - 0s - loss: 0.6042 - accuracy: 0.7928\n",
            "Epoch 13/60\n",
            "806/806 - 0s - loss: 0.6132 - accuracy: 0.8052\n",
            "Epoch 14/60\n",
            "806/806 - 0s - loss: 0.5908 - accuracy: 0.8015\n",
            "Epoch 15/60\n",
            "806/806 - 0s - loss: 0.5628 - accuracy: 0.8139\n",
            "Epoch 16/60\n",
            "806/806 - 0s - loss: 0.5641 - accuracy: 0.8027\n",
            "Epoch 17/60\n",
            "806/806 - 0s - loss: 0.5727 - accuracy: 0.7990\n",
            "Epoch 18/60\n",
            "806/806 - 0s - loss: 0.5259 - accuracy: 0.8201\n",
            "Epoch 19/60\n",
            "806/806 - 0s - loss: 0.5144 - accuracy: 0.8089\n",
            "Epoch 20/60\n",
            "806/806 - 0s - loss: 0.5327 - accuracy: 0.8052\n",
            "Epoch 21/60\n",
            "806/806 - 0s - loss: 0.5012 - accuracy: 0.8176\n",
            "Epoch 22/60\n",
            "806/806 - 0s - loss: 0.4633 - accuracy: 0.8412\n",
            "Epoch 23/60\n",
            "806/806 - 0s - loss: 0.4514 - accuracy: 0.8412\n",
            "Epoch 24/60\n",
            "806/806 - 0s - loss: 0.4635 - accuracy: 0.8387\n",
            "Epoch 25/60\n",
            "806/806 - 0s - loss: 0.4446 - accuracy: 0.8263\n",
            "Epoch 26/60\n",
            "806/806 - 0s - loss: 0.4348 - accuracy: 0.8400\n",
            "Epoch 27/60\n",
            "806/806 - 0s - loss: 0.4070 - accuracy: 0.8548\n",
            "Epoch 28/60\n",
            "806/806 - 0s - loss: 0.4231 - accuracy: 0.8486\n",
            "Epoch 29/60\n",
            "806/806 - 0s - loss: 0.4052 - accuracy: 0.8548\n",
            "Epoch 30/60\n",
            "806/806 - 0s - loss: 0.4076 - accuracy: 0.8573\n",
            "Epoch 31/60\n",
            "806/806 - 0s - loss: 0.3837 - accuracy: 0.8623\n",
            "Epoch 32/60\n",
            "806/806 - 0s - loss: 0.3623 - accuracy: 0.8809\n",
            "Epoch 33/60\n",
            "806/806 - 0s - loss: 0.3574 - accuracy: 0.8685\n",
            "Epoch 34/60\n",
            "806/806 - 0s - loss: 0.3439 - accuracy: 0.8772\n",
            "Epoch 35/60\n",
            "806/806 - 0s - loss: 0.3544 - accuracy: 0.8586\n",
            "Epoch 36/60\n",
            "806/806 - 0s - loss: 0.3466 - accuracy: 0.8660\n",
            "Epoch 37/60\n",
            "806/806 - 0s - loss: 0.3276 - accuracy: 0.8797\n",
            "Epoch 38/60\n",
            "806/806 - 0s - loss: 0.3572 - accuracy: 0.8734\n",
            "Epoch 39/60\n",
            "806/806 - 0s - loss: 0.3084 - accuracy: 0.8958\n",
            "Epoch 40/60\n",
            "806/806 - 0s - loss: 0.3033 - accuracy: 0.8933\n",
            "Epoch 41/60\n",
            "806/806 - 0s - loss: 0.3307 - accuracy: 0.8834\n",
            "Epoch 42/60\n",
            "806/806 - 0s - loss: 0.3065 - accuracy: 0.8883\n",
            "Epoch 43/60\n",
            "806/806 - 0s - loss: 0.2894 - accuracy: 0.8883\n",
            "Epoch 44/60\n",
            "806/806 - 0s - loss: 0.2816 - accuracy: 0.8958\n",
            "Epoch 45/60\n",
            "806/806 - 0s - loss: 0.2782 - accuracy: 0.9045\n",
            "Epoch 46/60\n",
            "806/806 - 0s - loss: 0.2683 - accuracy: 0.9069\n",
            "Epoch 47/60\n",
            "806/806 - 0s - loss: 0.2527 - accuracy: 0.9107\n",
            "Epoch 48/60\n",
            "806/806 - 0s - loss: 0.2643 - accuracy: 0.9007\n",
            "Epoch 49/60\n",
            "806/806 - 0s - loss: 0.2303 - accuracy: 0.9169\n",
            "Epoch 50/60\n",
            "806/806 - 0s - loss: 0.2211 - accuracy: 0.9231\n",
            "Epoch 51/60\n",
            "806/806 - 0s - loss: 0.2364 - accuracy: 0.9107\n",
            "Epoch 52/60\n",
            "806/806 - 0s - loss: 0.2355 - accuracy: 0.9094\n",
            "Epoch 53/60\n",
            "806/806 - 0s - loss: 0.2395 - accuracy: 0.9069\n",
            "Epoch 54/60\n",
            "806/806 - 0s - loss: 0.2080 - accuracy: 0.9280\n",
            "Epoch 55/60\n",
            "806/806 - 0s - loss: 0.2063 - accuracy: 0.9280\n",
            "Epoch 56/60\n",
            "806/806 - 0s - loss: 0.2114 - accuracy: 0.9243\n",
            "Epoch 57/60\n",
            "806/806 - 0s - loss: 0.2325 - accuracy: 0.9119\n",
            "Epoch 58/60\n",
            "806/806 - 0s - loss: 0.2009 - accuracy: 0.9268\n",
            "Epoch 59/60\n",
            "806/806 - 0s - loss: 0.1732 - accuracy: 0.9355\n",
            "Epoch 60/60\n",
            "806/806 - 0s - loss: 0.1635 - accuracy: 0.9417\n",
            "Score for fold 10: loss of 0.9641253305285165; accuracy of 73.03370833396912%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 1.444472368558248 - Accuracy: 67.77777671813965%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 1.0279531485504574 - Accuracy: 73.33333492279053%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6889876113997565 - Accuracy: 81.11110925674438%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.8601152486271328 - Accuracy: 77.77777910232544%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6397473586930169 - Accuracy: 80.0000011920929%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.9547809174891269 - Accuracy: 71.91011309623718%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 1.125199763292677 - Accuracy: 80.89887499809265%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.9078287897485026 - Accuracy: 75.28089880943298%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.9719217218709796 - Accuracy: 70.78651785850525%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.9641253305285165 - Accuracy: 73.03370833396912%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 75.19101142883301 (+- 4.366668284839214)\n",
            "> Loss: 0.9585132258758413\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "577xOM7sL1Ba"
      },
      "source": [
        "### **Results Study**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH3jygiY7i4K"
      },
      "source": [
        "\r\n",
        "Once the Accuracy is determined for both Classifiers it is time to choose between them, as expected from the previous study, Random Forest Classifier is more accurate than Naive Bayes. \r\n",
        "\r\n",
        "From now on Naive Bayes will be ignored and Random Rorest will be studied. The  mean accuracy recieved  during Cross Validation for this Classifier was a 78.39%. At first this could look like a mid Accuracy, though not the expected. But taking in consideration that it classifies over 10 different classes it is not a bad result.\r\n",
        "\r\n",
        "Now it is time to study which are the songs that were wrongly classified, their predicted class and the actual class and afterwards determine if the error is congruent or not.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpEc4vrXOTs_"
      },
      "source": [
        "#Obtain DataFrame with Expected and Actual Values and Song Indexes\r\n",
        "  #From the List of Accuracies kept before we obatined that the iteration with \r\n",
        "  #less accuracy  was the second. Which has the following x_test, X_train, y_train\r\n",
        "  #y_test\r\n",
        "\r\n",
        "x_test=X_Standarized.sample(n=85, random_state=1)\r\n",
        "indexes=x_test.index\r\n",
        "y_test=y.iloc[indexes,:]\r\n",
        "x_train=X_Standarized.drop(indexes)\r\n",
        "y_train=y.drop(indexes)\r\n",
        "\r\n",
        "RFClassifier= RFClassifier.fit(x_train.iloc[:,:-1],y_train.iloc[:,1])\r\n",
        "prediction=RFClassifier.predict(x_test.iloc[:,:-1])\r\n",
        "\r\n",
        "RFAccuracy_prove=(accuracy_score(y_test.iloc[:,1],prediction))\r\n",
        "\r\n",
        "results=pd.DataFrame(y_test)\r\n",
        "results['Predicted Class']=prediction\r\n",
        "results.columns=['Song Index','Expected Class', 'Predicted Class']\r\n",
        "\r\n",
        "#Capturing song name.\r\n",
        "Song_indexes=results['Song Index'].tolist()\r\n",
        "Song_Names=df.loc[Song_indexes,'Name']\r\n",
        "#Adding to DataFrame\r\n",
        "results[\"Song Name\"]=Song_Names.values\r\n",
        "\r\n",
        "#Check if prediction was accurate\r\n",
        "correct=results['Predicted Class'] == results['Expected Class']\r\n",
        "#Adding to DataFrame\r\n",
        "results[\"Correct Prediction?\"]=correct\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYOGfMImX1OU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e43875f-2d40-46f5-f2e0-60524f2b59b4"
      },
      "source": [
        "wrong_pred=results.loc[results['Correct Prediction?']== False]\r\n",
        "wrong_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Song Index</th>\n",
              "      <th>Expected Class</th>\n",
              "      <th>Predicted Class</th>\n",
              "      <th>Song Name</th>\n",
              "      <th>Correct Prediction?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>3605</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Lemonade</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>2642</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>Metal</td>\n",
              "      <td>Adios Reina Mia</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>1020</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Chill</td>\n",
              "      <td>Calentón</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>723</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Cash Machine</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>1114</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Chill</td>\n",
              "      <td>Guille Asesino</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>2185</td>\n",
              "      <td>Metal</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>Biggest &amp; The Best - Remastered version</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>622</th>\n",
              "      <td>4383</td>\n",
              "      <td>Chill</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>First</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>719</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>POWER</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>1012</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>Cheese Jardala</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>734</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Stir Fry</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>3601</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Fancy</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>3085</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Setting the World On Fire (with P!nk)</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>821</th>\n",
              "      <td>6125</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Feel Me</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>2159</td>\n",
              "      <td>Metal</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>The Trooper - 2015 Remaster</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>3591</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Throw Some D's</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>6047</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>Someone Like You (feat. Gia Koka)</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>3093</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Heartless (feat. Morgan Wallen)</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>3593</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>What a Job</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551</th>\n",
              "      <td>4312</td>\n",
              "      <td>Chill</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Once You Go Up - Original Mix</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>1040</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>Dime Que Sí</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>725</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Jordan Belfort</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>6062</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>Paper Thin</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>2653</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>Txus</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>3105</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Every Little Thing</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>2659</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>Chill</td>\n",
              "      <td>Mi Mejor Colega - original</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>3098</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Do I Make You Wanna</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>3597</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Nothin' on You (feat. Bruno Mars)</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>1126</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Jugador del Año</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>2201</td>\n",
              "      <td>Metal</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>River Runs Red</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>4348</td>\n",
              "      <td>Chill</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Reality</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>4314</td>\n",
              "      <td>Chill</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Ragga - Radio Edit</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>4358</td>\n",
              "      <td>Chill</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Baba Yetu - Deep Radio Edit</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>3069</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Dirt Road Anthem</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522</th>\n",
              "      <td>3592</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>So What</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>4340</td>\n",
              "      <td>Chill</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Help Our Souls (Urban Contact Radio Edit)</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>4379</td>\n",
              "      <td>Chill</td>\n",
              "      <td>BlueBallads</td>\n",
              "      <td>Ho Hey</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>2655</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>Metal</td>\n",
              "      <td>La locura</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>3099</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>Metal</td>\n",
              "      <td>Even If It Breaks Your Heart</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Song Index  ... Correct Prediction?\n",
              "535        3605  ...               False\n",
              "375        2642  ...               False\n",
              "81         1020  ...               False\n",
              "6           723  ...               False\n",
              "175        1114  ...               False\n",
              "228        2185  ...               False\n",
              "622        4383  ...               False\n",
              "2           719  ...               False\n",
              "73         1012  ...               False\n",
              "17          734  ...               False\n",
              "531        3601  ...               False\n",
              "435        3085  ...               False\n",
              "821        6125  ...               False\n",
              "202        2159  ...               False\n",
              "521        3591  ...               False\n",
              "743        6047  ...               False\n",
              "443        3093  ...               False\n",
              "523        3593  ...               False\n",
              "551        4312  ...               False\n",
              "101        1040  ...               False\n",
              "8           725  ...               False\n",
              "758        6062  ...               False\n",
              "386        2653  ...               False\n",
              "455        3105  ...               False\n",
              "392        2659  ...               False\n",
              "448        3098  ...               False\n",
              "527        3597  ...               False\n",
              "187        1126  ...               False\n",
              "244        2201  ...               False\n",
              "587        4348  ...               False\n",
              "553        4314  ...               False\n",
              "597        4358  ...               False\n",
              "419        3069  ...               False\n",
              "522        3592  ...               False\n",
              "579        4340  ...               False\n",
              "618        4379  ...               False\n",
              "388        2655  ...               False\n",
              "449        3099  ...               False\n",
              "\n",
              "[38 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qnpg7vWWQwX"
      },
      "source": [
        "from plotnine import *\r\n",
        "Accuracy_df=pd.DataFrame([x*100 for x in NBAccuracy]).astype(float)\r\n",
        "Accuracy_df.columns=[\"Accuracy NB\" ]\r\n",
        "Accuracy_df[\"Accuracy RF\"]=[x*100 for x in RFAccuracy]\r\n",
        "Accuracy_df[\"Accuracy DEEP NN\"]=acc_per_fold\r\n",
        "Accuracy_df[\"Fold\"]=[x for x in range(1,11)]\r\n",
        "Accuracy_df.reset_index\r\n",
        "\r\n",
        "plot_Only_AF_RFE=(\r\n",
        "    ggplot(Accuracy_df)+geom_line(aes(x=\"Fold\",y=\"Accuracy NB\"), color=\"r\")\r\n",
        "    + geom_line(aes(x=\"Fold\",y=\"Accuracy RF\"), color=\"b\")\r\n",
        "    + geom_line(aes(x=\"Fold\",y=\"Accuracy DEEP NN\"), color=\"y\")\r\n",
        "    + labs(x = \"Folds\", y= \"Accuracy\",color = \"Legend\")\r\n",
        ")\r\n",
        "#BLUE   = RANDOM FOREST\r\n",
        "#RED    = NAIVE BAYES\r\n",
        "#YELLOW = DEEP NN'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxjyFQjJdTse"
      },
      "source": [
        "## Features selected by our data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNE5Zn-FV05m"
      },
      "source": [
        "This second model will work with The AudioFeatures selected in the Data-Handling Notebook. These Features were selected by a detailed study taking in consideration: **Correlations, Conditional Entropy** ...\r\n",
        "\r\n",
        "The selected Features were **Acousticness, Danceability, Energy, Instrumentalness, Loudness and Valence**.\r\n",
        "\r\n",
        "The same way as before Playlist will be our Target value and the same list of ten Playlists will be used.\r\n",
        "\r\n",
        "The list of Playlists is:\r\n",
        "\r\n",
        "*   Tuff\r\n",
        "*   BlueBallads\r\n",
        "*   Punk Español\r\n",
        "*   Rap Español(TLob)\r\n",
        "*   Metal\r\n",
        "*   Romanticism\r\n",
        "*   PowerHour\r\n",
        "*   GoldSchool\r\n",
        "*   Chill\r\n",
        "*   CountryNights\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiFuSpEhaqLO"
      },
      "source": [
        "The Following cell will include:\r\n",
        "\r\n",
        "*   **The Sampling Process**: by selecting only the Chosen Playlists\r\n",
        "*   **Definition** of **X** set for the selected Audio Features\r\n",
        "*   **Standarization** of **X** \r\n",
        "*   **Definition** of Playlist as our **Y** value or our target value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuuqR_pea35k"
      },
      "source": [
        "#First we Sort our DataFrame for the selected Playlists in the list\r\n",
        "chosen_features=[\"Acousticness\", \"Danceability\", \"Energy\", \"Instrumentalness\", \"Loudness\" , \"Valence\"]\r\n",
        "chosen_pls = ['Tuff', 'BlueBallads', 'PunkEspanol', 'RapEspanol(TLob)', 'Metal', 'Romanticism', 'PowerHour', 'GoldSchool', 'Chill', 'CountryNights']\r\n",
        "Cut_df = df.loc[df['Playlist'].isin(chosen_pls)]\r\n",
        "\r\n",
        "#Creating our Features\r\n",
        "X=Cut_df.loc[:,chosen_features]\r\n",
        "features_names=X.columns\r\n",
        "indexes=X.index\r\n",
        "\r\n",
        "#Creating our Label \r\n",
        "y=Cut_df.loc[:,[\"Playlist\"]].reset_index()\r\n",
        "y.columns=[\"Song Index\", \"Playlist\"]\r\n",
        "\r\n",
        "#Standarize the features\r\n",
        "X_Standarized=StandardScaler().fit_transform(X)\r\n",
        "X_Standarized=pd.DataFrame(X_Standarized)\r\n",
        "X_Standarized.columns=features_names\r\n",
        "X_Standarized[\"Song Index\"]=indexes\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQSV5XHRcGHr"
      },
      "source": [
        "### **Random Forest Classification.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOBKAdYKcRQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be4658b-1e48-43c9-9fca-25a3e9615106"
      },
      "source": [
        "#Random Forest Classifier\r\n",
        "RFClassifier= RandomForestClassifier(random_state=2)\r\n",
        "RFAccuracy=[]\r\n",
        "\r\n",
        "#CrossValidation\r\n",
        "for i in range(0,10):\r\n",
        " \r\n",
        "  # Generate a print\r\n",
        "  print('------------------------------------------------------------------------')\r\n",
        "  print(f'Training for fold {i+1} ...')\r\n",
        "  \r\n",
        "  #split test set\r\n",
        "  x_test=X_Standarized.sample(n=85, random_state=i)\r\n",
        "  indexes=x_test.index\r\n",
        "  y_test=y.iloc[indexes,:]\r\n",
        "  \r\n",
        "  #split train set\r\n",
        "  x_train=X_Standarized.drop(indexes)\r\n",
        "  y_train=y.drop(indexes)\r\n",
        "\r\n",
        "  #fiting for the train set and predicting with the test set\r\n",
        "  RFClassifier= RFClassifier.fit(x_train.iloc[:,:-1],y_train.iloc[:,1])\r\n",
        "  prediction=RFClassifier.predict(x_test.iloc[:,:-1])\r\n",
        "\r\n",
        "  #Random Forest Classifier Accuracy \r\n",
        "  RFAccuracy.append(accuracy_score(y_test.iloc[:,1],prediction)*100)\r\n",
        "\r\n",
        "  #printing results\r\n",
        "  print(f'Score for fold {i*1}  {RFAccuracy[i]}%')\r\n",
        "\r\n",
        "\r\n",
        "print('------------------------------------------------------------------------')\r\n",
        "print('Average scores for all folds:')\r\n",
        "print(f'> Accuracy: {np.mean(RFAccuracy)} (+- {np.std(RFAccuracy)})')\r\n",
        "print('------------------------------------------------------------------------')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 0  67.05882352941175%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Score for fold 1  55.294117647058826%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Score for fold 2  62.35294117647059%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Score for fold 3  55.294117647058826%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Score for fold 4  74.11764705882354%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Score for fold 5  56.470588235294116%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Score for fold 6  60.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Score for fold 7  56.470588235294116%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Score for fold 8  64.70588235294117%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Score for fold 9  58.82352941176471%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 61.05882352941177 (+- 5.79822189663605)\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQrbpZSEccrY"
      },
      "source": [
        "### **Naive Bayes Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1GweVq7cjNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9428c83-2e8f-49ba-90ec-134a974a0800"
      },
      "source": [
        "#Naive Bayes Classifier \r\n",
        "NBClassifier = GaussianNB()\r\n",
        "NBAccuracy=[]\r\n",
        "\r\n",
        "#CrossValidation\r\n",
        "for i in range(0,10):\r\n",
        "\r\n",
        "  # Generate a print\r\n",
        "  print('------------------------------------------------------------------------')\r\n",
        "  print(f'Training for fold {i+1} ...')\r\n",
        "\r\n",
        "  #split test set\r\n",
        "  x_test=X_Standarized.sample(n=85, random_state=i)\r\n",
        "  indexes=x_test.index\r\n",
        "  y_test=y.iloc[indexes,:]\r\n",
        "  \r\n",
        "  #split train set\r\n",
        "  x_train=X_Standarized.drop(indexes)\r\n",
        "  y_train=y.drop(indexes)\r\n",
        "\r\n",
        "  #fiting for the train set and predicting with the test set\r\n",
        "  NBClassifier= NBClassifier.fit(x_train.iloc[:,:-1],y_train.iloc[:,1])\r\n",
        "  prediction=NBClassifier.predict(x_test.iloc[:,:-1])\r\n",
        "\r\n",
        "  #Naive Bayes Classifier Accuracy \r\n",
        "  NBAccuracy.append(accuracy_score(y_test.iloc[:,1], prediction))\r\n",
        "\r\n",
        "  #printing fold results\r\n",
        "  print(f'Score for fold {i*1}  {NBAccuracy[i]*100}%')\r\n",
        "\r\n",
        "#printing  overall results\r\n",
        "print('------------------------------------------------------------------------')\r\n",
        "print('Average scores for all folds:')\r\n",
        "print(f'> Accuracy: {np.mean(NBAccuracy)} (+- {np.std(NBAccuracy)})')\r\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 0  36.470588235294116%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Score for fold 1  35.294117647058826%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Score for fold 2  47.05882352941176%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Score for fold 3  41.17647058823529%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Score for fold 4  45.88235294117647%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Score for fold 5  31.76470588235294%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Score for fold 6  35.294117647058826%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Score for fold 7  29.411764705882355%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Score for fold 8  41.17647058823529%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Score for fold 9  42.35294117647059%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 0.38588235294117645 (+- 0.05563101372958536)\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oZing_RkbBf"
      },
      "source": [
        "### **Deep Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm-imjpKkbBv"
      },
      "source": [
        "# Scale continuous data \n",
        "features = ['Popularity', 'Acousticness', 'Danceability','Instrumentalness', 'Loudness']\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_continuous = scaler.fit_transform(Cut_df[features[:5]])\n",
        "X_continuous = pd.DataFrame(X_continuous)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WZN8XENkbBw"
      },
      "source": [
        "#### One Hot Encode Target Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD4iYOJNkbBx"
      },
      "source": [
        "### --- One Hot Encode Classes ---\n",
        "# Convert target playlist to one hot encoded playlist for Neural Network\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# First encode target values as integers from string\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Cut_df['Playlist'])\n",
        "hot_Y = encoder.transform(Cut_df['Playlist'])\n",
        "# Then perform one hot encoding\n",
        "hot_Y = np_utils.to_categorical(hot_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t239DDWZkbBx"
      },
      "source": [
        "### --- Split data into test and train data ---\n",
        "x_train, x_test, hot_y_train, hot_y_test = train_test_split(X_continuous, hot_Y, test_size=0.3)\n",
        "\n",
        "# Convert data to friendly arrays of TensorFlow\n",
        "x_train, x_test = x_train.values, x_test.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8IXe0dSkbBx"
      },
      "source": [
        "#### Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKkNFpQfkbBy",
        "outputId": "c65df051-00ff-4e6c-f911-9255cc625ea1"
      },
      "source": [
        "from keras.layers import Dense, Input \n",
        "from keras import Model\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "continuous_inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((hot_y_train,hot_y_test), axis=0)\n",
        "\n",
        "## --- Model configuration ---\n",
        "batch_size = 16\n",
        "epochs = 50\n",
        "\n",
        "## --- Cross Validation ---\n",
        "# Define the K-fold Cross Validator\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(continuous_inputs, targets):\n",
        "  ## --- Model Architecture ---\n",
        "  n_numerical_feats = 5\n",
        "  n_classes = 10 # number of classes/playlists\n",
        "  \n",
        "  # numerical features input\n",
        "  size_input = n_numerical_feats\n",
        "  numerical_input = Input(shape=(size_input), name='numeric_input')\n",
        "\n",
        "  # hidden layers\n",
        "  # we want to make the network abstract the input information by reducing the dimensions\n",
        "  size_hidden1 = int(size_input*32)\n",
        "  size_hidden2 = int(size_input*32) \n",
        "\n",
        "  hidden1 = Dense(size_hidden1, activation='relu')(numerical_input)\n",
        "  hidden2 = Dense(size_hidden2, activation='relu')(hidden1)\n",
        "\n",
        "  # output layers\n",
        "  output = Dense(n_classes, activation='softmax')(hidden2)\n",
        "\n",
        "  # define the model\n",
        "  model = Model(numerical_input, output)\n",
        "  # compile the model\n",
        "  model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'], experimental_run_tf_function=False)\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  print(targets[train].shape)\n",
        "  history = model.fit(continuous_inputs[train] , targets[train],batch_size=batch_size,epochs=epochs,verbose=2)\n",
        "  \n",
        "  # Plot the training vs validation curves to see if we are overfitting the model\n",
        "  #plot_training(history, fold_no)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(continuous_inputs[test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# Provide average scores\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "(805, 10)\n",
            "Train on 805 samples\n",
            "Epoch 1/50\n",
            "805/805 - 0s - loss: 1.9962 - accuracy: 0.3404\n",
            "Epoch 2/50\n",
            "805/805 - 0s - loss: 1.5783 - accuracy: 0.4783\n",
            "Epoch 3/50\n",
            "805/805 - 0s - loss: 1.3792 - accuracy: 0.5354\n",
            "Epoch 4/50\n",
            "805/805 - 0s - loss: 1.2713 - accuracy: 0.5677\n",
            "Epoch 5/50\n",
            "805/805 - 0s - loss: 1.2176 - accuracy: 0.5615\n",
            "Epoch 6/50\n",
            "805/805 - 0s - loss: 1.1690 - accuracy: 0.6000\n",
            "Epoch 7/50\n",
            "805/805 - 0s - loss: 1.1349 - accuracy: 0.6112\n",
            "Epoch 8/50\n",
            "805/805 - 0s - loss: 1.1191 - accuracy: 0.6050\n",
            "Epoch 9/50\n",
            "805/805 - 0s - loss: 1.1081 - accuracy: 0.6211\n",
            "Epoch 10/50\n",
            "805/805 - 0s - loss: 1.1179 - accuracy: 0.6037\n",
            "Epoch 11/50\n",
            "805/805 - 0s - loss: 1.0758 - accuracy: 0.6323\n",
            "Epoch 12/50\n",
            "805/805 - 0s - loss: 1.0621 - accuracy: 0.6224\n",
            "Epoch 13/50\n",
            "805/805 - 0s - loss: 1.0475 - accuracy: 0.6460\n",
            "Epoch 14/50\n",
            "805/805 - 0s - loss: 1.0525 - accuracy: 0.6286\n",
            "Epoch 15/50\n",
            "805/805 - 0s - loss: 1.0511 - accuracy: 0.6335\n",
            "Epoch 16/50\n",
            "805/805 - 0s - loss: 1.0398 - accuracy: 0.6335\n",
            "Epoch 17/50\n",
            "805/805 - 0s - loss: 1.0346 - accuracy: 0.6460\n",
            "Epoch 18/50\n",
            "805/805 - 0s - loss: 1.0213 - accuracy: 0.6398\n",
            "Epoch 19/50\n",
            "805/805 - 0s - loss: 1.0089 - accuracy: 0.6422\n",
            "Epoch 20/50\n",
            "805/805 - 0s - loss: 1.0056 - accuracy: 0.6509\n",
            "Epoch 21/50\n",
            "805/805 - 0s - loss: 0.9971 - accuracy: 0.6534\n",
            "Epoch 22/50\n",
            "805/805 - 0s - loss: 1.0026 - accuracy: 0.6497\n",
            "Epoch 23/50\n",
            "805/805 - 0s - loss: 0.9908 - accuracy: 0.6497\n",
            "Epoch 24/50\n",
            "805/805 - 0s - loss: 0.9755 - accuracy: 0.6621\n",
            "Epoch 25/50\n",
            "805/805 - 0s - loss: 0.9813 - accuracy: 0.6621\n",
            "Epoch 26/50\n",
            "805/805 - 0s - loss: 0.9773 - accuracy: 0.6609\n",
            "Epoch 27/50\n",
            "805/805 - 0s - loss: 0.9845 - accuracy: 0.6534\n",
            "Epoch 28/50\n",
            "805/805 - 0s - loss: 0.9609 - accuracy: 0.6571\n",
            "Epoch 29/50\n",
            "805/805 - 0s - loss: 0.9536 - accuracy: 0.6758\n",
            "Epoch 30/50\n",
            "805/805 - 0s - loss: 0.9432 - accuracy: 0.6646\n",
            "Epoch 31/50\n",
            "805/805 - 0s - loss: 0.9502 - accuracy: 0.6696\n",
            "Epoch 32/50\n",
            "805/805 - 0s - loss: 0.9603 - accuracy: 0.6460\n",
            "Epoch 33/50\n",
            "805/805 - 0s - loss: 0.9379 - accuracy: 0.6571\n",
            "Epoch 34/50\n",
            "805/805 - 0s - loss: 0.9404 - accuracy: 0.6522\n",
            "Epoch 35/50\n",
            "805/805 - 0s - loss: 0.9386 - accuracy: 0.6559\n",
            "Epoch 36/50\n",
            "805/805 - 0s - loss: 0.9345 - accuracy: 0.6634\n",
            "Epoch 37/50\n",
            "805/805 - 0s - loss: 0.9206 - accuracy: 0.6720\n",
            "Epoch 38/50\n",
            "805/805 - 0s - loss: 0.9253 - accuracy: 0.6696\n",
            "Epoch 39/50\n",
            "805/805 - 0s - loss: 0.9283 - accuracy: 0.6559\n",
            "Epoch 40/50\n",
            "805/805 - 0s - loss: 0.9277 - accuracy: 0.6708\n",
            "Epoch 41/50\n",
            "805/805 - 0s - loss: 0.9182 - accuracy: 0.6658\n",
            "Epoch 42/50\n",
            "805/805 - 0s - loss: 0.9239 - accuracy: 0.6845\n",
            "Epoch 43/50\n",
            "805/805 - 0s - loss: 0.9107 - accuracy: 0.6720\n",
            "Epoch 44/50\n",
            "805/805 - 0s - loss: 0.9142 - accuracy: 0.6708\n",
            "Epoch 45/50\n",
            "805/805 - 0s - loss: 0.9064 - accuracy: 0.6783\n",
            "Epoch 46/50\n",
            "805/805 - 0s - loss: 0.9055 - accuracy: 0.6745\n",
            "Epoch 47/50\n",
            "805/805 - 0s - loss: 0.9069 - accuracy: 0.6621\n",
            "Epoch 48/50\n",
            "805/805 - 0s - loss: 0.9029 - accuracy: 0.6658\n",
            "Epoch 49/50\n",
            "805/805 - 0s - loss: 0.9038 - accuracy: 0.6733\n",
            "Epoch 50/50\n",
            "805/805 - 0s - loss: 0.9038 - accuracy: 0.6783\n",
            "Score for fold 1: loss of 1.1581682761510212; accuracy of 56.66666626930237%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "(805, 10)\n",
            "Train on 805 samples\n",
            "Epoch 1/50\n",
            "805/805 - 0s - loss: 2.0642 - accuracy: 0.2907\n",
            "Epoch 2/50\n",
            "805/805 - 0s - loss: 1.6512 - accuracy: 0.4621\n",
            "Epoch 3/50\n",
            "805/805 - 0s - loss: 1.4133 - accuracy: 0.5565\n",
            "Epoch 4/50\n",
            "805/805 - 0s - loss: 1.2786 - accuracy: 0.5814\n",
            "Epoch 5/50\n",
            "805/805 - 0s - loss: 1.1977 - accuracy: 0.6062\n",
            "Epoch 6/50\n",
            "805/805 - 0s - loss: 1.1808 - accuracy: 0.5950\n",
            "Epoch 7/50\n",
            "805/805 - 0s - loss: 1.1396 - accuracy: 0.6211\n",
            "Epoch 8/50\n",
            "805/805 - 0s - loss: 1.1060 - accuracy: 0.6410\n",
            "Epoch 9/50\n",
            "805/805 - 0s - loss: 1.0875 - accuracy: 0.6311\n",
            "Epoch 10/50\n",
            "805/805 - 0s - loss: 1.0821 - accuracy: 0.6323\n",
            "Epoch 11/50\n",
            "805/805 - 0s - loss: 1.0825 - accuracy: 0.6410\n",
            "Epoch 12/50\n",
            "805/805 - 0s - loss: 1.0578 - accuracy: 0.6435\n",
            "Epoch 13/50\n",
            "805/805 - 0s - loss: 1.0485 - accuracy: 0.6609\n",
            "Epoch 14/50\n",
            "805/805 - 0s - loss: 1.0426 - accuracy: 0.6410\n",
            "Epoch 15/50\n",
            "805/805 - 0s - loss: 1.0405 - accuracy: 0.6460\n",
            "Epoch 16/50\n",
            "805/805 - 0s - loss: 1.0318 - accuracy: 0.6460\n",
            "Epoch 17/50\n",
            "805/805 - 0s - loss: 1.0135 - accuracy: 0.6497\n",
            "Epoch 18/50\n",
            "805/805 - 0s - loss: 1.0124 - accuracy: 0.6522\n",
            "Epoch 19/50\n",
            "805/805 - 0s - loss: 1.0106 - accuracy: 0.6509\n",
            "Epoch 20/50\n",
            "805/805 - 0s - loss: 1.0156 - accuracy: 0.6286\n",
            "Epoch 21/50\n",
            "805/805 - 0s - loss: 0.9923 - accuracy: 0.6596\n",
            "Epoch 22/50\n",
            "805/805 - 0s - loss: 0.9749 - accuracy: 0.6683\n",
            "Epoch 23/50\n",
            "805/805 - 0s - loss: 0.9857 - accuracy: 0.6435\n",
            "Epoch 24/50\n",
            "805/805 - 0s - loss: 0.9804 - accuracy: 0.6683\n",
            "Epoch 25/50\n",
            "805/805 - 0s - loss: 0.9756 - accuracy: 0.6472\n",
            "Epoch 26/50\n",
            "805/805 - 0s - loss: 0.9830 - accuracy: 0.6484\n",
            "Epoch 27/50\n",
            "805/805 - 0s - loss: 0.9719 - accuracy: 0.6596\n",
            "Epoch 28/50\n",
            "805/805 - 0s - loss: 0.9588 - accuracy: 0.6621\n",
            "Epoch 29/50\n",
            "805/805 - 0s - loss: 0.9552 - accuracy: 0.6683\n",
            "Epoch 30/50\n",
            "805/805 - 0s - loss: 0.9656 - accuracy: 0.6571\n",
            "Epoch 31/50\n",
            "805/805 - 0s - loss: 0.9529 - accuracy: 0.6559\n",
            "Epoch 32/50\n",
            "805/805 - 0s - loss: 0.9501 - accuracy: 0.6571\n",
            "Epoch 33/50\n",
            "805/805 - 0s - loss: 0.9508 - accuracy: 0.6522\n",
            "Epoch 34/50\n",
            "805/805 - 0s - loss: 0.9565 - accuracy: 0.6534\n",
            "Epoch 35/50\n",
            "805/805 - 0s - loss: 0.9397 - accuracy: 0.6683\n",
            "Epoch 36/50\n",
            "805/805 - 0s - loss: 0.9382 - accuracy: 0.6758\n",
            "Epoch 37/50\n",
            "805/805 - 0s - loss: 0.9396 - accuracy: 0.6596\n",
            "Epoch 38/50\n",
            "805/805 - 0s - loss: 0.9318 - accuracy: 0.6807\n",
            "Epoch 39/50\n",
            "805/805 - 0s - loss: 0.9401 - accuracy: 0.6671\n",
            "Epoch 40/50\n",
            "805/805 - 0s - loss: 0.9267 - accuracy: 0.6696\n",
            "Epoch 41/50\n",
            "805/805 - 0s - loss: 0.9296 - accuracy: 0.6683\n",
            "Epoch 42/50\n",
            "805/805 - 0s - loss: 0.9223 - accuracy: 0.6720\n",
            "Epoch 43/50\n",
            "805/805 - 0s - loss: 0.9399 - accuracy: 0.6683\n",
            "Epoch 44/50\n",
            "805/805 - 0s - loss: 0.9269 - accuracy: 0.6733\n",
            "Epoch 45/50\n",
            "805/805 - 0s - loss: 0.9116 - accuracy: 0.6671\n",
            "Epoch 46/50\n",
            "805/805 - 0s - loss: 0.9175 - accuracy: 0.6720\n",
            "Epoch 47/50\n",
            "805/805 - 0s - loss: 0.9157 - accuracy: 0.6745\n",
            "Epoch 48/50\n",
            "805/805 - 0s - loss: 0.9151 - accuracy: 0.6671\n",
            "Epoch 49/50\n",
            "805/805 - 0s - loss: 0.9224 - accuracy: 0.6696\n",
            "Epoch 50/50\n",
            "805/805 - 0s - loss: 0.9145 - accuracy: 0.6671\n",
            "Score for fold 2: loss of 1.1862080362108018; accuracy of 54.44444417953491%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "(805, 10)\n",
            "Train on 805 samples\n",
            "Epoch 1/50\n",
            "805/805 - 0s - loss: 2.0077 - accuracy: 0.3789\n",
            "Epoch 2/50\n",
            "805/805 - 0s - loss: 1.5483 - accuracy: 0.4932\n",
            "Epoch 3/50\n",
            "805/805 - 0s - loss: 1.3497 - accuracy: 0.5503\n",
            "Epoch 4/50\n",
            "805/805 - 0s - loss: 1.2439 - accuracy: 0.5814\n",
            "Epoch 5/50\n",
            "805/805 - 0s - loss: 1.1788 - accuracy: 0.5938\n",
            "Epoch 6/50\n",
            "805/805 - 0s - loss: 1.1530 - accuracy: 0.5938\n",
            "Epoch 7/50\n",
            "805/805 - 0s - loss: 1.1075 - accuracy: 0.6224\n",
            "Epoch 8/50\n",
            "805/805 - 0s - loss: 1.0857 - accuracy: 0.6211\n",
            "Epoch 9/50\n",
            "805/805 - 0s - loss: 1.0706 - accuracy: 0.6261\n",
            "Epoch 10/50\n",
            "805/805 - 0s - loss: 1.0794 - accuracy: 0.6311\n",
            "Epoch 11/50\n",
            "805/805 - 0s - loss: 1.0448 - accuracy: 0.6398\n",
            "Epoch 12/50\n",
            "805/805 - 0s - loss: 1.0401 - accuracy: 0.6348\n",
            "Epoch 13/50\n",
            "805/805 - 0s - loss: 1.0201 - accuracy: 0.6522\n",
            "Epoch 14/50\n",
            "805/805 - 0s - loss: 1.0506 - accuracy: 0.6373\n",
            "Epoch 15/50\n",
            "805/805 - 0s - loss: 1.0312 - accuracy: 0.6248\n",
            "Epoch 16/50\n",
            "805/805 - 0s - loss: 1.0019 - accuracy: 0.6522\n",
            "Epoch 17/50\n",
            "805/805 - 0s - loss: 1.0101 - accuracy: 0.6472\n",
            "Epoch 18/50\n",
            "805/805 - 0s - loss: 0.9874 - accuracy: 0.6534\n",
            "Epoch 19/50\n",
            "805/805 - 0s - loss: 0.9917 - accuracy: 0.6435\n",
            "Epoch 20/50\n",
            "805/805 - 0s - loss: 0.9786 - accuracy: 0.6596\n",
            "Epoch 21/50\n",
            "805/805 - 0s - loss: 0.9749 - accuracy: 0.6584\n",
            "Epoch 22/50\n",
            "805/805 - 0s - loss: 0.9627 - accuracy: 0.6708\n",
            "Epoch 23/50\n",
            "805/805 - 0s - loss: 0.9615 - accuracy: 0.6596\n",
            "Epoch 24/50\n",
            "805/805 - 0s - loss: 0.9532 - accuracy: 0.6720\n",
            "Epoch 25/50\n",
            "805/805 - 0s - loss: 0.9454 - accuracy: 0.6534\n",
            "Epoch 26/50\n",
            "805/805 - 0s - loss: 0.9443 - accuracy: 0.6522\n",
            "Epoch 27/50\n",
            "805/805 - 0s - loss: 0.9418 - accuracy: 0.6584\n",
            "Epoch 28/50\n",
            "805/805 - 0s - loss: 0.9396 - accuracy: 0.6596\n",
            "Epoch 29/50\n",
            "805/805 - 0s - loss: 0.9422 - accuracy: 0.6559\n",
            "Epoch 30/50\n",
            "805/805 - 0s - loss: 0.9273 - accuracy: 0.6621\n",
            "Epoch 31/50\n",
            "805/805 - 0s - loss: 0.9318 - accuracy: 0.6584\n",
            "Epoch 32/50\n",
            "805/805 - 0s - loss: 0.9310 - accuracy: 0.6584\n",
            "Epoch 33/50\n",
            "805/805 - 0s - loss: 0.9250 - accuracy: 0.6584\n",
            "Epoch 34/50\n",
            "805/805 - 0s - loss: 0.9184 - accuracy: 0.6596\n",
            "Epoch 35/50\n",
            "805/805 - 0s - loss: 0.9256 - accuracy: 0.6571\n",
            "Epoch 36/50\n",
            "805/805 - 0s - loss: 0.9171 - accuracy: 0.6683\n",
            "Epoch 37/50\n",
            "805/805 - 0s - loss: 0.9118 - accuracy: 0.6522\n",
            "Epoch 38/50\n",
            "805/805 - 0s - loss: 0.9101 - accuracy: 0.6720\n",
            "Epoch 39/50\n",
            "805/805 - 0s - loss: 0.9048 - accuracy: 0.6621\n",
            "Epoch 40/50\n",
            "805/805 - 0s - loss: 0.9012 - accuracy: 0.6634\n",
            "Epoch 41/50\n",
            "805/805 - 0s - loss: 0.9036 - accuracy: 0.6671\n",
            "Epoch 42/50\n",
            "805/805 - 0s - loss: 0.8949 - accuracy: 0.6795\n",
            "Epoch 43/50\n",
            "805/805 - 0s - loss: 0.8901 - accuracy: 0.6609\n",
            "Epoch 44/50\n",
            "805/805 - 0s - loss: 0.8779 - accuracy: 0.6646\n",
            "Epoch 45/50\n",
            "805/805 - 0s - loss: 0.8873 - accuracy: 0.6671\n",
            "Epoch 46/50\n",
            "805/805 - 0s - loss: 0.8796 - accuracy: 0.6634\n",
            "Epoch 47/50\n",
            "805/805 - 0s - loss: 0.8729 - accuracy: 0.6696\n",
            "Epoch 48/50\n",
            "805/805 - 0s - loss: 0.8766 - accuracy: 0.6720\n",
            "Epoch 49/50\n",
            "805/805 - 0s - loss: 0.8697 - accuracy: 0.6720\n",
            "Epoch 50/50\n",
            "805/805 - 0s - loss: 0.8726 - accuracy: 0.6733\n",
            "Score for fold 3: loss of 1.2153847376505533; accuracy of 63.333332538604736%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "(805, 10)\n",
            "Train on 805 samples\n",
            "Epoch 1/50\n",
            "805/805 - 0s - loss: 2.0051 - accuracy: 0.3354\n",
            "Epoch 2/50\n",
            "805/805 - 0s - loss: 1.6025 - accuracy: 0.4422\n",
            "Epoch 3/50\n",
            "805/805 - 0s - loss: 1.3998 - accuracy: 0.5366\n",
            "Epoch 4/50\n",
            "805/805 - 0s - loss: 1.2950 - accuracy: 0.5702\n",
            "Epoch 5/50\n",
            "805/805 - 0s - loss: 1.2263 - accuracy: 0.5839\n",
            "Epoch 6/50\n",
            "805/805 - 0s - loss: 1.1935 - accuracy: 0.5888\n",
            "Epoch 7/50\n",
            "805/805 - 0s - loss: 1.1653 - accuracy: 0.6062\n",
            "Epoch 8/50\n",
            "805/805 - 0s - loss: 1.1342 - accuracy: 0.6124\n",
            "Epoch 9/50\n",
            "805/805 - 0s - loss: 1.1302 - accuracy: 0.5975\n",
            "Epoch 10/50\n",
            "805/805 - 0s - loss: 1.1062 - accuracy: 0.6174\n",
            "Epoch 11/50\n",
            "805/805 - 0s - loss: 1.0844 - accuracy: 0.6286\n",
            "Epoch 12/50\n",
            "805/805 - 0s - loss: 1.0839 - accuracy: 0.6224\n",
            "Epoch 13/50\n",
            "805/805 - 0s - loss: 1.0608 - accuracy: 0.6398\n",
            "Epoch 14/50\n",
            "805/805 - 0s - loss: 1.0595 - accuracy: 0.6447\n",
            "Epoch 15/50\n",
            "805/805 - 0s - loss: 1.0490 - accuracy: 0.6348\n",
            "Epoch 16/50\n",
            "805/805 - 0s - loss: 1.0421 - accuracy: 0.6410\n",
            "Epoch 17/50\n",
            "805/805 - 0s - loss: 1.0389 - accuracy: 0.6311\n",
            "Epoch 18/50\n",
            "805/805 - 0s - loss: 1.0335 - accuracy: 0.6298\n",
            "Epoch 19/50\n",
            "805/805 - 0s - loss: 1.0213 - accuracy: 0.6360\n",
            "Epoch 20/50\n",
            "805/805 - 0s - loss: 1.0090 - accuracy: 0.6385\n",
            "Epoch 21/50\n",
            "805/805 - 0s - loss: 1.0173 - accuracy: 0.6497\n",
            "Epoch 22/50\n",
            "805/805 - 0s - loss: 1.0018 - accuracy: 0.6460\n",
            "Epoch 23/50\n",
            "805/805 - 0s - loss: 0.9958 - accuracy: 0.6435\n",
            "Epoch 24/50\n",
            "805/805 - 0s - loss: 0.9839 - accuracy: 0.6509\n",
            "Epoch 25/50\n",
            "805/805 - 0s - loss: 0.9891 - accuracy: 0.6522\n",
            "Epoch 26/50\n",
            "805/805 - 0s - loss: 0.9815 - accuracy: 0.6596\n",
            "Epoch 27/50\n",
            "805/805 - 0s - loss: 0.9947 - accuracy: 0.6422\n",
            "Epoch 28/50\n",
            "805/805 - 0s - loss: 0.9740 - accuracy: 0.6634\n",
            "Epoch 29/50\n",
            "805/805 - 0s - loss: 0.9798 - accuracy: 0.6484\n",
            "Epoch 30/50\n",
            "805/805 - 0s - loss: 0.9744 - accuracy: 0.6398\n",
            "Epoch 31/50\n",
            "805/805 - 0s - loss: 0.9633 - accuracy: 0.6559\n",
            "Epoch 32/50\n",
            "805/805 - 0s - loss: 0.9604 - accuracy: 0.6534\n",
            "Epoch 33/50\n",
            "805/805 - 0s - loss: 0.9521 - accuracy: 0.6596\n",
            "Epoch 34/50\n",
            "805/805 - 0s - loss: 0.9729 - accuracy: 0.6484\n",
            "Epoch 35/50\n",
            "805/805 - 0s - loss: 0.9463 - accuracy: 0.6646\n",
            "Epoch 36/50\n",
            "805/805 - 0s - loss: 0.9533 - accuracy: 0.6584\n",
            "Epoch 37/50\n",
            "805/805 - 0s - loss: 0.9429 - accuracy: 0.6609\n",
            "Epoch 38/50\n",
            "805/805 - 0s - loss: 0.9440 - accuracy: 0.6559\n",
            "Epoch 39/50\n",
            "805/805 - 0s - loss: 0.9411 - accuracy: 0.6596\n",
            "Epoch 40/50\n",
            "805/805 - 0s - loss: 0.9468 - accuracy: 0.6596\n",
            "Epoch 41/50\n",
            "805/805 - 0s - loss: 0.9352 - accuracy: 0.6596\n",
            "Epoch 42/50\n",
            "805/805 - 0s - loss: 0.9300 - accuracy: 0.6634\n",
            "Epoch 43/50\n",
            "805/805 - 0s - loss: 0.9218 - accuracy: 0.6584\n",
            "Epoch 44/50\n",
            "805/805 - 0s - loss: 0.9281 - accuracy: 0.6571\n",
            "Epoch 45/50\n",
            "805/805 - 0s - loss: 0.9331 - accuracy: 0.6596\n",
            "Epoch 46/50\n",
            "805/805 - 0s - loss: 0.9213 - accuracy: 0.6609\n",
            "Epoch 47/50\n",
            "805/805 - 0s - loss: 0.9304 - accuracy: 0.6609\n",
            "Epoch 48/50\n",
            "805/805 - 0s - loss: 0.9121 - accuracy: 0.6484\n",
            "Epoch 49/50\n",
            "805/805 - 0s - loss: 0.9485 - accuracy: 0.6509\n",
            "Epoch 50/50\n",
            "805/805 - 0s - loss: 0.9090 - accuracy: 0.6596\n",
            "Score for fold 4: loss of 0.8603709088431464; accuracy of 66.66666865348816%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "(805, 10)\n",
            "Train on 805 samples\n",
            "Epoch 1/50\n",
            "805/805 - 0s - loss: 2.0025 - accuracy: 0.3627\n",
            "Epoch 2/50\n",
            "805/805 - 0s - loss: 1.5931 - accuracy: 0.4584\n",
            "Epoch 3/50\n",
            "805/805 - 0s - loss: 1.3905 - accuracy: 0.5193\n",
            "Epoch 4/50\n",
            "805/805 - 0s - loss: 1.2687 - accuracy: 0.5764\n",
            "Epoch 5/50\n",
            "805/805 - 0s - loss: 1.2205 - accuracy: 0.5714\n",
            "Epoch 6/50\n",
            "805/805 - 0s - loss: 1.1696 - accuracy: 0.6087\n",
            "Epoch 7/50\n",
            "805/805 - 0s - loss: 1.1422 - accuracy: 0.6199\n",
            "Epoch 8/50\n",
            "805/805 - 0s - loss: 1.1173 - accuracy: 0.6149\n",
            "Epoch 9/50\n",
            "805/805 - 0s - loss: 1.1064 - accuracy: 0.6186\n",
            "Epoch 10/50\n",
            "805/805 - 0s - loss: 1.0956 - accuracy: 0.6311\n",
            "Epoch 11/50\n",
            "805/805 - 0s - loss: 1.0734 - accuracy: 0.6385\n",
            "Epoch 12/50\n",
            "805/805 - 0s - loss: 1.0824 - accuracy: 0.6335\n",
            "Epoch 13/50\n",
            "805/805 - 0s - loss: 1.0709 - accuracy: 0.6087\n",
            "Epoch 14/50\n",
            "805/805 - 0s - loss: 1.0460 - accuracy: 0.6410\n",
            "Epoch 15/50\n",
            "805/805 - 0s - loss: 1.0464 - accuracy: 0.6385\n",
            "Epoch 16/50\n",
            "805/805 - 0s - loss: 1.0344 - accuracy: 0.6497\n",
            "Epoch 17/50\n",
            "805/805 - 0s - loss: 1.0183 - accuracy: 0.6373\n",
            "Epoch 18/50\n",
            "805/805 - 0s - loss: 1.0308 - accuracy: 0.6335\n",
            "Epoch 19/50\n",
            "805/805 - 0s - loss: 1.0117 - accuracy: 0.6273\n",
            "Epoch 20/50\n",
            "805/805 - 0s - loss: 1.0119 - accuracy: 0.6248\n",
            "Epoch 21/50\n",
            "805/805 - 0s - loss: 1.0047 - accuracy: 0.6360\n",
            "Epoch 22/50\n",
            "805/805 - 0s - loss: 0.9950 - accuracy: 0.6472\n",
            "Epoch 23/50\n",
            "805/805 - 0s - loss: 0.9855 - accuracy: 0.6385\n",
            "Epoch 24/50\n",
            "805/805 - 0s - loss: 0.9913 - accuracy: 0.6484\n",
            "Epoch 25/50\n",
            "805/805 - 0s - loss: 0.9838 - accuracy: 0.6385\n",
            "Epoch 26/50\n",
            "805/805 - 0s - loss: 0.9764 - accuracy: 0.6410\n",
            "Epoch 27/50\n",
            "805/805 - 0s - loss: 0.9746 - accuracy: 0.6398\n",
            "Epoch 28/50\n",
            "805/805 - 0s - loss: 0.9617 - accuracy: 0.6484\n",
            "Epoch 29/50\n",
            "805/805 - 0s - loss: 0.9625 - accuracy: 0.6547\n",
            "Epoch 30/50\n",
            "805/805 - 0s - loss: 0.9691 - accuracy: 0.6335\n",
            "Epoch 31/50\n",
            "805/805 - 0s - loss: 0.9803 - accuracy: 0.6286\n",
            "Epoch 32/50\n",
            "805/805 - 0s - loss: 0.9650 - accuracy: 0.6410\n",
            "Epoch 33/50\n",
            "805/805 - 0s - loss: 0.9445 - accuracy: 0.6509\n",
            "Epoch 34/50\n",
            "805/805 - 0s - loss: 0.9544 - accuracy: 0.6484\n",
            "Epoch 35/50\n",
            "805/805 - 0s - loss: 0.9547 - accuracy: 0.6385\n",
            "Epoch 36/50\n",
            "805/805 - 0s - loss: 0.9571 - accuracy: 0.6571\n",
            "Epoch 37/50\n",
            "805/805 - 0s - loss: 0.9379 - accuracy: 0.6385\n",
            "Epoch 38/50\n",
            "805/805 - 0s - loss: 0.9426 - accuracy: 0.6559\n",
            "Epoch 39/50\n",
            "805/805 - 0s - loss: 0.9402 - accuracy: 0.6497\n",
            "Epoch 40/50\n",
            "805/805 - 0s - loss: 0.9411 - accuracy: 0.6522\n",
            "Epoch 41/50\n",
            "805/805 - 0s - loss: 0.9364 - accuracy: 0.6472\n",
            "Epoch 42/50\n",
            "805/805 - 0s - loss: 0.9298 - accuracy: 0.6547\n",
            "Epoch 43/50\n",
            "805/805 - 0s - loss: 0.9209 - accuracy: 0.6522\n",
            "Epoch 44/50\n",
            "805/805 - 0s - loss: 0.9268 - accuracy: 0.6683\n",
            "Epoch 45/50\n",
            "805/805 - 0s - loss: 0.9205 - accuracy: 0.6596\n",
            "Epoch 46/50\n",
            "805/805 - 0s - loss: 0.9169 - accuracy: 0.6534\n",
            "Epoch 47/50\n",
            "805/805 - 0s - loss: 0.9137 - accuracy: 0.6522\n",
            "Epoch 48/50\n",
            "805/805 - 0s - loss: 0.9174 - accuracy: 0.6683\n",
            "Epoch 49/50\n",
            "805/805 - 0s - loss: 0.9048 - accuracy: 0.6472\n",
            "Epoch 50/50\n",
            "805/805 - 0s - loss: 0.9095 - accuracy: 0.6696\n",
            "Score for fold 5: loss of 0.9550993574990166; accuracy of 71.11111283302307%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "(806, 10)\n",
            "Train on 806 samples\n",
            "Epoch 1/50\n",
            "806/806 - 0s - loss: 2.0219 - accuracy: 0.3598\n",
            "Epoch 2/50\n",
            "806/806 - 0s - loss: 1.6440 - accuracy: 0.4541\n",
            "Epoch 3/50\n",
            "806/806 - 0s - loss: 1.4359 - accuracy: 0.5037\n",
            "Epoch 4/50\n",
            "806/806 - 0s - loss: 1.3143 - accuracy: 0.5571\n",
            "Epoch 5/50\n",
            "806/806 - 0s - loss: 1.2316 - accuracy: 0.5856\n",
            "Epoch 6/50\n",
            "806/806 - 0s - loss: 1.1968 - accuracy: 0.5732\n",
            "Epoch 7/50\n",
            "806/806 - 0s - loss: 1.1623 - accuracy: 0.6092\n",
            "Epoch 8/50\n",
            "806/806 - 0s - loss: 1.1354 - accuracy: 0.6042\n",
            "Epoch 9/50\n",
            "806/806 - 0s - loss: 1.1074 - accuracy: 0.6241\n",
            "Epoch 10/50\n",
            "806/806 - 0s - loss: 1.0997 - accuracy: 0.6055\n",
            "Epoch 11/50\n",
            "806/806 - 0s - loss: 1.0948 - accuracy: 0.6203\n",
            "Epoch 12/50\n",
            "806/806 - 0s - loss: 1.0608 - accuracy: 0.6352\n",
            "Epoch 13/50\n",
            "806/806 - 0s - loss: 1.0552 - accuracy: 0.6253\n",
            "Epoch 14/50\n",
            "806/806 - 0s - loss: 1.0545 - accuracy: 0.6228\n",
            "Epoch 15/50\n",
            "806/806 - 0s - loss: 1.0368 - accuracy: 0.6439\n",
            "Epoch 16/50\n",
            "806/806 - 0s - loss: 1.0314 - accuracy: 0.6290\n",
            "Epoch 17/50\n",
            "806/806 - 0s - loss: 1.0321 - accuracy: 0.6365\n",
            "Epoch 18/50\n",
            "806/806 - 0s - loss: 1.0141 - accuracy: 0.6414\n",
            "Epoch 19/50\n",
            "806/806 - 0s - loss: 0.9955 - accuracy: 0.6501\n",
            "Epoch 20/50\n",
            "806/806 - 0s - loss: 0.9993 - accuracy: 0.6563\n",
            "Epoch 21/50\n",
            "806/806 - 0s - loss: 0.9974 - accuracy: 0.6439\n",
            "Epoch 22/50\n",
            "806/806 - 0s - loss: 0.9823 - accuracy: 0.6464\n",
            "Epoch 23/50\n",
            "806/806 - 0s - loss: 0.9778 - accuracy: 0.6452\n",
            "Epoch 24/50\n",
            "806/806 - 0s - loss: 0.9910 - accuracy: 0.6365\n",
            "Epoch 25/50\n",
            "806/806 - 0s - loss: 0.9697 - accuracy: 0.6464\n",
            "Epoch 26/50\n",
            "806/806 - 0s - loss: 0.9663 - accuracy: 0.6489\n",
            "Epoch 27/50\n",
            "806/806 - 0s - loss: 0.9577 - accuracy: 0.6501\n",
            "Epoch 28/50\n",
            "806/806 - 0s - loss: 0.9586 - accuracy: 0.6514\n",
            "Epoch 29/50\n",
            "806/806 - 0s - loss: 0.9638 - accuracy: 0.6352\n",
            "Epoch 30/50\n",
            "806/806 - 0s - loss: 0.9561 - accuracy: 0.6452\n",
            "Epoch 31/50\n",
            "806/806 - 0s - loss: 0.9500 - accuracy: 0.6501\n",
            "Epoch 32/50\n",
            "806/806 - 0s - loss: 0.9445 - accuracy: 0.6675\n",
            "Epoch 33/50\n",
            "806/806 - 0s - loss: 0.9461 - accuracy: 0.6476\n",
            "Epoch 34/50\n",
            "806/806 - 0s - loss: 0.9468 - accuracy: 0.6514\n",
            "Epoch 35/50\n",
            "806/806 - 0s - loss: 0.9482 - accuracy: 0.6501\n",
            "Epoch 36/50\n",
            "806/806 - 0s - loss: 0.9352 - accuracy: 0.6526\n",
            "Epoch 37/50\n",
            "806/806 - 0s - loss: 0.9227 - accuracy: 0.6675\n",
            "Epoch 38/50\n",
            "806/806 - 0s - loss: 0.9255 - accuracy: 0.6538\n",
            "Epoch 39/50\n",
            "806/806 - 0s - loss: 0.9217 - accuracy: 0.6613\n",
            "Epoch 40/50\n",
            "806/806 - 0s - loss: 0.9170 - accuracy: 0.6551\n",
            "Epoch 41/50\n",
            "806/806 - 0s - loss: 0.9231 - accuracy: 0.6563\n",
            "Epoch 42/50\n",
            "806/806 - 0s - loss: 0.9150 - accuracy: 0.6650\n",
            "Epoch 43/50\n",
            "806/806 - 0s - loss: 0.9398 - accuracy: 0.6514\n",
            "Epoch 44/50\n",
            "806/806 - 0s - loss: 0.9119 - accuracy: 0.6588\n",
            "Epoch 45/50\n",
            "806/806 - 0s - loss: 0.9000 - accuracy: 0.6749\n",
            "Epoch 46/50\n",
            "806/806 - 0s - loss: 0.9012 - accuracy: 0.6687\n",
            "Epoch 47/50\n",
            "806/806 - 0s - loss: 0.8978 - accuracy: 0.6650\n",
            "Epoch 48/50\n",
            "806/806 - 0s - loss: 0.8863 - accuracy: 0.6762\n",
            "Epoch 49/50\n",
            "806/806 - 0s - loss: 0.8983 - accuracy: 0.6663\n",
            "Epoch 50/50\n",
            "806/806 - 0s - loss: 0.8890 - accuracy: 0.6687\n",
            "Score for fold 6: loss of 1.1993493350704065; accuracy of 65.16854166984558%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "(806, 10)\n",
            "Train on 806 samples\n",
            "Epoch 1/50\n",
            "806/806 - 0s - loss: 1.9661 - accuracy: 0.3424\n",
            "Epoch 2/50\n",
            "806/806 - 0s - loss: 1.5800 - accuracy: 0.4727\n",
            "Epoch 3/50\n",
            "806/806 - 0s - loss: 1.3852 - accuracy: 0.5261\n",
            "Epoch 4/50\n",
            "806/806 - 0s - loss: 1.2919 - accuracy: 0.5645\n",
            "Epoch 5/50\n",
            "806/806 - 0s - loss: 1.2133 - accuracy: 0.5906\n",
            "Epoch 6/50\n",
            "806/806 - 0s - loss: 1.1859 - accuracy: 0.5918\n",
            "Epoch 7/50\n",
            "806/806 - 0s - loss: 1.1516 - accuracy: 0.6154\n",
            "Epoch 8/50\n",
            "806/806 - 0s - loss: 1.1362 - accuracy: 0.5980\n",
            "Epoch 9/50\n",
            "806/806 - 0s - loss: 1.1174 - accuracy: 0.6141\n",
            "Epoch 10/50\n",
            "806/806 - 0s - loss: 1.1082 - accuracy: 0.6141\n",
            "Epoch 11/50\n",
            "806/806 - 0s - loss: 1.0975 - accuracy: 0.6104\n",
            "Epoch 12/50\n",
            "806/806 - 0s - loss: 1.0854 - accuracy: 0.6241\n",
            "Epoch 13/50\n",
            "806/806 - 0s - loss: 1.0781 - accuracy: 0.6216\n",
            "Epoch 14/50\n",
            "806/806 - 0s - loss: 1.0774 - accuracy: 0.6390\n",
            "Epoch 15/50\n",
            "806/806 - 0s - loss: 1.0605 - accuracy: 0.6290\n",
            "Epoch 16/50\n",
            "806/806 - 0s - loss: 1.0626 - accuracy: 0.6377\n",
            "Epoch 17/50\n",
            "806/806 - 0s - loss: 1.0346 - accuracy: 0.6427\n",
            "Epoch 18/50\n",
            "806/806 - 0s - loss: 1.0495 - accuracy: 0.6303\n",
            "Epoch 19/50\n",
            "806/806 - 0s - loss: 1.0518 - accuracy: 0.6390\n",
            "Epoch 20/50\n",
            "806/806 - 0s - loss: 1.0270 - accuracy: 0.6377\n",
            "Epoch 21/50\n",
            "806/806 - 0s - loss: 1.0146 - accuracy: 0.6352\n",
            "Epoch 22/50\n",
            "806/806 - 0s - loss: 1.0174 - accuracy: 0.6365\n",
            "Epoch 23/50\n",
            "806/806 - 0s - loss: 1.0112 - accuracy: 0.6365\n",
            "Epoch 24/50\n",
            "806/806 - 0s - loss: 0.9971 - accuracy: 0.6352\n",
            "Epoch 25/50\n",
            "806/806 - 0s - loss: 1.0114 - accuracy: 0.6414\n",
            "Epoch 26/50\n",
            "806/806 - 0s - loss: 0.9958 - accuracy: 0.6452\n",
            "Epoch 27/50\n",
            "806/806 - 0s - loss: 0.9912 - accuracy: 0.6452\n",
            "Epoch 28/50\n",
            "806/806 - 0s - loss: 0.9868 - accuracy: 0.6414\n",
            "Epoch 29/50\n",
            "806/806 - 0s - loss: 0.9725 - accuracy: 0.6551\n",
            "Epoch 30/50\n",
            "806/806 - 0s - loss: 0.9872 - accuracy: 0.6563\n",
            "Epoch 31/50\n",
            "806/806 - 0s - loss: 0.9832 - accuracy: 0.6427\n",
            "Epoch 32/50\n",
            "806/806 - 0s - loss: 0.9686 - accuracy: 0.6514\n",
            "Epoch 33/50\n",
            "806/806 - 0s - loss: 0.9747 - accuracy: 0.6514\n",
            "Epoch 34/50\n",
            "806/806 - 0s - loss: 0.9605 - accuracy: 0.6452\n",
            "Epoch 35/50\n",
            "806/806 - 0s - loss: 0.9670 - accuracy: 0.6427\n",
            "Epoch 36/50\n",
            "806/806 - 0s - loss: 0.9531 - accuracy: 0.6514\n",
            "Epoch 37/50\n",
            "806/806 - 0s - loss: 0.9568 - accuracy: 0.6514\n",
            "Epoch 38/50\n",
            "806/806 - 0s - loss: 0.9576 - accuracy: 0.6501\n",
            "Epoch 39/50\n",
            "806/806 - 0s - loss: 0.9485 - accuracy: 0.6613\n",
            "Epoch 40/50\n",
            "806/806 - 0s - loss: 0.9526 - accuracy: 0.6576\n",
            "Epoch 41/50\n",
            "806/806 - 0s - loss: 0.9562 - accuracy: 0.6538\n",
            "Epoch 42/50\n",
            "806/806 - 0s - loss: 0.9492 - accuracy: 0.6476\n",
            "Epoch 43/50\n",
            "806/806 - 0s - loss: 0.9387 - accuracy: 0.6650\n",
            "Epoch 44/50\n",
            "806/806 - 0s - loss: 0.9388 - accuracy: 0.6489\n",
            "Epoch 45/50\n",
            "806/806 - 0s - loss: 0.9483 - accuracy: 0.6427\n",
            "Epoch 46/50\n",
            "806/806 - 0s - loss: 0.9268 - accuracy: 0.6650\n",
            "Epoch 47/50\n",
            "806/806 - 0s - loss: 0.9404 - accuracy: 0.6538\n",
            "Epoch 48/50\n",
            "806/806 - 0s - loss: 0.9239 - accuracy: 0.6638\n",
            "Epoch 49/50\n",
            "806/806 - 0s - loss: 0.9285 - accuracy: 0.6638\n",
            "Epoch 50/50\n",
            "806/806 - 0s - loss: 0.9088 - accuracy: 0.6650\n",
            "Score for fold 7: loss of 0.9086779289031297; accuracy of 69.66292262077332%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "(806, 10)\n",
            "Train on 806 samples\n",
            "Epoch 1/50\n",
            "806/806 - 0s - loss: 2.0598 - accuracy: 0.3610\n",
            "Epoch 2/50\n",
            "806/806 - 0s - loss: 1.6045 - accuracy: 0.4467\n",
            "Epoch 3/50\n",
            "806/806 - 0s - loss: 1.3883 - accuracy: 0.5186\n",
            "Epoch 4/50\n",
            "806/806 - 0s - loss: 1.2911 - accuracy: 0.5533\n",
            "Epoch 5/50\n",
            "806/806 - 0s - loss: 1.2231 - accuracy: 0.5831\n",
            "Epoch 6/50\n",
            "806/806 - 0s - loss: 1.1816 - accuracy: 0.5844\n",
            "Epoch 7/50\n",
            "806/806 - 0s - loss: 1.1387 - accuracy: 0.6228\n",
            "Epoch 8/50\n",
            "806/806 - 0s - loss: 1.1297 - accuracy: 0.6117\n",
            "Epoch 9/50\n",
            "806/806 - 0s - loss: 1.1111 - accuracy: 0.6278\n",
            "Epoch 10/50\n",
            "806/806 - 0s - loss: 1.0946 - accuracy: 0.6390\n",
            "Epoch 11/50\n",
            "806/806 - 0s - loss: 1.0843 - accuracy: 0.6129\n",
            "Epoch 12/50\n",
            "806/806 - 0s - loss: 1.0792 - accuracy: 0.6290\n",
            "Epoch 13/50\n",
            "806/806 - 0s - loss: 1.0704 - accuracy: 0.6290\n",
            "Epoch 14/50\n",
            "806/806 - 0s - loss: 1.0597 - accuracy: 0.6266\n",
            "Epoch 15/50\n",
            "806/806 - 0s - loss: 1.0443 - accuracy: 0.6464\n",
            "Epoch 16/50\n",
            "806/806 - 0s - loss: 1.0439 - accuracy: 0.6464\n",
            "Epoch 17/50\n",
            "806/806 - 0s - loss: 1.0373 - accuracy: 0.6377\n",
            "Epoch 18/50\n",
            "806/806 - 0s - loss: 1.0276 - accuracy: 0.6377\n",
            "Epoch 19/50\n",
            "806/806 - 0s - loss: 1.0184 - accuracy: 0.6377\n",
            "Epoch 20/50\n",
            "806/806 - 0s - loss: 1.0187 - accuracy: 0.6328\n",
            "Epoch 21/50\n",
            "806/806 - 0s - loss: 1.0298 - accuracy: 0.6303\n",
            "Epoch 22/50\n",
            "806/806 - 0s - loss: 1.0144 - accuracy: 0.6390\n",
            "Epoch 23/50\n",
            "806/806 - 0s - loss: 1.0184 - accuracy: 0.6402\n",
            "Epoch 24/50\n",
            "806/806 - 0s - loss: 0.9918 - accuracy: 0.6489\n",
            "Epoch 25/50\n",
            "806/806 - 0s - loss: 0.9911 - accuracy: 0.6402\n",
            "Epoch 26/50\n",
            "806/806 - 0s - loss: 0.9832 - accuracy: 0.6402\n",
            "Epoch 27/50\n",
            "806/806 - 0s - loss: 0.9871 - accuracy: 0.6365\n",
            "Epoch 28/50\n",
            "806/806 - 0s - loss: 0.9733 - accuracy: 0.6576\n",
            "Epoch 29/50\n",
            "806/806 - 0s - loss: 0.9661 - accuracy: 0.6476\n",
            "Epoch 30/50\n",
            "806/806 - 0s - loss: 0.9672 - accuracy: 0.6427\n",
            "Epoch 31/50\n",
            "806/806 - 0s - loss: 0.9710 - accuracy: 0.6538\n",
            "Epoch 32/50\n",
            "806/806 - 0s - loss: 0.9578 - accuracy: 0.6563\n",
            "Epoch 33/50\n",
            "806/806 - 0s - loss: 0.9574 - accuracy: 0.6414\n",
            "Epoch 34/50\n",
            "806/806 - 0s - loss: 0.9716 - accuracy: 0.6365\n",
            "Epoch 35/50\n",
            "806/806 - 0s - loss: 0.9708 - accuracy: 0.6563\n",
            "Epoch 36/50\n",
            "806/806 - 0s - loss: 0.9502 - accuracy: 0.6576\n",
            "Epoch 37/50\n",
            "806/806 - 0s - loss: 0.9557 - accuracy: 0.6501\n",
            "Epoch 38/50\n",
            "806/806 - 0s - loss: 0.9450 - accuracy: 0.6588\n",
            "Epoch 39/50\n",
            "806/806 - 0s - loss: 0.9410 - accuracy: 0.6501\n",
            "Epoch 40/50\n",
            "806/806 - 0s - loss: 0.9413 - accuracy: 0.6625\n",
            "Epoch 41/50\n",
            "806/806 - 0s - loss: 0.9265 - accuracy: 0.6576\n",
            "Epoch 42/50\n",
            "806/806 - 0s - loss: 0.9419 - accuracy: 0.6588\n",
            "Epoch 43/50\n",
            "806/806 - 0s - loss: 0.9332 - accuracy: 0.6663\n",
            "Epoch 44/50\n",
            "806/806 - 0s - loss: 0.9389 - accuracy: 0.6538\n",
            "Epoch 45/50\n",
            "806/806 - 0s - loss: 0.9220 - accuracy: 0.6588\n",
            "Epoch 46/50\n",
            "806/806 - 0s - loss: 0.9326 - accuracy: 0.6687\n",
            "Epoch 47/50\n",
            "806/806 - 0s - loss: 0.9150 - accuracy: 0.6700\n",
            "Epoch 48/50\n",
            "806/806 - 0s - loss: 0.9194 - accuracy: 0.6700\n",
            "Epoch 49/50\n",
            "806/806 - 0s - loss: 0.9155 - accuracy: 0.6489\n",
            "Epoch 50/50\n",
            "806/806 - 0s - loss: 0.9191 - accuracy: 0.6600\n",
            "Score for fold 8: loss of 0.9182009817509169; accuracy of 68.53932738304138%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "(806, 10)\n",
            "Train on 806 samples\n",
            "Epoch 1/50\n",
            "806/806 - 0s - loss: 2.0175 - accuracy: 0.3908\n",
            "Epoch 2/50\n",
            "806/806 - 0s - loss: 1.5876 - accuracy: 0.4541\n",
            "Epoch 3/50\n",
            "806/806 - 0s - loss: 1.3845 - accuracy: 0.5248\n",
            "Epoch 4/50\n",
            "806/806 - 0s - loss: 1.2923 - accuracy: 0.5422\n",
            "Epoch 5/50\n",
            "806/806 - 0s - loss: 1.2187 - accuracy: 0.6055\n",
            "Epoch 6/50\n",
            "806/806 - 0s - loss: 1.1922 - accuracy: 0.5906\n",
            "Epoch 7/50\n",
            "806/806 - 0s - loss: 1.1694 - accuracy: 0.6117\n",
            "Epoch 8/50\n",
            "806/806 - 0s - loss: 1.1471 - accuracy: 0.6154\n",
            "Epoch 9/50\n",
            "806/806 - 0s - loss: 1.1199 - accuracy: 0.6154\n",
            "Epoch 10/50\n",
            "806/806 - 0s - loss: 1.1189 - accuracy: 0.6228\n",
            "Epoch 11/50\n",
            "806/806 - 0s - loss: 1.1126 - accuracy: 0.6179\n",
            "Epoch 12/50\n",
            "806/806 - 0s - loss: 1.0923 - accuracy: 0.6278\n",
            "Epoch 13/50\n",
            "806/806 - 0s - loss: 1.0800 - accuracy: 0.6365\n",
            "Epoch 14/50\n",
            "806/806 - 0s - loss: 1.0707 - accuracy: 0.6328\n",
            "Epoch 15/50\n",
            "806/806 - 0s - loss: 1.0699 - accuracy: 0.6328\n",
            "Epoch 16/50\n",
            "806/806 - 0s - loss: 1.0682 - accuracy: 0.6266\n",
            "Epoch 17/50\n",
            "806/806 - 0s - loss: 1.0466 - accuracy: 0.6489\n",
            "Epoch 18/50\n",
            "806/806 - 0s - loss: 1.0453 - accuracy: 0.6563\n",
            "Epoch 19/50\n",
            "806/806 - 0s - loss: 1.0426 - accuracy: 0.6365\n",
            "Epoch 20/50\n",
            "806/806 - 0s - loss: 1.0475 - accuracy: 0.6303\n",
            "Epoch 21/50\n",
            "806/806 - 0s - loss: 1.0321 - accuracy: 0.6290\n",
            "Epoch 22/50\n",
            "806/806 - 0s - loss: 1.0314 - accuracy: 0.6365\n",
            "Epoch 23/50\n",
            "806/806 - 0s - loss: 1.0190 - accuracy: 0.6514\n",
            "Epoch 24/50\n",
            "806/806 - 0s - loss: 1.0082 - accuracy: 0.6526\n",
            "Epoch 25/50\n",
            "806/806 - 0s - loss: 0.9983 - accuracy: 0.6427\n",
            "Epoch 26/50\n",
            "806/806 - 0s - loss: 0.9970 - accuracy: 0.6538\n",
            "Epoch 27/50\n",
            "806/806 - 0s - loss: 0.9823 - accuracy: 0.6427\n",
            "Epoch 28/50\n",
            "806/806 - 0s - loss: 0.9897 - accuracy: 0.6650\n",
            "Epoch 29/50\n",
            "806/806 - 0s - loss: 0.9782 - accuracy: 0.6563\n",
            "Epoch 30/50\n",
            "806/806 - 0s - loss: 0.9707 - accuracy: 0.6501\n",
            "Epoch 31/50\n",
            "806/806 - 0s - loss: 0.9851 - accuracy: 0.6414\n",
            "Epoch 32/50\n",
            "806/806 - 0s - loss: 0.9706 - accuracy: 0.6625\n",
            "Epoch 33/50\n",
            "806/806 - 0s - loss: 0.9719 - accuracy: 0.6489\n",
            "Epoch 34/50\n",
            "806/806 - 0s - loss: 0.9556 - accuracy: 0.6514\n",
            "Epoch 35/50\n",
            "806/806 - 0s - loss: 0.9659 - accuracy: 0.6650\n",
            "Epoch 36/50\n",
            "806/806 - 0s - loss: 0.9470 - accuracy: 0.6501\n",
            "Epoch 37/50\n",
            "806/806 - 0s - loss: 0.9491 - accuracy: 0.6514\n",
            "Epoch 38/50\n",
            "806/806 - 0s - loss: 0.9470 - accuracy: 0.6563\n",
            "Epoch 39/50\n",
            "806/806 - 0s - loss: 0.9448 - accuracy: 0.6452\n",
            "Epoch 40/50\n",
            "806/806 - 0s - loss: 0.9446 - accuracy: 0.6576\n",
            "Epoch 41/50\n",
            "806/806 - 0s - loss: 0.9379 - accuracy: 0.6613\n",
            "Epoch 42/50\n",
            "806/806 - 0s - loss: 0.9343 - accuracy: 0.6563\n",
            "Epoch 43/50\n",
            "806/806 - 0s - loss: 0.9419 - accuracy: 0.6476\n",
            "Epoch 44/50\n",
            "806/806 - 0s - loss: 0.9338 - accuracy: 0.6650\n",
            "Epoch 45/50\n",
            "806/806 - 0s - loss: 0.9271 - accuracy: 0.6476\n",
            "Epoch 46/50\n",
            "806/806 - 0s - loss: 0.9334 - accuracy: 0.6600\n",
            "Epoch 47/50\n",
            "806/806 - 0s - loss: 0.9187 - accuracy: 0.6476\n",
            "Epoch 48/50\n",
            "806/806 - 0s - loss: 0.9145 - accuracy: 0.6600\n",
            "Epoch 49/50\n",
            "806/806 - 0s - loss: 0.9193 - accuracy: 0.6563\n",
            "Epoch 50/50\n",
            "806/806 - 0s - loss: 0.9124 - accuracy: 0.6638\n",
            "Score for fold 9: loss of 0.8706057661035088; accuracy of 65.16854166984558%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "(806, 10)\n",
            "Train on 806 samples\n",
            "Epoch 1/50\n",
            "806/806 - 0s - loss: 1.9660 - accuracy: 0.3697\n",
            "Epoch 2/50\n",
            "806/806 - 0s - loss: 1.5682 - accuracy: 0.4615\n",
            "Epoch 3/50\n",
            "806/806 - 0s - loss: 1.3659 - accuracy: 0.5484\n",
            "Epoch 4/50\n",
            "806/806 - 0s - loss: 1.2510 - accuracy: 0.5806\n",
            "Epoch 5/50\n",
            "806/806 - 0s - loss: 1.1943 - accuracy: 0.6154\n",
            "Epoch 6/50\n",
            "806/806 - 0s - loss: 1.1526 - accuracy: 0.6216\n",
            "Epoch 7/50\n",
            "806/806 - 0s - loss: 1.1341 - accuracy: 0.6079\n",
            "Epoch 8/50\n",
            "806/806 - 0s - loss: 1.1264 - accuracy: 0.6067\n",
            "Epoch 9/50\n",
            "806/806 - 0s - loss: 1.0972 - accuracy: 0.6203\n",
            "Epoch 10/50\n",
            "806/806 - 0s - loss: 1.0804 - accuracy: 0.6216\n",
            "Epoch 11/50\n",
            "806/806 - 0s - loss: 1.0844 - accuracy: 0.6340\n",
            "Epoch 12/50\n",
            "806/806 - 0s - loss: 1.0619 - accuracy: 0.6390\n",
            "Epoch 13/50\n",
            "806/806 - 0s - loss: 1.0501 - accuracy: 0.6476\n",
            "Epoch 14/50\n",
            "806/806 - 0s - loss: 1.0412 - accuracy: 0.6514\n",
            "Epoch 15/50\n",
            "806/806 - 0s - loss: 1.0395 - accuracy: 0.6414\n",
            "Epoch 16/50\n",
            "806/806 - 0s - loss: 1.0320 - accuracy: 0.6414\n",
            "Epoch 17/50\n",
            "806/806 - 0s - loss: 1.0158 - accuracy: 0.6538\n",
            "Epoch 18/50\n",
            "806/806 - 0s - loss: 1.0203 - accuracy: 0.6427\n",
            "Epoch 19/50\n",
            "806/806 - 0s - loss: 0.9992 - accuracy: 0.6514\n",
            "Epoch 20/50\n",
            "806/806 - 0s - loss: 1.0041 - accuracy: 0.6551\n",
            "Epoch 21/50\n",
            "806/806 - 0s - loss: 0.9987 - accuracy: 0.6526\n",
            "Epoch 22/50\n",
            "806/806 - 0s - loss: 0.9859 - accuracy: 0.6687\n",
            "Epoch 23/50\n",
            "806/806 - 0s - loss: 0.9868 - accuracy: 0.6588\n",
            "Epoch 24/50\n",
            "806/806 - 0s - loss: 0.9759 - accuracy: 0.6501\n",
            "Epoch 25/50\n",
            "806/806 - 0s - loss: 0.9731 - accuracy: 0.6650\n",
            "Epoch 26/50\n",
            "806/806 - 0s - loss: 0.9752 - accuracy: 0.6638\n",
            "Epoch 27/50\n",
            "806/806 - 0s - loss: 0.9628 - accuracy: 0.6588\n",
            "Epoch 28/50\n",
            "806/806 - 0s - loss: 0.9510 - accuracy: 0.6787\n",
            "Epoch 29/50\n",
            "806/806 - 0s - loss: 0.9540 - accuracy: 0.6762\n",
            "Epoch 30/50\n",
            "806/806 - 0s - loss: 0.9454 - accuracy: 0.6675\n",
            "Epoch 31/50\n",
            "806/806 - 0s - loss: 0.9653 - accuracy: 0.6576\n",
            "Epoch 32/50\n",
            "806/806 - 0s - loss: 0.9526 - accuracy: 0.6625\n",
            "Epoch 33/50\n",
            "806/806 - 0s - loss: 0.9359 - accuracy: 0.6675\n",
            "Epoch 34/50\n",
            "806/806 - 0s - loss: 0.9504 - accuracy: 0.6625\n",
            "Epoch 35/50\n",
            "806/806 - 0s - loss: 0.9486 - accuracy: 0.6563\n",
            "Epoch 36/50\n",
            "806/806 - 0s - loss: 0.9392 - accuracy: 0.6625\n",
            "Epoch 37/50\n",
            "806/806 - 0s - loss: 0.9451 - accuracy: 0.6625\n",
            "Epoch 38/50\n",
            "806/806 - 0s - loss: 0.9176 - accuracy: 0.6712\n",
            "Epoch 39/50\n",
            "806/806 - 0s - loss: 0.9240 - accuracy: 0.6725\n",
            "Epoch 40/50\n",
            "806/806 - 0s - loss: 0.9162 - accuracy: 0.6774\n",
            "Epoch 41/50\n",
            "806/806 - 0s - loss: 0.9229 - accuracy: 0.6638\n",
            "Epoch 42/50\n",
            "806/806 - 0s - loss: 0.9109 - accuracy: 0.6749\n",
            "Epoch 43/50\n",
            "806/806 - 0s - loss: 0.9118 - accuracy: 0.6749\n",
            "Epoch 44/50\n",
            "806/806 - 0s - loss: 0.9221 - accuracy: 0.6613\n",
            "Epoch 45/50\n",
            "806/806 - 0s - loss: 0.9069 - accuracy: 0.6787\n",
            "Epoch 46/50\n",
            "806/806 - 0s - loss: 0.8980 - accuracy: 0.6737\n",
            "Epoch 47/50\n",
            "806/806 - 0s - loss: 0.8944 - accuracy: 0.6737\n",
            "Epoch 48/50\n",
            "806/806 - 0s - loss: 0.8910 - accuracy: 0.6799\n",
            "Epoch 49/50\n",
            "806/806 - 0s - loss: 0.9052 - accuracy: 0.6737\n",
            "Epoch 50/50\n",
            "806/806 - 0s - loss: 0.8935 - accuracy: 0.6749\n",
            "Score for fold 10: loss of 1.0953372790572349; accuracy of 53.93258333206177%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 1.1581682761510212 - Accuracy: 56.66666626930237%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 1.1862080362108018 - Accuracy: 54.44444417953491%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 1.2153847376505533 - Accuracy: 63.333332538604736%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.8603709088431464 - Accuracy: 66.66666865348816%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.9550993574990166 - Accuracy: 71.11111283302307%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 1.1993493350704065 - Accuracy: 65.16854166984558%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.9086779289031297 - Accuracy: 69.66292262077332%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.9182009817509169 - Accuracy: 68.53932738304138%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.8706057661035088 - Accuracy: 65.16854166984558%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 1.0953372790572349 - Accuracy: 53.93258333206177%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 63.46941411495209 (+- 5.976225841605745)\n",
            "> Loss: 1.0367402607239735\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf8KFDDWdR7K"
      },
      "source": [
        "### **Results Study**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCy2Z_Q9dR7M"
      },
      "source": [
        "\r\n",
        "Once the Accuracy is determined for both Classifiers it is time to choose between them, as expected from the previous study, Random Forest Classifier is more accurate than Naive Bayes. \r\n",
        "\r\n",
        "From now on Naive Bayes will be ignored and Random Rorest will be studied. The  mean accuracy recieved  during Cross Validation for this Classifier was a 61.71%. The result is lower than with all the AudioFeatures.\r\n",
        "\r\n",
        "Now it is time to study which are the songs that were wrongly classified, their predicted class and the actual class and afterwards determine if the error is congruent or not.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "YCLwIeSUdR7Q",
        "outputId": "adf53f8b-08a6-4554-d03d-939280d851f2"
      },
      "source": [
        "#Obtain DataFrame with Expected and Actual Values and Song Indexes\r\n",
        "  #From the List of Accuracies kept before we obatined that the iteration with \r\n",
        "  #less accuracy  was the second. Which has the following x_test, X_train, y_train\r\n",
        "  #y_test\r\n",
        "\r\n",
        "x_test=X_Standarized.sample(n=85, random_state=1)\r\n",
        "indexes=x_test.index\r\n",
        "y_test=y.iloc[indexes,:]\r\n",
        "x_train=X_Standarized.drop(indexes)\r\n",
        "y_train=y.drop(indexes)\r\n",
        "\r\n",
        "\r\n",
        "RFClassifier= RFClassifier.fit(x_train.iloc[:,:-1],y_train.iloc[:,1])\r\n",
        "prediction=RFClassifier.predict(x_test.iloc[:,:-1])\r\n",
        "RFAccuracy_prove=(accuracy_score(y_test.iloc[:,1],prediction))\r\n",
        "\r\n",
        "results=pd.DataFrame(y_test)\r\n",
        "results['Predicted Class']=prediction\r\n",
        "results.columns=['Song Index','Expected Class', 'Predicted Class']\r\n",
        "\r\n",
        "#Capturing song name.\r\n",
        "Song_indexes=results['Song Index'].tolist()\r\n",
        "Song_Names=df.loc[Song_indexes,'Name']\r\n",
        "#Adding to DataFrame\r\n",
        "results[\"Song Name\"]=Song_Names.values\r\n",
        "\r\n",
        "#Check if prediction was accurate\r\n",
        "correct=results['Predicted Class'] == results['Expected Class']\r\n",
        "#Adding to DataFrame\r\n",
        "results[\"Correct Prediction?\"]=correct\r\n",
        "\r\n",
        "results\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Song Index</th>\n",
              "      <th>Expected Class</th>\n",
              "      <th>Predicted Class</th>\n",
              "      <th>Song Name</th>\n",
              "      <th>Correct Prediction?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>3605</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Lemonade</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>973</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Mi Crucifixión</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>2642</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>Metal</td>\n",
              "      <td>Adios Reina Mia</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>1020</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Chill</td>\n",
              "      <td>Calentón</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>723</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Cash Machine</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>613</th>\n",
              "      <td>4374</td>\n",
              "      <td>Chill</td>\n",
              "      <td>Chill</td>\n",
              "      <td>Grey Lynn Park</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>711</th>\n",
              "      <td>4632</td>\n",
              "      <td>Romanticism</td>\n",
              "      <td>Romanticism</td>\n",
              "      <td>Six Piano Pieces, Op. 118: Intermezzo in A Maj...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>4379</td>\n",
              "      <td>Chill</td>\n",
              "      <td>BlueBallads</td>\n",
              "      <td>Ho Hey</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>2655</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>Metal</td>\n",
              "      <td>La locura</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>3099</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>Metal</td>\n",
              "      <td>Even If It Breaks Your Heart</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>85 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Song Index  ... Correct Prediction?\n",
              "535        3605  ...               False\n",
              "34          973  ...                True\n",
              "375        2642  ...               False\n",
              "81         1020  ...               False\n",
              "6           723  ...               False\n",
              "..          ...  ...                 ...\n",
              "613        4374  ...                True\n",
              "711        4632  ...                True\n",
              "618        4379  ...               False\n",
              "388        2655  ...               False\n",
              "449        3099  ...               False\n",
              "\n",
              "[85 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tq6ZQkPTdR7U",
        "outputId": "427f5e4b-470f-4252-81bd-9214244f396e"
      },
      "source": [
        "wrong_pred=results.loc[results['Correct Prediction?']== False]\r\n",
        "wrong_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Song Index</th>\n",
              "      <th>Expected Class</th>\n",
              "      <th>Predicted Class</th>\n",
              "      <th>Song Name</th>\n",
              "      <th>Correct Prediction?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>3605</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Lemonade</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>2642</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>Metal</td>\n",
              "      <td>Adios Reina Mia</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>1020</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Chill</td>\n",
              "      <td>Calentón</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>723</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Cash Machine</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>1114</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Chill</td>\n",
              "      <td>Guille Asesino</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>2185</td>\n",
              "      <td>Metal</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>Biggest &amp; The Best - Remastered version</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>622</th>\n",
              "      <td>4383</td>\n",
              "      <td>Chill</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>First</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>719</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>POWER</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>1012</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>Cheese Jardala</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>734</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Stir Fry</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>3601</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Fancy</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>3085</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Setting the World On Fire (with P!nk)</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>821</th>\n",
              "      <td>6125</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Feel Me</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>2159</td>\n",
              "      <td>Metal</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>The Trooper - 2015 Remaster</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>3591</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Throw Some D's</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>6047</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>Someone Like You (feat. Gia Koka)</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>3093</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Heartless (feat. Morgan Wallen)</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>3593</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>What a Job</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551</th>\n",
              "      <td>4312</td>\n",
              "      <td>Chill</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Once You Go Up - Original Mix</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>1040</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>Dime Que Sí</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>725</td>\n",
              "      <td>Tuff</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Jordan Belfort</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>6062</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>Paper Thin</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>2653</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>Txus</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>3105</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Every Little Thing</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>2659</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>Chill</td>\n",
              "      <td>Mi Mejor Colega - original</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>3098</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Do I Make You Wanna</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>3597</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Nothin' on You (feat. Bruno Mars)</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>1126</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Jugador del Año</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>2201</td>\n",
              "      <td>Metal</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>River Runs Red</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>4348</td>\n",
              "      <td>Chill</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Reality</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>4314</td>\n",
              "      <td>Chill</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Ragga - Radio Edit</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>4358</td>\n",
              "      <td>Chill</td>\n",
              "      <td>PowerHour</td>\n",
              "      <td>Baba Yetu - Deep Radio Edit</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>3069</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Dirt Road Anthem</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522</th>\n",
              "      <td>3592</td>\n",
              "      <td>GoldSchool</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>So What</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>4340</td>\n",
              "      <td>Chill</td>\n",
              "      <td>RapEspanol(TLob)</td>\n",
              "      <td>Help Our Souls (Urban Contact Radio Edit)</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>4379</td>\n",
              "      <td>Chill</td>\n",
              "      <td>BlueBallads</td>\n",
              "      <td>Ho Hey</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>2655</td>\n",
              "      <td>PunkEspanol</td>\n",
              "      <td>Metal</td>\n",
              "      <td>La locura</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>3099</td>\n",
              "      <td>CountryNights</td>\n",
              "      <td>Metal</td>\n",
              "      <td>Even If It Breaks Your Heart</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Song Index  ... Correct Prediction?\n",
              "535        3605  ...               False\n",
              "375        2642  ...               False\n",
              "81         1020  ...               False\n",
              "6           723  ...               False\n",
              "175        1114  ...               False\n",
              "228        2185  ...               False\n",
              "622        4383  ...               False\n",
              "2           719  ...               False\n",
              "73         1012  ...               False\n",
              "17          734  ...               False\n",
              "531        3601  ...               False\n",
              "435        3085  ...               False\n",
              "821        6125  ...               False\n",
              "202        2159  ...               False\n",
              "521        3591  ...               False\n",
              "743        6047  ...               False\n",
              "443        3093  ...               False\n",
              "523        3593  ...               False\n",
              "551        4312  ...               False\n",
              "101        1040  ...               False\n",
              "8           725  ...               False\n",
              "758        6062  ...               False\n",
              "386        2653  ...               False\n",
              "455        3105  ...               False\n",
              "392        2659  ...               False\n",
              "448        3098  ...               False\n",
              "527        3597  ...               False\n",
              "187        1126  ...               False\n",
              "244        2201  ...               False\n",
              "587        4348  ...               False\n",
              "553        4314  ...               False\n",
              "597        4358  ...               False\n",
              "419        3069  ...               False\n",
              "522        3592  ...               False\n",
              "579        4340  ...               False\n",
              "618        4379  ...               False\n",
              "388        2655  ...               False\n",
              "449        3099  ...               False\n",
              "\n",
              "[38 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "wkfp17GDU0q_",
        "outputId": "b5cbbdc6-4667-4dde-d803-246e52c618bf"
      },
      "source": [
        "\r\n",
        "Accuracy_df=pd.DataFrame()\r\n",
        "Accuracy_df=pd.DataFrame([x*100 for x in NBAccuracy]).astype(float)\r\n",
        "Accuracy_df.columns=[\"Accuracy NB\" ]\r\n",
        "Accuracy_df[\"Accuracy RF\"]=RFAccuracy\r\n",
        "Accuracy_df[\"Accuracy DEEP NN\"]=acc_per_fold\r\n",
        "Accuracy_df[\"Fold\"]=[x for x in range(1,11)]\r\n",
        "Accuracy_df.reset_index\r\n",
        "\r\n",
        "plot_Own_AF_RFE=(\r\n",
        "    ggplot(Accuracy_df)+geom_line(aes(x=\"Fold\",y=\"Accuracy NB\"), color=\"r\")\r\n",
        "    + geom_line(aes(x=\"Fold\",y=\"Accuracy RF\"), color=\"b\")\r\n",
        "    + geom_line(aes(x=\"Fold\",y=\"Accuracy DEEP NN\"), color=\"y\")\r\n",
        "    + labs(x = \"Folds\", y= \"Accuracy\",color = \"Legend\")\r\n",
        ")\r\n",
        "#BLUE   = RANDOM FOREST\r\n",
        "#RED    = NAIVE BAYES\r\n",
        "#YELLOW = DEEP NN'''\r\n",
        "plot_Own_AF_RFE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ggplot: (-9223363296542371566)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZAlJiRhWiCm"
      },
      "source": [
        "# Models with Continuous and Categorical Features "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHnaIA9qU7Lw"
      },
      "source": [
        "In this section two different studies will be selected in order to classify different Songs into Playlist. \r\n",
        "\r\n",
        "\r\n",
        "The first one will avoid and forget our analytical analysis from the Data Handling note book and will take in cosideration a Recursive Feature Elimination used to select the most important features in our set.\r\n",
        "\r\n",
        "\r\n",
        "Afterwards, A model with only the selected features from our previous study will be construted.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tSczOPCVTg9"
      },
      "source": [
        "Following a **Classification** model will be implemented using three different Classifying techniques:\r\n",
        "\r\n",
        "*   **Random Forest Classifier**\r\n",
        "*   **Naive Bayes**\r\n",
        "*   **Deep Neural Network**\r\n",
        "\r\n",
        "\r\n",
        "In order to do so, \"**Playlist**\" will be selected as the Target Label. From the previous study in the Data-Handling notebook, a list of the 10 most different Playlists is obtained. \r\n",
        "\r\n",
        "Each song in the Test set will be classified into one of the different Playlists by taking in consideration  whole set of **Audio Features** from that song and the Artist as a categorical feature. As previously explained, Audio Features are the Characteristics from a song, which are retrieved from Spotify´s API.\r\n",
        "\r\n",
        "The list of Playlists is:\r\n",
        "\r\n",
        "*   Tuff\r\n",
        "*   BlueBallads\r\n",
        "*   Punk Español\r\n",
        "*   Rap Español(TLob)\r\n",
        "*   Metal\r\n",
        "*   Romanticism\r\n",
        "*   PowerHour\r\n",
        "*   GoldSchool\r\n",
        "*   Chill\r\n",
        "*   CountryNights\r\n",
        "\r\n",
        "In order to do the classification an **embedding** will be constructed for the Artist Column, which will be later added to the continious features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T9fShz4PPXu"
      },
      "source": [
        "def plot_training(history, embedding_dim, batch_size):\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title(f'Model Accuracy embedding_dims={embedding_dim}, batch_size={batch_size}')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title(f'Model Loss embedding_dims={embedding_dim}, batch_size={batch_size}')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DylnxUQAEHbq"
      },
      "source": [
        "### --- Use playlists chose in Data Analysis section ---\n",
        "chosen_pls = ['Tuff', 'BlueBallads', 'PunkEspanol', 'RapEspanol(TLob)', 'Metal', 'Romanticism', 'PowerHour', 'GoldSchool', 'Chill', 'CountryNights']\n",
        "df_10_playlist = df.loc[df['Playlist'].isin(chosen_pls)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExWhzXqrs3yR"
      },
      "source": [
        "## Features selected by RFE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itBD8TtEjn0C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "0fa572f4-8a44-497a-a1f4-1315c96ee624"
      },
      "source": [
        "# Scale continuous data\n",
        "non_wanted_features = ['Name', 'Artist', 'Playlist', 'Album'] # we want now all continuous features \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_continuous=df_10_playlist.drop(columns=non_wanted_features)\n",
        "features=X_continuous.columns\n",
        "\n",
        "X_continuous = scaler.fit_transform(X_continuous)\n",
        "X_continuous = pd.DataFrame(X_continuous)\n",
        "X_continuous.columns=features\n",
        "\n",
        "X_continuous"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Duration</th>\n",
              "      <th>Explicit</th>\n",
              "      <th>Popularity</th>\n",
              "      <th>Key</th>\n",
              "      <th>Mode</th>\n",
              "      <th>Time Signature</th>\n",
              "      <th>Acousticness</th>\n",
              "      <th>Danceability</th>\n",
              "      <th>Energy</th>\n",
              "      <th>Instrumentalness</th>\n",
              "      <th>Liveness</th>\n",
              "      <th>Loudness</th>\n",
              "      <th>Speechiness</th>\n",
              "      <th>Valence</th>\n",
              "      <th>Tempo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.067015</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.022727</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.052408</td>\n",
              "      <td>0.609729</td>\n",
              "      <td>0.601225</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.466076</td>\n",
              "      <td>0.891234</td>\n",
              "      <td>0.183161</td>\n",
              "      <td>0.455461</td>\n",
              "      <td>0.410860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.075587</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.038954</td>\n",
              "      <td>0.961538</td>\n",
              "      <td>0.546984</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078189</td>\n",
              "      <td>0.837884</td>\n",
              "      <td>0.456425</td>\n",
              "      <td>0.710580</td>\n",
              "      <td>0.270645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.151691</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.886364</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.016163</td>\n",
              "      <td>0.520362</td>\n",
              "      <td>0.915625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.765749</td>\n",
              "      <td>0.912589</td>\n",
              "      <td>0.131462</td>\n",
              "      <td>0.583021</td>\n",
              "      <td>0.651194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.085218</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.784091</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.014958</td>\n",
              "      <td>0.949095</td>\n",
              "      <td>0.466626</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.333122</td>\n",
              "      <td>0.821763</td>\n",
              "      <td>0.140325</td>\n",
              "      <td>0.273234</td>\n",
              "      <td>0.535097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.106419</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.534091</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.019577</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.913616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.225493</td>\n",
              "      <td>0.920134</td>\n",
              "      <td>0.048006</td>\n",
              "      <td>0.957123</td>\n",
              "      <td>0.483551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>0.375817</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.990950</td>\n",
              "      <td>0.561047</td>\n",
              "      <td>0.626927</td>\n",
              "      <td>0.064577</td>\n",
              "      <td>0.836041</td>\n",
              "      <td>0.638109</td>\n",
              "      <td>0.235717</td>\n",
              "      <td>0.483474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>891</th>\n",
              "      <td>0.068400</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.013954</td>\n",
              "      <td>0.897059</td>\n",
              "      <td>0.930692</td>\n",
              "      <td>0.004275</td>\n",
              "      <td>0.080089</td>\n",
              "      <td>0.965154</td>\n",
              "      <td>0.242245</td>\n",
              "      <td>0.511202</td>\n",
              "      <td>0.470708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>0.108448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.420455</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.006534</td>\n",
              "      <td>0.679864</td>\n",
              "      <td>0.783035</td>\n",
              "      <td>0.018397</td>\n",
              "      <td>0.063100</td>\n",
              "      <td>0.954095</td>\n",
              "      <td>0.023634</td>\n",
              "      <td>0.184264</td>\n",
              "      <td>0.509424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>0.101885</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.563348</td>\n",
              "      <td>0.637386</td>\n",
              "      <td>0.808839</td>\n",
              "      <td>0.062573</td>\n",
              "      <td>0.866857</td>\n",
              "      <td>0.024815</td>\n",
              "      <td>0.049416</td>\n",
              "      <td>0.483293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>894</th>\n",
              "      <td>0.087236</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.443182</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.001795</td>\n",
              "      <td>0.684389</td>\n",
              "      <td>0.914620</td>\n",
              "      <td>0.763618</td>\n",
              "      <td>0.219162</td>\n",
              "      <td>0.864055</td>\n",
              "      <td>0.049778</td>\n",
              "      <td>0.700933</td>\n",
              "      <td>0.470618</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>895 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Duration  Explicit  Popularity  ...  Speechiness   Valence     Tempo\n",
              "0    0.067015       1.0    0.022727  ...     0.183161  0.455461  0.410860\n",
              "1    0.075587       1.0    0.818182  ...     0.456425  0.710580  0.270645\n",
              "2    0.151691       1.0    0.886364  ...     0.131462  0.583021  0.651194\n",
              "3    0.085218       1.0    0.784091  ...     0.140325  0.273234  0.535097\n",
              "4    0.106419       0.0    0.534091  ...     0.048006  0.957123  0.483551\n",
              "..        ...       ...         ...  ...          ...       ...       ...\n",
              "890  0.375817       0.0    0.409091  ...     0.638109  0.235717  0.483474\n",
              "891  0.068400       0.0    0.590909  ...     0.242245  0.511202  0.470708\n",
              "892  0.108448       0.0    0.420455  ...     0.023634  0.184264  0.509424\n",
              "893  0.101885       0.0    0.500000  ...     0.024815  0.049416  0.483293\n",
              "894  0.087236       0.0    0.443182  ...     0.049778  0.700933  0.470618\n",
              "\n",
              "[895 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5gChvglkNmu"
      },
      "source": [
        "### --- Encode Artist Names ---\n",
        "# Integer encode artist names\n",
        "X = X_continuous\n",
        "X['Artist'] = df_10_playlist['Artist'].values\n",
        "# We will estimate the vocabulary size of (unique_artists*1000), which is much larger than needed to\n",
        "# reduce the probability of collisions from the hash function. (unique_artists = 510)\n",
        "vocab_size = len(df['Artist'].unique()) * 1000\n",
        "from keras.preprocessing.text import one_hot\n",
        "X['Encoded Artist'] = ''\n",
        "for idx, row in X.iterrows():\n",
        "    X.at[idx, 'Encoded Artist'] = one_hot(row['Artist'],vocab_size)\n",
        "    #print(\"The encoding for \", row['Artist'] ,\"is :\", X.at[idx, 'Encoded Artist'])\n",
        "\n",
        "# Padding\n",
        "## Find max len to do padding\n",
        "maxlen = -1\n",
        "for idx, row in X.iterrows():\n",
        "  if len(row['Encoded Artist']) > maxlen:\n",
        "    maxlen = len(row['Encoded Artist'])\n",
        "\n",
        "## Perform padding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "encodings_padded = []\n",
        "for idx, row in X.iterrows():\n",
        "    encodings_padded.append(row['Encoded Artist'])\n",
        "\n",
        "encodings_padded = pad_sequences(encodings_padded, maxlen=maxlen, padding='post')\n",
        "\n",
        "X.drop(columns='Encoded Artist')\n",
        "X['Encoded Artist'] = None\n",
        "for (idx, row), encoding in zip(X.iterrows(), encodings_padded):\n",
        "    X.at[idx, 'Encoded Artist'] = encoding\n",
        "X.drop(columns=['Artist'], inplace=True)\n",
        "\n",
        "Y = df_10_playlist['Playlist']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8q-Ubfeka9F"
      },
      "source": [
        "### --- Split data into test and train data ---\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
        "\n",
        "# Split data into continuous and categorical data\n",
        "x_train_artist, x_test_artist = x_train['Encoded Artist'], x_test['Encoded Artist']\n",
        "x_train_continuous, x_test_continuous = x_train.drop(columns=['Encoded Artist']), x_test.drop(columns=['Encoded Artist'])\n",
        "\n",
        "# Convert data to friendly arrays of TensorFlow\n",
        "x_train_artist, x_test_artist  = x_train_artist.values, x_test_artist.values\n",
        "x_train_artist, x_test_artist = x_train_artist.tolist(), x_test_artist.tolist()\n",
        "x_train_artist, x_test_artist = np.asarray(x_train_artist).astype('float32'), np.asarray(x_test_artist).astype('float32')\n",
        "\n",
        "x_train_continuous, x_test_continuous = x_train_continuous.values, x_test_continuous.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pand7-64s62X"
      },
      "source": [
        "### **Deep Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0FQaHg2kju0"
      },
      "source": [
        "#### One Hot Encode Target Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUMSon_Lkju1"
      },
      "source": [
        "### --- One Hot Encode Classes ---\n",
        "# Convert target playlist to one hot encoded playlist for Neural Network\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# First encode target values as integers from string\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "hot_Y = encoder.transform(Y)\n",
        "# Then perform one hot encoding\n",
        "hot_Y = np_utils.to_categorical(hot_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gELxAe9gkp47"
      },
      "source": [
        "#### Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj7jB2d4kp48"
      },
      "source": [
        "from keras.layers import Embedding, concatenate, Dense, Input, Flatten\n",
        "from keras import Model\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "continuous_inputs = np.concatenate((x_train_continuous, x_test_continuous), axis=0)\n",
        "artist_inputs  = np.concatenate((x_train_artist, x_test_artist), axis=0)\n",
        "inputs = np.concatenate((continuous_inputs, artist_inputs), axis=1)\n",
        "targets = np.concatenate((hot_y_train,hot_y_test), axis=0)\n",
        "\n",
        "## --- Model configuration ---\n",
        "batch_size = 16\n",
        "epochs = 5\n",
        "artist_embd_dim = 9\n",
        "\n",
        "## --- Cross Validation ---\n",
        "# Define the K-fold Cross Validator\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "  ## --- Model Architecture ---\n",
        "  n_numerical_feats = 15\n",
        "  n_classes = 10 # number of classes/playlists\n",
        "  # artist input to embeddings\n",
        "  artist_input = Input(shape=(maxlen,), name='artist_input')\n",
        "  artist_embedding = Embedding(vocab_size, artist_embd_dim,input_length=maxlen)(artist_input)\n",
        "  artist_vec=Flatten()(artist_embedding)\n",
        "  # numerical features input\n",
        "  numerical_input = Input(shape=(n_numerical_feats), name='numeric_input')\n",
        "\n",
        "  # input layer\n",
        "  merged = concatenate([numerical_input, artist_vec])\n",
        "\n",
        "  # hidden layers\n",
        "  # we want to make the network abstract the input information by reducing the dimensions\n",
        "  size_input = n_numerical_feats+(artist_embd_dim*maxlen)\n",
        "  size_hidden1 = int(size_input*32)\n",
        "  size_hidden2 = int(size_input*32) \n",
        "\n",
        "  hidden1 = Dense(size_hidden1, activation='relu')(merged)\n",
        "  hidden2 = Dense(size_hidden2, activation='relu')(hidden1)\n",
        "\n",
        "  # output layers\n",
        "  output = Dense(n_classes, activation='softmax')(hidden2)\n",
        "\n",
        "  # define the model\n",
        "  model = Model([numerical_input,artist_input], output)\n",
        "  # compile the model\n",
        "  model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'], experimental_run_tf_function=False)\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  artist_input_train = inputs[train][:,15:15+maxlen]; continuous_input_train = inputs[train][:,:15]\n",
        "  artist_input_test = inputs[test][:,15:15+maxlen]; continuous_input_test = inputs[test][:,:15]\n",
        "\n",
        "  history = model.fit([continuous_input_train,artist_input_train], [targets[train]],batch_size=batch_size,epochs=epochs,verbose=2)\n",
        "  \n",
        "  # Plot the training vs validation curves to see if we are overfitting the model\n",
        "  #plot_training(history, fold_no)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate([continuous_input_test,artist_input_test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# Provide average scores\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OX8eSxGkp5I"
      },
      "source": [
        "# Save model representation picture\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='/content/drive/MyDrive/Colab Notebooks/spotify-playlist-recommender/dnn_cat_num.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_gcmpv0kzQ0"
      },
      "source": [
        "### **Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3zHlnpdkzQ0"
      },
      "source": [
        "from keras.layers import Embedding, Input, Flatten\n",
        "from keras import Model\n",
        "from keras import optimizers\n",
        "artist_embd_dim = 9\n",
        "# artist embeddings\n",
        "artist_input = Input(shape=(maxlen,), name='artist_input')\n",
        "artist_embedding = Embedding(vocab_size, artist_embd_dim,input_length=maxlen)(artist_input)\n",
        "artist_vec=Flatten()(artist_embedding)\n",
        "\n",
        "# Define the model\n",
        "embed_model = Model([artist_input], artist_vec)\n",
        "embed_model.compile(optimizer=optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy']) \n",
        "\n",
        "# Generate embeddings\n",
        "embed_artist_train, embed_artist_test = embed_model.predict(x_train_artist), embed_model.predict(x_test_artist)\n",
        "\n",
        "x_train_embedded, x_test_embedded = [None]*len(x_train_continuous), [None]*len(x_test_continuous)\n",
        "\n",
        "x_train_embedded = [list(x_train_continuous[i]) + list(embed_artist_train[i]) for i in range(len(x_train_continuous))]\n",
        "x_test_embedded = [list(x_test_continuous[i]) + list(embed_artist_test[i]) for i in range(len(x_test_continuous))]\n",
        "x_train_embedded, x_test_embedded  = np.asarray(np.array(x_train_embedded)).astype('float32'), np.asarray(np.array(x_test_embedded)).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhOfVbRtkzQ1",
        "outputId": "6637f864-d0f1-4da3-83e6-c7e14f2a319d"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "#Naive Bayes Classifier \n",
        "NBClassifier = GaussianNB()\n",
        "NBAccuracy=[]\n",
        "\n",
        "inputs = np.concatenate((x_train_embedded,x_test_embedded), axis=0)\n",
        "targets = np.concatenate((y_train,y_test), axis=0)\n",
        "\n",
        "#CrossValidation\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "acc_per_foldNB = []\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "  \n",
        "  #fiting for the train set and predicting with the test set\n",
        "  NBClassifier= NBClassifier.fit(inputs[train],targets[train])\n",
        "  prediction=NBClassifier.predict(inputs[test])\n",
        "\n",
        "  #Naive Bayes Classifier Accuracy \n",
        "  accuracy = accuracy_score(targets[test], prediction)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "  acc_per_foldNB.append(accuracy * 100)\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# Provide average scores\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_foldNB)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Accuracy: {acc_per_foldNB[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_foldNB)} (+- {np.std(acc_per_foldNB)})')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Accuracy: 40.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Accuracy: 37.77777777777778%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Accuracy: 32.22222222222222%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Accuracy: 41.11111111111111%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Accuracy: 38.88888888888889%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Accuracy: 32.58426966292135%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Accuracy: 38.20224719101123%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Accuracy: 32.58426966292135%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Accuracy: 35.95505617977528%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Accuracy: 39.325842696629216%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 36.865168539325836 (+- 3.1569548350396635)\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fE3HQmEk6Vc"
      },
      "source": [
        "### **Random Forest** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0gPd1uEk6Vc",
        "outputId": "fc817e73-495d-45da-834a-4291cb583ca6"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "#Random Forest Classifier\n",
        "RFClassifier= RandomForestClassifier(random_state=2)\n",
        "RFAccuracy=[]\n",
        "\n",
        "inputs = np.concatenate((x_train_embedded,x_test_embedded), axis=0)\n",
        "targets = np.concatenate((y_train,y_test), axis=0)\n",
        "\n",
        "#CrossValidation\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "acc_per_foldRF = []\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "  \n",
        "  #fiting for the train set and predicting with the test set\n",
        "  RFClassifier= RFClassifier.fit(inputs[train],targets[train])\n",
        "  prediction=RFClassifier.predict(inputs[test])\n",
        "\n",
        "  #Naive Bayes Classifier Accuracy \n",
        "  accuracy = accuracy_score(targets[test], prediction)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "  acc_per_foldRF.append(accuracy * 100)\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# Provide average scores\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_foldRF)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Accuracy: {acc_per_foldRF[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_foldRF)} (+- {np.std(acc_per_foldRF)})')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Accuracy: 80.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Accuracy: 84.44444444444444%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Accuracy: 88.88888888888889%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Accuracy: 81.11111111111111%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Accuracy: 92.22222222222223%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Accuracy: 76.40449438202246%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Accuracy: 84.26966292134831%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Accuracy: 79.7752808988764%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Accuracy: 71.91011235955057%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Accuracy: 84.26966292134831%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 82.32958801498128 (+- 5.567740988374248)\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGWYUup8eIds"
      },
      "source": [
        "### Results Study\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "cRHCplwydiEE",
        "outputId": "b6d96ba5-956c-43fa-cd02-0b10875b75dd"
      },
      "source": [
        "Accuracy_df=pd.DataFrame()\r\n",
        "Accuracy_df=pd.DataFrame(acc_per_foldNB).astype(float)\r\n",
        "Accuracy_df.columns=[\"Accuracy NB\" ]\r\n",
        "Accuracy_df[\"Accuracy RF\"]=acc_per_foldRF\r\n",
        "Accuracy_df[\"Accuracy DEEP NN\"]=acc_per_fold\r\n",
        "Accuracy_df[\"Fold\"]=[x for x in range(1,11)]\r\n",
        "Accuracy_df.reset_index\r\n",
        "\r\n",
        "plot_cat_all_AF_RFE=(\r\n",
        "    ggplot(Accuracy_df)+geom_line(aes(x=\"Fold\",y=\"Accuracy NB\"), color=\"r\")\r\n",
        "    + geom_line(aes(x=\"Fold\",y=\"Accuracy RF\"), color=\"b\")\r\n",
        "    + geom_line(aes(x=\"Fold\",y=\"Accuracy DEEP NN\"), color=\"y\")\r\n",
        "    + labs(x = \"Folds\", y= \"Accuracy\",color = \"Legend\")\r\n",
        ")\r\n",
        "#BLUE   = RANDOM FOREST\r\n",
        "#RED    = NAIVE BAYES\r\n",
        "#YELLOW = DEEP NN'''\r\n",
        "plot_cat_all_AF_RFE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ggplot: (-9223363296546241427)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zocLu_CMsir2"
      },
      "source": [
        "## Features selected by our Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skpsX8URN4K0"
      },
      "source": [
        "# Scale continuous data \n",
        "features = ['Popularity', 'Acousticness', 'Danceability','Instrumentalness', 'Loudness', 'Encoded Artist']\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_continuous = scaler.fit_transform(df_10_playlist[features[:5]])\n",
        "X_continuous = pd.DataFrame(X_continuous)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea6CU8nBFX48"
      },
      "source": [
        "### --- Encode Artist Names ---\n",
        "# Integer encode artist names\n",
        "X = X_continuous\n",
        "X['Artist'] = df_10_playlist['Artist'].values\n",
        "# We will estimate the vocabulary size of (unique_artists*1000), which is much larger than needed to\n",
        "# reduce the probability of collisions from the hash function. (unique_artists = 510)\n",
        "vocab_size = len(df['Artist'].unique()) * 1000\n",
        "from keras.preprocessing.text import one_hot\n",
        "X['Encoded Artist'] = ''\n",
        "for idx, row in X.iterrows():\n",
        "    X.at[idx, 'Encoded Artist'] = one_hot(row['Artist'],vocab_size)\n",
        "    #print(\"The encoding for \", row['Artist'] ,\"is :\", X.at[idx, 'Encoded Artist'])\n",
        "\n",
        "# Padding\n",
        "## Find max len to do padding\n",
        "maxlen = -1\n",
        "for idx, row in X.iterrows():\n",
        "  if len(row['Encoded Artist']) > maxlen:\n",
        "    maxlen = len(row['Encoded Artist'])\n",
        "\n",
        "## Perform padding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "encodings_padded = []\n",
        "for idx, row in X.iterrows():\n",
        "    encodings_padded.append(row['Encoded Artist'])\n",
        "\n",
        "encodings_padded = pad_sequences(encodings_padded, maxlen=maxlen, padding='post')\n",
        "\n",
        "X.drop(columns='Encoded Artist')\n",
        "X['Encoded Artist'] = None\n",
        "for (idx, row), encoding in zip(X.iterrows(), encodings_padded):\n",
        "    X.at[idx, 'Encoded Artist'] = encoding\n",
        "X.drop(columns=['Artist'], inplace=True)\n",
        "\n",
        "Y = df_10_playlist['Playlist']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERQDW7yodYFA"
      },
      "source": [
        "### --- Split data into test and train data ---\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
        "\n",
        "# Split data into continuous and categorical data\n",
        "x_train_artist, x_test_artist = x_train['Encoded Artist'], x_test['Encoded Artist']\n",
        "x_train_continuous, x_test_continuous = x_train.drop(columns=['Encoded Artist']), x_test.drop(columns=['Encoded Artist'])\n",
        "\n",
        "# Convert data to friendly arrays of TensorFlow\n",
        "x_train_artist, x_test_artist  = x_train_artist.values, x_test_artist.values\n",
        "x_train_artist, x_test_artist = x_train_artist.tolist(), x_test_artist.tolist()\n",
        "x_train_artist, x_test_artist = np.asarray(x_train_artist).astype('float32'), np.asarray(x_test_artist).astype('float32')\n",
        "\n",
        "x_train_continuous, x_test_continuous = x_train_continuous.values, x_test_continuous.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzUPmAZAd2gv"
      },
      "source": [
        "### **Deep Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoFS-mdOD1q7"
      },
      "source": [
        "#### One Hot Encode Target Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPENouFPD3hi"
      },
      "source": [
        "### --- One Hot Encode Classes ---\n",
        "# Convert target playlist to one hot encoded playlist for Neural Network\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# First encode target values as integers from string\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "hot_Y = encoder.transform(Y)\n",
        "# Then perform one hot encoding\n",
        "hot_Y = np_utils.to_categorical(hot_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOjHOE3vE5Hn"
      },
      "source": [
        "#### Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7DNmi6zPVBA"
      },
      "source": [
        "from keras.layers import Embedding, concatenate, Dense, Input, Flatten\n",
        "from keras import Model\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "continuous_inputs = np.concatenate((x_train_continuous, x_test_continuous), axis=0)\n",
        "artist_inputs  = np.concatenate((x_train_artist, x_test_artist), axis=0)\n",
        "inputs = np.concatenate((continuous_inputs, artist_inputs), axis=1)\n",
        "targets = np.concatenate((hot_y_train,hot_y_test), axis=0)\n",
        "\n",
        "## --- Model configuration ---\n",
        "batch_size = 16\n",
        "epochs = 5\n",
        "artist_embd_dim = 9\n",
        "\n",
        "## --- Cross Validation ---\n",
        "# Define the K-fold Cross Validator\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "  ## --- Model Architecture ---\n",
        "  n_numerical_feats = 5\n",
        "  n_classes = 10 # number of classes/playlists\n",
        "  # artist input to embeddings\n",
        "  artist_input = Input(shape=(maxlen,), name='artist_input')\n",
        "  artist_embedding = Embedding(vocab_size, artist_embd_dim,input_length=maxlen)(artist_input)\n",
        "  artist_vec=Flatten()(artist_embedding)\n",
        "  # numerical features input\n",
        "  numerical_input = Input(shape=(n_numerical_feats), name='numeric_input')\n",
        "\n",
        "  # input layer\n",
        "  merged = concatenate([numerical_input, artist_vec])\n",
        "\n",
        "  # hidden layers\n",
        "  # we want to make the network abstract the input information by reducing the dimensions\n",
        "  size_input = n_numerical_feats+(artist_embd_dim*maxlen)\n",
        "  size_hidden1 = int(size_input*32)\n",
        "  size_hidden2 = int(size_input*32) \n",
        "\n",
        "  hidden1 = Dense(size_hidden1, activation='relu')(merged)\n",
        "  hidden2 = Dense(size_hidden2, activation='relu')(hidden1)\n",
        "\n",
        "  # output layers\n",
        "  output = Dense(n_classes, activation='softmax')(hidden2)\n",
        "\n",
        "  # define the model\n",
        "  model = Model([numerical_input,artist_input], output)\n",
        "  # compile the model\n",
        "  model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'], experimental_run_tf_function=False)\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  artist_input_train = inputs[train][:,5:5+maxlen]; continuous_input_train = inputs[train][:,:5]\n",
        "  artist_input_test = inputs[test][:,5:5+maxlen]; continuous_input_test = inputs[test][:,:5]\n",
        "\n",
        "  history = model.fit([continuous_input_train,artist_input_train], [targets[train]],batch_size=batch_size,epochs=epochs,verbose=2)\n",
        "  \n",
        "  # Plot the training vs validation curves to see if we are overfitting the model\n",
        "  #plot_training(history, fold_no)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate([continuous_input_test,artist_input_test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# Provide average scores\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqvW_kFMPwuG"
      },
      "source": [
        "# Save model representation picture\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='/content/drive/MyDrive/Colab Notebooks/spotify-playlist-recommender/dnn_num.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIXNepeOr3zt"
      },
      "source": [
        "### **Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6W90jezchpv"
      },
      "source": [
        "from keras.layers import Embedding, Input, Flatten\n",
        "from keras import Model\n",
        "from keras import optimizers\n",
        "artist_embd_dim = 9\n",
        "# artist embeddings\n",
        "artist_input = Input(shape=(maxlen,), name='artist_input')\n",
        "artist_embedding = Embedding(vocab_size, artist_embd_dim,input_length=maxlen)(artist_input)\n",
        "artist_vec=Flatten()(artist_embedding)\n",
        "\n",
        "# Define the model\n",
        "embed_model = Model([artist_input], artist_vec)\n",
        "embed_model.compile(optimizer=optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy']) \n",
        "\n",
        "# Generate embeddings\n",
        "embed_artist_train, embed_artist_test = embed_model.predict(x_train_artist), embed_model.predict(x_test_artist)\n",
        "\n",
        "x_train_embedded, x_test_embedded = [None]*len(x_train_continuous), [None]*len(x_test_continuous)\n",
        "\n",
        "x_train_embedded = [list(x_train_continuous[i]) + list(embed_artist_train[i]) for i in range(len(x_train_continuous))]\n",
        "x_test_embedded = [list(x_test_continuous[i]) + list(embed_artist_test[i]) for i in range(len(x_test_continuous))]\n",
        "x_train_embedded, x_test_embedded  = np.asarray(np.array(x_train_embedded)).astype('float32'), np.asarray(np.array(x_test_embedded)).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEMsQ1wk2PoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0736ef90-02e3-4475-f9af-13f1c6abc0f0"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "#Naive Bayes Classifier \n",
        "NBClassifier = GaussianNB()\n",
        "NBAccuracy=[]\n",
        "\n",
        "inputs = np.concatenate((x_train_embedded,x_test_embedded), axis=0)\n",
        "targets = np.concatenate((y_train,y_test), axis=0)\n",
        "\n",
        "#CrossValidation\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "acc_per_foldNB = []\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "  \n",
        "  #fiting for the train set and predicting with the test set\n",
        "  NBClassifier= NBClassifier.fit(inputs[train],targets[train])\n",
        "  prediction=NBClassifier.predict(inputs[test])\n",
        "\n",
        "  #Naive Bayes Classifier Accuracy \n",
        "  accuracy = accuracy_score(targets[test], prediction)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "  acc_per_foldNB.append(accuracy * 100)\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# Provide average scores\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_foldNB)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Accuracy: {acc_per_foldNB[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_foldNB)} (+- {np.std(acc_per_foldNB)})')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Accuracy: 21.11111111111111%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Accuracy: 28.888888888888886%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Accuracy: 35.55555555555556%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Accuracy: 30.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Accuracy: 35.55555555555556%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Accuracy: 25.842696629213485%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Accuracy: 33.70786516853933%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Accuracy: 30.337078651685395%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Accuracy: 23.595505617977526%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Accuracy: 28.08988764044944%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 29.26841448189763 (+- 4.6090592304364195)\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0yZ3l27sApe"
      },
      "source": [
        "### **Random Forest** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNA7fy-V19rP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d741044-b3b6-47b2-ceab-237d842c8bdb"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "#Random Forest Classifier\n",
        "RFClassifier= RandomForestClassifier(random_state=2)\n",
        "RFAccuracy=[]\n",
        "\n",
        "inputs = np.concatenate((x_train_embedded,x_test_embedded), axis=0)\n",
        "targets = np.concatenate((y_train,y_test), axis=0)\n",
        "\n",
        "#CrossValidation\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "acc_per_foldRF = []\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "  \n",
        "  #fiting for the train set and predicting with the test set\n",
        "  RFClassifier= RFClassifier.fit(inputs[train],targets[train])\n",
        "  prediction=RFClassifier.predict(inputs[test])\n",
        "\n",
        "  #Naive Bayes Classifier Accuracy \n",
        "  accuracy = accuracy_score(targets[test], prediction)\n",
        "    \n",
        "  # Generate generalization metrics\n",
        "  acc_per_foldRF.append(accuracy * 100)\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# Provide average scores\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_foldRF)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Accuracy: {acc_per_foldRF[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_foldRF)} (+- {np.std(acc_per_foldRF)})')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Accuracy: 67.77777777777779%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Accuracy: 82.22222222222221%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Accuracy: 81.11111111111111%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Accuracy: 66.66666666666666%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Accuracy: 72.22222222222221%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Accuracy: 75.28089887640449%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Accuracy: 64.04494382022472%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Accuracy: 79.7752808988764%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Accuracy: 73.03370786516854%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Accuracy: 76.40449438202246%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 73.85393258426966 (+- 5.953375783977049)\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac27fFpEhFbC"
      },
      "source": [
        "### Results Study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cInbzuUShP7p"
      },
      "source": [
        "acc_per_fold=[71,89,87,73,70,71,81,77,75,73]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "UwlqJK5QhLHM",
        "outputId": "0097e472-81bb-4d94-f9b7-98af312dbc80"
      },
      "source": [
        "Accuracy_df=pd.DataFrame()\r\n",
        "Accuracy_df=pd.DataFrame(acc_per_foldNB).astype(float)\r\n",
        "Accuracy_df.columns=[\"Accuracy NB\" ]\r\n",
        "Accuracy_df[\"Accuracy RF\"]=acc_per_foldRF\r\n",
        "Accuracy_df[\"Accuracy DEEP NN\"]=acc_per_fold\r\n",
        "Accuracy_df[\"Fold\"]=[x for x in range(1,11)]\r\n",
        "Accuracy_df.reset_index\r\n",
        "\r\n",
        "plot_cat_all_AF_Selected=(\r\n",
        "    ggplot(Accuracy_df)+geom_line(aes(x=\"Fold\",y=\"Accuracy NB\"), color=\"r\")\r\n",
        "    + geom_line(aes(x=\"Fold\",y=\"Accuracy RF\"), color=\"b\")\r\n",
        "    + geom_line(aes(x=\"Fold\",y=\"Accuracy DEEP NN\"), color=\"y\")\r\n",
        "    + labs(x = \"Folds\", y= \"Accuracy\",color = \"Legend\")\r\n",
        ")\r\n",
        "#BLUE   = RANDOM FOREST\r\n",
        "#RED    = NAIVE BAYES\r\n",
        "#YELLOW = DEEP NN'''\r\n",
        "plot_cat_all_AF_Selected"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ggplot: (-9223363296546276366)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5PB5ZWaS9QZ"
      },
      "source": [
        "# OverAll Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5OpsHoqTD0J"
      },
      "source": [
        "The deductions taken after the study are:\r\n",
        "\r\n",
        "*   The model mostly classifies in a correct way between the different Genres of songs. Most of the errors are Between diefferent Rap Playlists or Punk-Metal...\r\n",
        "*   The model does not recognize the lenguaje of the song, classyfing RAP_ESPANOL with TUFF which both are RAP, one American Rap and The other Spanish Rap.\r\n",
        "*   PowerHour Playlist is not an appropiate since it is a Gym Playlist which has songs of Rap, Punk, EDM the example of the song \"GooseBumps\" demostrates it. This song could be classified in both Playlists.\r\n",
        "\r\n",
        "* The use of **Embeddings** to use a categorical variable like the Artist with such a big dimensional space gives a lot of information to the models and that is why we can see such improvement in RF model and Deep Neural Network.\r\n",
        "\r\n",
        "* **Random Forest** is the classifier that should be used.\r\n",
        "\r\n",
        "* **Neural Networks** also have a very great perfomance, very close to the RF models. However, the optimization of the network could be better and in case you optimize it for this problem you will most likely get better results than with the RF model.\r\n",
        "\r\n",
        "* The whole set of continious features will give a decent job, using categorical would give a slightly better result. \r\n",
        "\r\n",
        "As an Overall, I think that it is performing in a good manner\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2IcWdXKT3G0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}